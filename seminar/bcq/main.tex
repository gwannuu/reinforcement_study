\documentclass[11pt]{beamer}
\usefonttheme[onlymath]{serif}

\setbeamersize{text margin left=1.5em}
\setbeamersize{text margin right=1.5em}




\setbeamertemplate{frametitle}{%
  \vskip1ex
  \usebeamerfont{frametitle}%
  \insertframetitle\par %frame 에서 지정한 title 사용
  %\insertsubsectionhead\par        % subsection의 header를 사용
  \vskip1ex
  \hrule                             % 밑줄(선택)
}

\setbeamertemplate{blocks}[rounded][shadow=true] % 블록 테두리 둥글게

\setbeamertemplate{itemize item}{\usebeamerfont{itemize item}\textbullet}
\setbeamertemplate{itemize subitem}{\usebeamerfont{itemize subitem}\textbullet}
\setbeamertemplate{itemize subsubitem}{\usebeamerfont{itemize subsubitem}\textbullet}



%\setbeamerfont{itemize/enumerate subbody}{parent=itemize/enumerate body} % 
%\setbeamerfont{itemize/enumerate subbody}{size=\usebeamerfont{itemize/enumerate body}\size}

\makeatletter
% Taken from beamer.cls' default geometry settings
% http://mirrors.ctan.org/macros/latex/contrib/beamer/base/beamer.cls
\geometry{%
  papersize={\fpeval{\beamer@paperwidth*1.5}pt,\fpeval{\beamer@paperheight*1.5}pt},
  hmargin=\fpeval{0.5 * 1.5}cm,% 1cm
  vmargin=0cm,%
  head=\fpeval{0.5*1.5}cm,% 0.5cm
  headsep=0pt,%
  foot=\fpeval{0.5*1.5}cm% 0.5cm
}
\makeatother %from search keyword beamer size, get this search result -> https://tex.stackexchange.com/questions/586756/beamer-use-glyphs-from-smaller-font-size-but-enlarge


% Reference bibtex
% style = numeric, apa, authoryear-comp
\usepackage[backend=biber, style=authoryear-comp, natbib=true]{biblatex}
\addbibresource{../../../references.bib}



% 테마 선택 (선택 사항)
% \usetheme{Madrid} % 기본 테마, 다른 테마 사용 가능
% \font{serif}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage[T1]{fontenc} % To use combination of textbf, textit
\usepackage[dvipsnames]{xcolor}   % can use more variant colors
%\usepackage{lmodern} %다른 폰트 사용: 문서의 서문에 추가하면 Computer Modern 폰트의 확장 버전인 Latin Modern 폰트를 사용할 수 있습니다. 이 폰트는 더 다양한 크기와 스타일을 지원하여 문제를 해결해 줄 수 있습니다.

% \setcounter{MaxMatrixCols}{20}

% (필요한 패키지들)
% \usepackage{amsthm}
\setbeamertemplate{theorems}[numbered]  % 정리, 정의 등에 번호를 달아줌

% \theoremstyle{plain} % insert bellow all blocks you want in italic
% \newtheorem{theorem}{Theorem}[section] % to number according to section
% 
% \theoremstyle{definition} % insert bellow all blocks you want in normal text
% \newtheorem{definition}{Definition}[section] % to number according to section
% \newtheorem*{idea}{Proof idea} % no numbered block

\newtheorem{proposition}[theorem]{Proposition}

\usepackage{tcolorbox}

% 필요할 경우 패키지 추가
\usepackage{graphicx} % 이미지 삽입을 위한 패키지
\usepackage{amsmath}   % 수식 사용
\usepackage{hyperref}  % 하이퍼링크 추가
\hypersetup{
  colorlinks=true,
  linkcolor=blue,
} % search keyword: beamer hyperref color -> https://tex.stackexchange.com/questions/13423/how-to-change-the-color-of-href-links-for-real
%search keyword: hyperref color -> https://www.overleaf.com/learn/latex/Hyperlinks
\usepackage{cleveref}
\usepackage{multicol}  % 여러 열 나누기
\usepackage{ulem} % 취소선 및줄 나누기
\usepackage{mathtools} % dcases
%\usepackage{xparse} % NewDocumentCommand
\usepackage[boxed, lined]{algorithm2e} % to use algorithm block



% \NewDocumentCommand{\DefThreeOp}{m}{%
%   % \csname #1\endcsname 라는 이름으로, 3개 인자를 받는 새 매크로를 정의
%   \expandafter\NewDocumentCommand\csname #1\endcsname{mmm}{%
%     \operatorname{#1}\!\bigl(##1,\,##2,\,##3\bigr)%
%   }%
% }

\newcommand{\mrm}[1]{\mathrm{#1}}
\newcommand{\mbb}[1]{\mathbb{#1}}
\newcommand{\mb}[1]{\mathbf{#1}}
\newcommand{\mc}[1]{\mathcal{#1}}
\newcommand{\tb}[1]{\textbf{#1}}
\newcommand{\ti}[1]{\textit{#1}}
\newcommand{\Pois}[1]{\operatorname{Pois}(#1)}

\newcommand{\myber}[1]{\operatorname{Bern}\!\left(#1\right)}
\newcommand{\Bin}[2]{\operatorname{Bin}\!\left(#1,#2\right)}
\newcommand{\NBin}[2]{\operatorname{NBin}\!\left(#1,#2\right)}
\newcommand{\mytoinf}[1]{#1 \rightarrow \infty}
\newcommand{\myexp}[1]{\exp{\left(#1\right)}}
\newcommand{\Unif}[2]{\operatorname{Unif}\!\left(#1, #2\right)}
\newcommand{\UnifOne}[1]{\operatorname{Unif}\!\left(#1\right)}
\newcommand{\mygeom}[1]{\operatorname{Geom}\!\left(#1\right)}
\newcommand{\Expo}[1]{\operatorname{Expo}\!\left(#1\right)}
\newcommand{\abs}[1]{\left\lvert #1 \right\rvert}
\newcommand{\floor}[1]{\left\lfloor #1 \right \rfloor}
\newcommand{\expec}[1]{\operatorname{E}\left[ #1 \right]}
\newcommand{\Var}[1]{\operatorname{Var}\left[#1\right]}
\newcommand{\myskew}[1]{\operatorname{Skew}\!\left[#1\right]}
\newcommand{\mykurt}[1]{\operatorname{Kurt}\!\left[#1\right]}
\newcommand{\mywei}[2]{\operatorname{Wei}\!\left(#1, #2\right)}
\newcommand{\Span}{\operatorname{Span}}
\newcommand{\Cov}[2]{\operatorname{Cov}\!\left(#1, #2\right)}
\newcommand{\intinfty}{\int_{-\infty}^\infty}
\newcommand{\Corr}[2]{\operatorname{Corr}\!\left(#1, #2\right)}
\newcommand{\Mult}[3]{\operatorname{Mult}_{#1}\!\left(#2, #3\right)}
\newcommand{\Beta}[2]{\operatorname{Beta}\!\left(#1, #2\right)}
\newcommand{\HGeom}[3]{\operatorname{HGeom}\!\left(#1, #2, #3\right)}
\newcommand{\NHGeom}[3]{\operatorname{NHGeom}\!\left(#1,#2, #3\right)}
\newcommand{\GammaDist}[2]{\operatorname{Gamma}\!\left(#1, #2\right)}
%\DefThreeOp{PHGeom}

\newcommand{\im}{\operatorname{im}}
\newcommand{\tr}{\operatorname{tr}}
\newcommand{\supp}{\operatorname{supp}}


% 발표 제목, 저자, 날짜 설정
\title{Off-Policy Deep Reinforcement Learning without Exploration (BCQ)}
\author{Gwanwoo Choi}
% \date{}

\begin{document}
% 표지 슬라이드

\begin{frame}
    \titlepage
\end{frame}

% \subsection{Domain Randomization}
% \begingroup
%     \setbeamertemplate{frametitle}{%
%     \vskip1ex
%     \usebeamerfont{frametitle}%
%     \insertframetitle\par        %  ← 원하는 대로 변경 가능
%     \vskip1ex
%     \hrule                             % 밑줄(선택)
%     }
%     \begin{frame}
%         \frametitle{Table of Contents}
%         \tableofcontents[currentsubsection]
%     \end{frame}
% \endgroup

% % 목차 

% \begin{frame}{Curriculum Learning}

% \end{frame}

\begin{frame}{Extrapolation Error}
    Extrapolation Error: an error in off-policy value learning which is introduced by \tb{the mismatch between the dataset and true state-action visitation of the current policy}.

    \begin{block}{Absent Data}
        If any state-action pair $(s,a)$ is unavailable, then error is introduced as some function of the amount of similar data and approximation error.
        This means that the estimate of $Q_\theta(s^\prime, \pi(s^\prime))$ may be arbitrary bad without sufficient data near $(s^\prime, \pi(s^\prime))$.
    \end{block}

    \begin{block}{Model Bias}
        In offline RL, the bellman operator $\mc{T}^\pi$ is approximated by sampling transitions $(s,a,r,s^\prime)$ from $\mc{B}$ to estimate the expectation over $s^\prime$.
        \[
            \mc{T}^\pi Q (s,a) \approx \mbb{E}_{\textcolor{red}{s^\prime \sim \mc{B}}}[r + \gamma Q(s^\prime, \pi(s^\prime))]
        \]
        Without infinite state-action visitation, this produces a biased estimate of the transition dynamics.
    \end{block}

    \begin{block}{Training Mismatch}
        Even with sufficient datat, batch $\mc{B}$ is fixed.
        In Deep Q-Learning in batch setting,
        \[
            \approx \frac{1}{\abs{\mc{B}}} \sum_{\textcolor{red}{(s,a,r,s^\prime) \in \mc{B}}} \abs{r + \gamma Q_{\theta^\prime}(s^\prime, \pi(s^\prime)) - Q_\theta (s,a)}^2
        \]
        If the distribution of batch mismatches with the distribution under the current policy, learned value function may poor estimate
    \end{block}
\end{frame}

\begin{frame}{Motivation}
    Datasets for Experiment
    \begin{itemize}
        \item Final buffer : replay buffer gathered by training a DDPG agent over a million time steps.
        \item Concurrent : baselines and BCQ train concurrently, with the same replay buffer, as the behavioral DDPG policy.
        \item Imitation : dataset collected by an expert policy.
        \item Imperfect demonstration : With 30\% of dataset, action is randomly chosen. With remainings, expert policy selects actions randomly with adding high exploration noise $\mc{N}(0, 0.3)$
    \end{itemize}

    \begin{figure}
        \centering
        \includegraphics[width=0.65\textwidth]{Figure1Clipped.png}
    \end{figure}
\end{frame}

\begin{frame}{Motivation}
    \begin{columns}
        \begin{column}{0.5\textwidth}
            \begin{figure}
                \centering
            \includegraphics[width=0.9\textwidth]{figure1.png}
            \end{figure}
        \end{column}
        \begin{column}{0.5\textwidth}
            \begin{itemize}
                \item In each task, the off-policy agent performances significantly worse than the behavioral agent.
                \item Even in the concurrent experiment, where both agents are trained with the same dataset, there is a large gap in performance in every single trial.
                \item This result suggests that differences in the state distribution under the initial policies is enough for extrapolation error to drastically offset the performance of the off-policy agent.
            \end{itemize}
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}{Batch-Constrained Reinforcement Learning}
    \begin{itemize}
        \item Batch MDP : $M_{\mc{B}}$
        \item Batch Transition Probability: $p_{\mc{B}}(s^\prime|s,a) = \frac{N(s,a,s^\prime)}{\sum_{\tilde{S}}N(s,a,\tilde{s})}$


        \item Set $s_{\text{init}}$ as pseudo terminal state. If $\sum_{\tilde{s}} N(s,a, \tilde{s}) = 0$, then denote $p_{\mc{B}}(s_{\text{{init}}|s,a}) = 1$.

        Note that in batch constrained setting, for transition $(s,a,s_{\text{init}})$, $Q(s,a)$ will not update.
        \item Batch-Constrained Policy $\pi \in \Pi_{\mc{B}}$ satisfies that if for all $(s,a)$ such that $d^{\pi}(s) > 0$ and $\pi(a|s)>0$, then $(s,a) \in \mc{B}$.
        \item $Q^\pi_{\mc{B}}$ computed with the batch $\mc{B}$ and the value function $Q^\pi$ computed with the true MDP $M$
    \end{itemize}

    \begin{block}{Batch-Constrained Q-Learning (BCQL)}
        Batch-constrained Q-Learning follows the standard tabular Q-Learning update while constraining the possible actions with respect to the batch
        \[
            Q(s,a) \leftarrow (1-\alpha) Q(s,a) + \alpha (r + \gamma \max_{ \textcolor{red}{a^\prime \text{ s.t. }(s^\prime, a^\prime) \in \mc{B}}} Q(s^\prime, a^\prime))
        \]
    \end{block}

    \begin{block}{Theorem4}
        Given a deterministic MDP and coherent batch $\mc{B}$, along with the Robbins-Monro stochastic convergence conditions on the learning rate $\alpha$ and standard sampling requirements on the batch $\mc{B}$, \ti{BCQL converges to $Q^{\pi}_\mc{B} (s,a)$ where $\pi^\ast(s) = \arg\max_{a \text{ s.t. }(s,a) \in \mc{B}} Q^\pi_{\mc{B}}(s,a)$ is the optimal batch-constrained policy}
    \end{block}

\end{frame}

\begin{frame}{Batch-Constrained Deep Reinforcement Learning}
    BCQ approaches the notion of batch-constrained through a generative model.
    \begin{itemize}
        \item For a given state, BCQ generates plausible candidate actions with high similarity to the batch.
        \item And then selects the highest valued action through a learned Q-Network.
    \end{itemize}

    Penalize Q value of rare, unseen states via modified version of TD3. (Critic update)
\end{frame}

\begin{frame}{Batch-Constrained Deep Reinforcement Learning}
    \tb{BCQ approaches the notion of batch-constrained through a generative model. (Policy update)}
    \begin{itemize}
        \item For a given state, BCQ generates plausible candidate actions with high similarity to the batch.
        \item And then selects the highest valued action through a learned Q-Network.
    \end{itemize}

    Penalize Q value of rare, unseen states via modified version of TD3. (Critic update)
    \bigskip

    \vfill

    \begin{enumerate}
        \item In ideal situation, we can learn state-conditioned marginal likelihood $P^G_\mc{B}(a|s) \approx P_{\mc{B}}(a|s)$.
        But it is difficult to learn $P^G_\mc{B}(a|s)$ in continuous state space and high-dimensional action space.
        $\implies$ We instead train a variational auto-endocer(VAE): $G_\omega (s)$. This fucntion generates action $a$ by given state $s$.
        \item Generate $n$ actions $\{a_i\} \sim G_\omega (s)$.
        And define $\pi(s) \coloneqq \arg \max_{a_i} Q_\theta (s, a_i)$
        \item Increase diversity of selected action, with introducion of perturbation model, $\xi_\phi(s,a,\Phi)$, where $s,a$ is input and $\Phi$ is hyperparameter
        $\implies$ $\pi(s) \coloneqq \arg \max_{a_i + \xi_\phi(s,a_i, \Phi)} Q_\theta (s,a_i + \xi_\phi (s,a_i, \Phi)), \{a_i \sim G_\omega(s)\}_{i=1}^n$

    \end{enumerate}

    \begin{figure}
        \centering
        \includegraphics[width=0.5\textwidth]{Actor.png}
    \end{figure}
\end{frame}

\begin{frame}{Batch-Constrained Deep Reinforcement Learning}
    \tb{BCQ approaches the notion of batch-constrained through a generative model. (Policy update)}
    \begin{itemize}
        \item For a given state, BCQ generates plausible candidate actions with high similarity to the batch.
        \item And then selects the highest valued action through a learned Q-Network.
    \end{itemize}

    Penalize Q value of rare, unseen states via modified version of TD3. (Critic update)
    \bigskip

    \vfill

    Note that hyperparameters $n$ and $\Phi$ determine the ration between Imitation Learning and Reinforcement Learning.
    $\pi(s) \coloneqq \arg \max_{a_i + \xi_\phi(s,a_i, \Phi)} Q_\theta (s,a_i + \xi_\phi (s,a_i, \Phi)), \{a_i \sim G_\omega(s)\}_{i=1}^n$.
    \begin{itemize}
        \item If $n=1$ and $\Phi=0$, then the policy resembles behavior cloning
        \item If $n=\infty$ and $\Phi = a_{\text{max}} - a_{\text{min}}$, then the algorithm approaches Q-Learning, as the the policy begins to greedily maximize the value function over the entire action space.
    \end{itemize}
\end{frame}


\begin{frame}{Batch-Constrained Deep Reinforcement Learning}
    \tb{BCQ approaches the notion of batch-constrained through a generative model. (Policy update)}
    \begin{itemize}
        \item For a given state, BCQ generates plausible candidate actions with high similarity to the batch.
        \item And then selects the highest valued action through a learned Q-Network.
    \end{itemize}

    Penalize Q value of rare, unseen states via modified version of TD3. (Critic update)
    \bigskip

    \vfill

    The perturbation model $\xi_\phi$ can be trained to maximize $Q_\theta(s,a)$ through the DDPG style update by sampling $a \sim G_\omega (s)$
    \[
        \phi \leftarrow \arg\max_\phi \sum_{(s,a) \in \mc{B}} Q_\theta (s, a+ \xi_\phi (s,a, \Phi))
    \]
\end{frame}

\begin{frame}{Batch-Constrained Deep Reinforcement Learning}
    BCQ approaches the notion of batch-constrained through a generative model.
    \begin{itemize}
        \item For a given state, BCQ generates plausible candidate actions with high similarity to the batch.
        \item And then selects the highest valued action through a learned Q-Network.
    \end{itemize}

    \tb{Penalize Q value of rare, unseen states via modified version of TD3. (Critic update)}
    \bigskip

    \vfill

    To penalize uncertainty over future states, modify Clipped Double Q-Learning (TD3).
    The learning target used by both Q-Network is defined by
    \[
        y = r + \gamma \max_{a_i} \left[ \lambda \min_{j=1,2} Q_{\theta^\prime_j} (s^\prime,a_i) + (1-\lambda) \max_{j=1,2} Q_{\theta^\prime_j} (s^\prime, a_i)\right]
    \]
    \begin{itemize}
        \item A higher value of $\lambda$ places more weight on the minimum, while a lower value of $1 - \lambda$ places less weight on the maxmium.
        \item When a state $s$ is infrequent in the batch $\mc{B}$, it is likely that $Q_1$ and $Q_2$ will produce very different values.
        In this case, the minimum operator \tb{prevents overestimation} by selecting the lower (more conservative) value.
        \item Conversely, when a state $s$ is frequenty in the batch $\mc{B}$, $Q_1$ and $Q_2$ will likely have similar values.
        Here, the minimum operator has a \tb{lower effect}, and the penalty for overestimation is minimal.
    \end{itemize}
\end{frame}

\begin{frame}{Batch-Constrained Deep Reinforcement Learning}
    \begin{figure}
        \centering
        \includegraphics[width=0.5\textwidth]{algorithm.png}
    \end{figure}
    BCQ maintains four parametrized model: a generative model $G_\omega(s)$, a perturbation model $\xi_\phi (s,a)$, and two Q Networks $Q_{\theta_1}(s,a), Q_{\theta_2}(s,a)$.
\end{frame}


\begin{frame}{Experiments}
    \begin{columns}
        \begin{column}{0.45\textwidth}
            \includegraphics[width=1.0\textwidth]{figure2-1.png}
        \end{column}
        \begin{column}{0.45\textwidth}
            \includegraphics[width=1.0\textwidth]{figure2-2.png}
        \end{column}
    \end{columns}

    \smallskip
    \begin{itemize}
        \item BCQ is the only algorithm which succeeds at all tasks, matching or outperforming the behavioral policy in each instance, and outperforming all other agents.
        \item In the imperfect demonstrations task, we find that both deep reinforcement learning and imitation learning algorithms perform poorly. BCQ, however, is able to strongly outperforms the noisy demonstrator, disentangling poor and expert actions.
        \item Compared to current deep reinforcement learning algorithms, which can requires millions of time steps, BCQ attains a high performance in remarkably few iterations.
    \end{itemize}
\end{frame}

% \begin{frame}[t]{Experiments}
\begin{frame}{Experiments}
    \begin{figure}
        \centering
        \includegraphics[width=0.5\textwidth]{figure3.png}
        \vfill
    \end{figure}

    \begin{itemize}
        \item BCQ exhibits a highly stable value function in the presence of off-policy samples, suggesting extrapolation error has been successfully mitigated through the batch-constraint.
    \end{itemize}
\end{frame}


\end{document}