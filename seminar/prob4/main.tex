\documentclass[8pt]{beamer}
\usefonttheme[onlymath]{serif}


% 테마 선택 (선택 사항)
\usetheme{Madrid} % 기본 테마, 다른 테마 사용 가능
% \font{serif}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage[T1]{fontenc} % To use combination of textbf, textit


% \setcounter{MaxMatrixCols}{20}

% (필요한 패키지들)
% \usepackage{amsthm}
\setbeamertemplate{theorems}[numbered]  % 정리, 정의 등에 번호를 달아줌

% \theoremstyle{plain} % insert bellow all blocks you want in italic
% \newtheorem{theorem}{Theorem}[section] % to number according to section
% 
% \theoremstyle{definition} % insert bellow all blocks you want in normal text
% \newtheorem{definition}{Definition}[section] % to number according to section
% \newtheorem*{idea}{Proof idea} % no numbered block
\usepackage{tcolorbox}

% 필요할 경우 패키지 추가
\usepackage{graphicx} % 이미지 삽입을 위한 패키지
\usepackage{amsmath}   % 수식 사용
\usepackage{hyperref}  % 하이퍼링크 추가
\usepackage{cleveref}
\usepackage{multicol}  % 여러 열 나누기
\usepackage{ulem} % 취소선 및줄 나누기



\newcommand{\mrm}[1]{\mathrm{#1}}
\newcommand{\mbb}[1]{\mathbb{#1}}
\newcommand{\mb}[1]{\mathbf{#1}}
\newcommand{\mc}[1]{\mathcal{#1}}
\newcommand{\tb}[1]{\textbf{#1}}
\newcommand{\ti}[1]{\textit{#1}}
\newcommand{\mypois}[1]{\operatorname{Pois}(#1)}

\newcommand{\mybin}[2]{\operatorname{Bin}\!\left(#1,#2\right)}
\newcommand{\mytoinf}[1]{#1 \rightarrow \infty}
\newcommand{\myexp}[1]{\exp{\left(#1\right)}}
\newcommand{\myunif}[2]{\operatorname{Unif}\!\left(#1, #2\right)}

% 발표 제목, 저자, 날짜 설정
\title{Probability}
\author{Gwanwoo Choi}
% \date{}

\begin{document}
% 표지 슬라이드
\begin{frame}
    \titlepage
\end{frame}

% 목차 슬라이드
\begin{frame}{contents}
    \tableofcontents
\end{frame}

\begin{frame}{Continuous Random Variable}
    \begin{definition}[Continuous r.v.]
        An r.v. has a continuous distribution if its CDF is differentiable. We also allow there to be endpoints (or finitely many points) where the CDF is continuous but not differentiable, as long as the CDF is differentiable everywhere else. A continuous random variableis a random variable with a continuous distribution.
    \end{definition}

    \begin{definition}[Probability density function]
        For a continuous r.v. $X$ with CDF $F$, the probability density function (PDF) of $X$ is the derivative $f$ of the CDF, given by $f(X) = F^\prime(x)$. The support of $X$, and of its distribution, is the set of all $x$ where $f(x)>0$.
    \end{definition}

    \begin{block}{Proposition 3 (PDF to CDF)}
        Let $X$ be a continuous r.v. with PDF $f$. Then the CDF of $X$ is given by 
        \[F(x) = \int^x_{- \infty} f(t) dt\]
    \end{block}

\end{frame}

\begin{frame}{Continuous Rancom Variable}
    \begin{itemize}
        \item Note that $P(X=x) = 0, \forall x $  in continuous r.v. setting. We can discuss about probability with only integral of PDF.
        \item By the fact $P(X=x)=0, \forall x$, $P(a<X\leq b) = P(a\leq X <b) = P(a< X <b) = P(a \leq X \leq b)$
        \item Because the quantity of PDF $f(x)$ is not a probability, for some values of $x$, it is possible to have $f(x) > 1$.
        \item  Although the height of PDF directly represents the probability, it is closely related to the probability. For small $\epsilon$, 
        \[
            P(3 - \epsilon /2 < X \leq 3 + \epsilon/2 ) = \int_{3- \epsilon/2}^{3 + \epsilon/2} f(x) dx \approx f(x) dx
        \]
    \end{itemize}

    \begin{theorem}[Valid PDFs]
        The PDF $f$ of a continuous r.v. must satisfy the following two criteria:
        \begin{itemize}
            \item Nonnegative: $f(x)\leq 0$
            \item Integrates to $1$: $\int_{-\infty}^\infty f(x) dx = 1$
        \end{itemize}
    \end{theorem}
\end{frame}

\begin{frame}{Continuous Random Variable}
    \begin{example}
        \begin{itemize}
            \item Logistic 
            \[
                F(x; \mu, \sigma) = \frac{\myexp{\frac{x-\mu}{\sigma }}}{1 + \myexp{\frac{x-\mu}{\sigma }}}, \quad f(x; \mu, \sigma) = \frac{\myexp{\frac{x-\mu}{\sigma }}}{\sigma \left(1 + \myexp{\frac{x-\mu}{\sigma }}\right)^2}
            \]
            \item Rayleigh 
            \[
                F(x; \mu, \sigma) = 1  - \myexp{- \frac{(x-\mu)^2}{2 \sigma^2}}, x > \mu \quad f(x; \mu, \sigma) = \frac{x-\mu}{\sigma^2} \myexp{-\frac{(x-\mu)^2}{2 \sigma^2}}, x > 0
            \]
        \end{itemize}
    \end{example}

   

    \begin{definition}[Expectation of a continuous r.v.]
        The expected value (also called the expectation or mean) of a continuous r.v. $X$ with PDF $f$ is 
        \[
        E(X) = \int_{-\infty}^\infty x f(x) dx
        \]
    
    \end{definition}
\end{frame}

\begin{frame}{Continuous Random Variable}
    \begin{theorem}[LOTUS, continuous]
        If $X$ is a continuous r.v. with PDF $f$ and $g$ is a function from $\mathbb{R}$ to $\mathbb{R}$, then
        \[
            E[g(X)] = \int_{-\infty}^\infty g(x)f(x) dx
        \]
    \end{theorem}

    \begin{definition}[Uniform distribution]
        A continuous r.v. is said to have Uniform distribution on the interval $(a,b)$ if its PDF is
        \[
        f(x) = \begin{cases}
            \frac{1}{b-a}\quad \text{if } a<x<b, \\ 0 \quad \text{otherwise},
        \end{cases}
        \]
        This is denoted by $U \sim \myunif{a}{b} $
    \end{definition}

    PDF of uniform distribution $F(x)$ is defined as
    \[
        F(x) = \begin{cases}
            0 \quad \text{if } x\leq a,
            \\ \frac{x-a}{b-a} \quad \text{if } a < x <b,
            \\ 1 \quad \text{if } x \geq b,
        \end{cases}
    \]
\end{frame}

\begin{frame}{Continuous Random Variable}
    Expectation of uniform distribution $U \sim \myunif{a}{b}$ is simply calculated by 
    \[
    E[U] = \int_a^b \frac{x}{b-a} dx = \frac{1}{b-a} \frac{(b-a)^2}{2} = \frac{a+b}{2}
    \]

    $E[U^2]$ is calculated by using the continuous version of LOTUS
    \[
    E[U] = \int_a^b \frac{x^2}{b-a} dx = \frac{1}{b-a} \frac{1}{3}(b^3-a^3) = \frac{b^2 + ab + a^2}{3}
    \]

    By $Var[U] = E[U^2] - E[U]^2$,
    \[
        E[U] = \frac{(a-b)^2}{12}
    \]

\end{frame}

\begin{frame}{Continuous Random Variable}

    \begin{definition}[Location-scale transformation]
        Let $X$ be a random variable and $Y = \sigma X + \mu$, where $\mu$ and $\sigma$ are constants with $\sigma >0$. Then we say that $Y$ has been obtained as a \textit{Location-scale transformation} of $X$. Here $\mu$ controls how the location is changed and $\sigma$ controls how the scale is changed.
    \end{definition}

    \begin{example}
        In a location-scale trainsformation, starting with $X\sim \myunif{a}{b}$ and transforming it to $Y = cX + d$ where $c$ and $d$ are constants with $c >0$, $Y$ is a linear function of $X$ and Uniformity is preserved: $Y \sim \myunif{ca+d}{cb+d}$.  But if $Y$ is defined as a \textit{nonlinear} transformation of $X$, then $Y$ will \textit{not} be Uniform in general. For example, for $X \sim Unif(a,b)$ with $0 \leq a <b$, the transformed r.v. $Y=X^2$ has support $(a^2,b^2)$ but is \textit{not} Uniform on that interval.
    \end{example}



\end{frame}

\begin{frame}{Continuous Random variable}
    Let $U \sim \myunif{0}{1}$. Then 
    \[
    \begin{gathered}
        E[U] = \int_0^1 x dx = \frac{1}{2} \\
        E[U^2] = \int_0^1 x^2 dx = \frac{1}{3} \\
        Var[U] = E[U^2] - E[U]^2 = \frac{1}{3} - \frac{1}{4} = \frac{1}{12} 
    \end{gathered}
    \]
    
    With another uniform distribution $U^\prime \sim \myunif{a}{b}$, $U$ has relation $U^\prime = (b-a)U +a$.
    Then by locaion-scale transformation,
    \[
    \begin{gathered}
        E[U^\prime] = E[(b-a)U+a] = a + (b-a)E[U] = \frac{a+b}{2}\\
        Var[U^\prime] = Var[(b-a)U+a] = Var[(b-a)U] = \frac{(b-a)^2}{12}
    \end{gathered}
    \]

    Via location-scale transformation, it is quite easy to obtain $E[U^\prime]$ and $Var[U^\prime]$.

\end{frame}

\begin{frame}{Continuous Random Variable}
    The technique of location-scale transformation will work for any family of distributions such that shifting and scaling an r.v. whose distribution in the family produces another r.v. whose distribution is in the family. 

\bigskip

    This technique does not apply to families of \textbf{discrete distributions} (with a fixed support) since, for example, shifting or scaling $X \sim \mybin{n}{p}$ changes the support and produces an r.v. that is no longer Binomial. 
    \begin{itemize}
        \item  A Binomial r.v. must be able to take on all integer values between $0$ and some upper bound
        \item but $X + 4$ can't take on any value in $\{0, 1, 2, 3\}$ and $2X$ can only take even values, so neither of these r.v.s has a Binomial distribution.
    \end{itemize}
\end{frame}

\begin{frame}{Continuous Random Variable}
    \begin{theorem}[Universality of the Uniform]
        Let $F$ be a CDF which is a continuous function and strictly increasing on the support of the distribution. This ensures that the inverse function $F^{-1}: (0,1) \mapsto \mathbb{R}$ exist.($F^{-1}$ is sometimes called as the \textit{quantile function}) We than have the following results
        \begin{itemize}
            \item Let $U \sim \myunif{0}{1}$ and $X = F^{-1}$. Then $X$ is an r.v. with CDF $F$.
            \item Let $X$ be an r.v. with CDF $F$. Then $F(X) \sim \myunif{0}{1}$.
        \end{itemize}
    \end{theorem}

    \textit{Proof.}
    \begin{itemize}
        \item Let $U \sim \myunif{0}{1}$ and $X = F^{-1}$. For all real $x$,
         \[P(X \leq x) = P(F^{-1}(U)\leq x) = P(U \leq F(x)) = \int_0^{F(x)} 1 du =F(x)\]
        \item Let $X$ have CDF, and find the CDF of $Y = F(X)$. Since $Y$ takes values in $(0,1)$, 
        \begin{itemize}
            \item $P(Y\leq y)$ equals $0$ for $y \leq 0$
            \item $P(Y\leq y)$ equals $1$ for $y \geq 1$
            \item For $y \in (0,1)$, $P(Y \leq y) = P(F(X) \leq y) = P(X \leq F^{-1}(y)) = F(F^{-1}(y)) = y$
        \end{itemize}
        Thus $Y$ has the $\myunif{0}{1}$ CDF.
    \end{itemize}
    \textit{Recall}: PDF of uniform distribution $F(x)$ is defined as
    \[
        F(x) = \begin{cases}
            0 \quad \text{if } x\leq a,
            \\ \frac{x-a}{b-a} \quad \text{if } a < x <b,
            \\ 1 \quad \text{if } x \geq b,
        \end{cases}
    \]

\end{frame}

\begin{frame}{Continuous Random Variable}
    \begin{example}[Universality with Logistic]
        The Logistic CDF is $F = \frac{e^x}{1+e^x}, \forall x \in \mathbb{R}$ and $F^{-1} = \log{\frac{u}{1-u}}, \forall u \in (0,1)$. If we plug $U \sim \myunif{0}{1}$ in $F^{-1}$, then $F^{-1}(U) = \log{\frac{U}{1-U}}$.
        \[\forall x \in \mathbb{R},
        P\left(\log{\frac{U}{1-U}}\leq x\right) = P\left(U \leq \frac{e^x}{1+ e^x} \right)
        \]
        $\therefore \log{\frac{U}{1-U}} \sim$ Logistic. And also, $F(X) = \frac{e^X}{1+e^X} \sim \myunif{0}{1}$ 
    \end{example}
    \begin{figure}
        \centering
        \includegraphics[width=0.45\textwidth]{fig1.png}
    \end{figure}
\end{frame}

\begin{frame}{Continuous Random Variable}
    \begin{example}[Universality with Rayleigh]
        The Rayleigh CDF is $F = 1 - e^{- x^2/2}, \forall x \in \mathbb{R}$ and $F^{-1} = \sqrt{-2 \log{(1-u)}}, \forall u \in (0,1)$.
        $U \sim \myunif{0}{1} \implies \sqrt{-2 \log{(1-U)}} \sim $Layleigh, and $X \sim \text{Rayleigh} \implies F(X)=1-e^{-X^2/2} \sim \myunif{0}{1}$.
    \end{example}
    \begin{figure}
        \centering
        \includegraphics[width=0.52\textwidth]{fig2.png}
    \end{figure}
\end{frame}

\begin{frame}{Continuous Random Variable}
    Is Universality of the Uniform can be applied to discrete r.v.? Partially, yes.
    For discrete r.v., it's CDF function doesn't have inverse $F^{-1}$ because $F$ of discrete r.v. has jumps and flat regions. We can instead using PMF.

    Suppose we want to use $U \sim \myunif{0}{1}$ to construct a discrete r.v. $X$ with PMF $p_j = P(X=j)$ for $j=0,1,2,\dots,n$.
    Consider the following trick that chop up the interval $(0,1)$ into pieces of lengths $p_0, p_1, \dots, p_n$
    
    \begin{figure}
        \centering
        \includegraphics[width=0.6\textwidth]{fig3.png}
    \end{figure}

    The probability $P(X=j)$ is the probabiliy that $U$ falls into the interval of length $p_j$. But for a $\myunif{0}{1}$ r.v., probability is length, so $P(X=j)$ is precisely $p_j$, as desired

\bigskip
    For discrete r.v.,
    \begin{itemize}
        \item Let $U \sim \myunif{0}{1}$ and $X = F^{-1}$. Then $X$ is an r.v. with CDF $F$.
        \item \sout{Let $X$ be an r.v. with CDF $F$. Then $F(X) \sim \myunif{0}{1}$.}
    \end{itemize}


\end{frame}

\begin{frame}{Continuous Random Variable}
    \begin{definition}[Survival function]
        The survival function of an r.v. $X$ with CDF $F$ is the function $G$ given by $G(x) = 1 - F(x) = P(X>x)$.
    \end{definition}
    \begin{theorem}[Expectation by integrating the survival function]
        Let $X$ be a \textbf{nonnegative} r.v. Its expectation can be found by integrating its survival function:
        \[
        E[X] = \int_0^\infty P(X>x)dx
        \]
    \end{theorem}

    Proof. Let $I(x>t)$ be $1$ if $x\geq t$ and $0$ otherwise. Then $\forall x > 0$, $x = \int_0^x dt = \int_0^\infty I(x>t) dt$. So $X = \int_0^\infty I(X>t) dt$. Taking the expectation of both sides and swapping the $E$ with the integral (which can be justified using results from real analysis), we have
    \[E[X] = E\left[\int_0^\infty I(x>t)dt \right] = \int_0^\infty E[I(X>t)]dt  = \int_0^\infty P(X>t)dt\].
\end{frame}

\begin{frame}{Continuous Random Variable}
    \begin{figure}
        \centering
        \includegraphics[width=0.63\textwidth]{fig4.png}
    \end{figure}

    For $U \sim \myunif{0}{1}$, \[\int_0^1 F^{-1}(u) du = E_{U}[F^{-1}(U)]= E[X]\]

\end{frame}

\subsection{Normal Distribution}

\begin{frame}
    \frametitle{Table of Contents}
    \tableofcontents[currentsection]
\end{frame}

\begin{frame}{Normal Distribution}
    \begin{definition}[Standard Normal distribution]
        A continuous r.v. $Z$ is said to have the \textit{Standard Normal distribution} if its PDF $\varphi$ is given by
        \[\varphi (z) = \frac{1}{\sqrt{2\pi}} e^{-z^2/2}, Z \in \mathbb{R}\]
        We write this as $Z \sim \mathcal{N}(0,1)$
        The Standard Normal CDF $\Phi$ is the accumulated area under the PDF
        \[ \Phi(z) = \int^z_{-\infty} \varphi(t) dt = \int^z_{-\infty}\frac{1}{\sqrt{2\pi}}e^{-t^2/2}dt\]
    \end{definition}
\end{frame}

\begin{frame}{Normal Distribution}
    Check that $\varphi$ satisfies property of PDF, $\int_{-\infty}^\infty \varphi =1$. First, calculate $\int_{-\infty}^\infty e^{-z^2/2}dz$.
    \[
    \begin{gathered}
        \left( \int_{-\infty}^\infty e^{-z^2/2} dz \right)\left( \int_{-\infty}^\infty e^{-z^2/2} dz \right) =  \left( \int_{-\infty}^\infty e^{-x^2/2} dx \right)\left( \int_{-\infty}^\infty e^{-y^2/2} dy \right)\\
        = \int_{-\infty}^\infty \int_{-\infty}^\infty e^{-\frac{x^2 + y^2}{2}} dx dy = \int_{0}^{2\pi} \int_{0}^{\infty} e^{-r^2 /2} r dr d\theta = \int_{0}^{2\pi} \int_0^\infty e^{-u} du d\theta =\int_0^{2\pi} 1 d\theta = 2\pi  \\
        \therefore  \left( \int_{-\infty}^\infty e^{-z^2/2} dz \right) = \sqrt{2\pi}
    \end{gathered}
    \]
    So,
    \[
    \int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi}}e^{-z^2/2} dz = 1
    \]
    And $\varphi$ is a valid PDF.

\end{frame}

\begin{frame}{Normal Distribution}
    How can we calculate mean and variance of Standard Normal distribution?
    \[E[Z] = \int_{-\infty}^\infty \frac{1}{\sqrt{2\pi}} z e^{-z^2/2} dz = -\int_{\infty}^0 \frac{1}{\sqrt{2\pi}} (-z)e^{-(-z)^2/2} dz + \int_0^\infty \frac{1}{\sqrt{2\pi}}ze^{-z^2/2}dz = 0\]
    Also, variance of $Z$ can be calculated by
    \[
    \begin{gathered}
        Var[Z] = E[Z^2] - E[Z]^2 = E[Z^2]\\
        E[Z^2] =  \frac{2}{\sqrt{2\pi}}\int_0^{\infty}z^2 e^{-z^2/2} dz = \frac{2}{\sqrt{2\pi}}\int_0^\infty z\cdot ze^{-z^2/2}dz
    \end{gathered}
    \]
\end{frame}


\end{document}