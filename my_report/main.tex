\documentclass[
	10pt, % Set the default font size, options include: 8pt, 9pt, 10pt, 11pt, 12pt, 14pt, 17pt, 20pt
	%t, % Uncomment to vertically align all slide content to the top of the slide, rather than the default centered
	%aspectratio=169, % Uncomment to set the aspect ratio to a 16:9 ratio which matches the aspect ratio of 1080p and 4K screens and projectors
]{article}
\usepackage{booktabs} % Allows the use of \toprule, \midrule and \bottomrule for better rules in tables
\usepackage{amsmath} % Automatically numbers and align the equations
\usepackage{amssymb} % to use \becuase and \therefore
\usepackage{amsfonts} % To use mathematical fonts like mathbb
\usepackage{amsthm} % To use theorem
\usepackage[linesnumbered]{algorithm2e} % use algorithm
\usepackage[usenames,dvipsnames]{color} % to use textcolor
\usepackage{float} %to use, in \begin{algorithm}[H], [H] option
\usepackage[hidelinks]{hyperref} % hide red rectangle on reference. hyperref is used packaged as default.
\usepackage{cleveref} % clearly reference equation in text
\usepackage{braket} % braket
\usepackage{lmodern} %font for equation
\usepackage[default]{opensans} % Use the Open Sans font for sans serif 
\usepackage{bm} %access bold symbols in math modes, can be replaced by boldmath
\RestyleAlgo{ruled} % caption style for algorithm2e
% cleveref 설정: algorithm 환경을 인식하도록 설정
% \crefname{algorithm}{Algorithm}{Algorithms}
% \Crefname{algorithm}{Algorithm}{Algorithms}
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
\SetKwInOut{Init}{Initialize}


% setting about amsthm
% \newtheorem{theorem}{Theorem}[subsection]
\newtheorem{theorem}{Theorem}[section]
% \newtheorem{definition}{Definition}[subsection]
\newtheorem{definition}{Definition}[section]
% \newtheorem{lemma}{Lemma}[section]
% \newtheorem{corollary}{Corollary}[theorem]
\crefname{algorithm}{Algorithm}{Algorithms}
\Crefname{algorithm}{Algorithm}{Algorithms}
\crefname{theorem}{Theorem}{Theorems}
\Crefname{theorem}{Theorem}{Theorems}
\crefname{definition}{Definition}{Definitions}
\Crefname{definition}{Definition}{Definitions}
% \crefname{lemma}{Lemma}{Lemmas}
% \Crefname{lemma}{Lemma}{Lemmas}
\theoremstyle{plain}



\def\tcr{\textcolor{red}}
\def\tcb{\textcolor{blue}}
\def\n{\newline}

\newcommand{\mrm}[1]{\mathrm{#1}}
\newcommand{\mbb}[1]{\mathbb{#1}}
\newcommand{\mb}[1]{\mathbf{#1}}
\newcommand{\mc}[1]{\mathcal{#1}}
\newcommand{\tb}[1]{\textbf{#1}}
\newcommand{\ti}[1]{\textit{#1}}

\numberwithin{equation}{subsection} % command in amsmath package
% define argmax argmin (https://tex.stackexchange.com/questions/5223/command-for-argmin-or-argmax)
% \DeclareMathOperator*{\argmax}{arg\,max} % not good
% \newcommand{\argmax}{\mathop{\mathrm{arg\,max}}\limits}
% \newcommand{\argmin}{\mathop{\mathrm{arg\,min}}\limits}
\newcommand{\argmax}{\mathop{\mathrm{argmax}}\limits}
\newcommand{\argmin}{\mathop{\mathrm{argmin}}\limits}



% \setlength{\parindent}{15pt} % set indent length
\title{Reinforcement Learning basic study}
\author{Gwanwoo Choi}

\begin{document}
\maketitle
\newpage
\tableofcontents
\newpage
\section{Markov Process}
In probability theory and statistics, a markov process is a stochastic process describing a sequence of possible events in which the probability of each event depends only on the state attained in the previous event.
If stochastic process satisfies \cref{eq:eq1}, then that is \tb{markov process} and state $S_t$ is \tb{markov}.
\begin{equation} \label{eq:eq1}
    \mbb{P}[S_{t+1}|S_t] = \mbb{P}[S_{t+1}|S_t,S_{t-1},\dots,S_{1}]
\end{equation}
For a markov state $s$ and successor state $s'$, the state transition probability is defined by \cref{eq:eq2}.
\begin{equation} \label{eq:eq2}
    \mc{P}_{ss'} = \mbb{P}[S_{t+1} = s' | S_t = s]
\end{equation}
Let $\mc{S}$ be a set of all states and $\mc{P}_{ss'}$ be a set of all state-transition probability.
 \ti{markov process} is uniquely determined by a tuple $\braket{\mc{S}, \mc{P}}$. 


\subsection{Markov Reward Process}


$R_t$ is called \tb{reward} for corresponding state $S_t$. \tb{Return} $G_t$ is defined by \cref{eq:eq3}.   
\begin{equation} \label{eq:eq3}
    G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots = \sum^{\infty}_{k=0}\gamma^k R_{t+k+1}
\end{equation}
where $\gamma \ \in [0,1] $ is called \tb{discount factor}.
$\mc{R}_s$ is defined by \cref{eq:eq4}
\begin{equation} \label{eq:eq4}
    \mc{R}_s = \mbb{E}[R_{t+1}|S_t=s]
\end{equation}
And let denote $\mc{R}$ as set of all $\mc{R}_s$ for all $s \in \mc{S}$. Then \tb{markov reward process} is uniquely determined by a tuple $\braket{\mc{S}, \mc{P}, \mc{R}, \gamma}$.

The \tb{value funtion} $v(s)$ represents long-term value of state s. \ti{value function} is defined by \cref{eq:eq5}.
\begin{equation} \label{eq:eq5}
    v(s) = \mbb{E}[G_t|S_t=s]
\end{equation}


\subsection{Markov Decision Process}
$A_t$ is called \tb{action}. For each time step $t$, action $A_t$ is elements of the set $\mc{A}$ ($\forall t, A_t \in \mc{A}(s) \text{ where } s \in \mc{S}$). This action set can be both finite and infinite set. Unlike transition probability in \ti{markov process}, \tb{transition probability} is newly deefined in markov decision process as \cref{eq:eq8}. Also \tb{reward function} is newly defined as \cref{eq:eq9}.
\begin{equation} \label{eq:eq8}
    \mc{P}_{ss'}^a = \mbb{P}[S_{t+1}=s'|S_t=s, A_t=a]
\end{equation}
\begin{equation} \label{eq:eq9}
    \mc{R}_{s}^a = \mbb{E}[R_{t+1}|s_{t}=s,A_t=a]
\end{equation}
similarly with markov process and markov reward process, markov decision process is uniquely determined by a tuple $\braket{\mc{S}, \mc{A}, \mc{P}, \mc{R}, \gamma}$

Now we can define the probability distribution over actions given states, named \tb{policy} $\pi$ as \cref{eq:eq10}.
\begin{equation} \label{eq:eq10}
    \pi(a|s) = \mbb{P}[A_t=a|S_t=s]
\end{equation}
A policy fully defines the behaviour of agent.
\n

Gieven an MDP $\mc{M}=\braket{\mc{S, A, P, R}, \gamma}$ and a policy $\pi$, $\braket{\mc{S}, \mc{P}^\pi}$ is \ti{MP} and $\braket{\mc{S},\mc{P}^\pi, \mc{R}^\pi, \gamma}$ is \ti{MRP}. $\mc{P}^\pi$ is set of all $\mc{P}^\pi_{s,s'}$ which is defined by \cref{eq:eq11}.
\begin{equation} \label{eq:eq11}
    \mc{P}^\pi_{s,s'} = \sum_{a \in \mc{A}}\pi(a|s)\mc{P}^a_{ss'}
\end{equation}
And $R^\pi$ is set of all $R^pi_s$ which is defined by \cref{eq:eq12}
\begin{equation} \label{eq:eq12}
    \mc{R}^\pi_s = \sum_{a \in \mc{A}}\pi(a|s)\mc{R}^a_s
\end{equation}
\n

In MDP, we can define \tb{value function} as new manner by \cref{eq:eq13}.
\begin{equation} \label{eq:eq13}
    v_\pi(s) = \mbb{E}_\pi[G_t|S_t=s]
\end{equation}
Also, \tb{action-value function} $q_\pi(s,a)$, which is the expected return starting from state s, taking action a, and then following policy $\pi$, is defined by \cref{eq:eq14}.
\begin{equation} \label{eq:eq14}
    q_\pi(s,a) = \mbb{E}_\pi[G_t|S_t=s, A_t=a]
\end{equation}
\n

\section{Bellman Equation}
\subsection{Bellman Equation for MRP and MDP}
The \tb{bellman equation}, which tells returns can be decomposed into immediate reward and discounted (action) value of successor state, is described as below.
\cref{eq:eq6} represents \ti{Bellman equation for MRPs}.
\begin{equation} \label{eq:eq6}
    \begin{aligned}
        v(s) &= \mbb{E}[G_{t+1}|S_t = s] \\
        &= \mbb{E}[R_{t+1} + \gamma v(S_{t+1})|S_t = s] \\
        &= \mc{R}_s + \gamma \sum_{s' \in \mc{S}} \mc{P}_{ss'}v(s')
    \end{aligned}
\end{equation}
value function can be calculated by utilizing $\gamma, \mc{P} \text{ and } \mc{R}$. 
In vector formulation, value funtion can be obtained by \cref{eq:eq7}.
\begin{equation} \label{eq:eq7}
    v = (I - \gamma \mc{P})^{-1}\mc{R}
\end{equation}

\cref{eq:eq15} represents \ti{Bellman equation} for \ti{MDP}.
\begin{equation} \label{eq:eq15}
    \begin{aligned} 
        v_\pi(s)&=\mbb{E}_\pi[R_{t+1}+\gamma v_\pi(S_{t+1})|s_t=s]\\
        &=\sum_{a \in \mc{A}}\pi(a|s)\left(\mc{R}^a_{s}+\gamma\sum_{s' \in \mc{S}}\mc{P}^a_{ss'}v_\pi(s')\right)\\
        q_\pi(s,a)&=\mbb{E}_\pi[R_{t+1}+\gamma q_\pi(S_{t+1},A_{t+1})|S_t=s,A_t=a]\\
        &=\mc{R}^a_s+\gamma \sum_{s' \in \mc{S}}\mc{P}_{ss'}^a \sum_{a' \in \mc{A}} \pi(a'|s')q_\pi(s',a')
    \end{aligned}
\end{equation}
this value function and action value function can be expressed mixedly.
\begin{equation} \label{eq:eq16}
    \begin{gathered}
        v_\pi(s) = \sum_{a \in \mc{A}}\pi(a|s)q_\pi(s,a)\\
        q_\pi(s,a) = \mc{R}^a_s + \gamma\sum_{s' \in \mc{S}}\mc{P}^a_{ss'}v_\pi(s')
    \end{gathered}
\end{equation}

Similarly with \cref{eq:eq7}, bellman equation related with a specific policy can be expressed by \cref{eq:eq19}
\begin{equation} \label{eq:eq19}
    v_\pi = \mc{R}^\pi + \gamma \mc{P}^\pi v_\pi\\
\end{equation}
And we can get direct solution with process such as \cref{eq:eq20}
\begin{equation} \label{eq:eq20}
    v_\pi = (I - \gamma \mc{P}^\pi)^{-1} \mc{R}^\pi
\end{equation}

The time complexity is $\mc{O}(n^3)$ for calculating direct solution where $n$ is $|\mc{S}|$.
So direct solution is suitable for small MRPs (where $n$ is small).
There exists indirect iterative methods such as \ti{DP}, \ti{MC} and \ti{TD} which is suitable for calculating solution larger MRPs.

\subsection{Optimal Value Functions}
Value functions define a partial ordering over policies. A policy $\pi$ is said to be better than or equal to a policy $\pi'$ (i.e. $\pi \geq \pi'$) if and only if $v_\pi(s) \geq v_{\pi'}(s)$.
\tb{Optimal state-value function} for all state $s$ is defined by \cref{eq:eq17}.
\begin{equation} \label{eq:eq17}
    v_*(s) = \max_{\pi} v_\pi(s)
\end{equation}
\tb{Optimal action-value function} for all state and action pair $(s, a)$ is defined bt \cref{eq:eq18}
\begin{equation} \label{eq:eq18}
    % \begin{aligned}
    q_*(s,a) = \max_\pi q_\pi(s,a)
    % &=\mbb{E}[R_{t+1} + \gamma v_*(S_{t+1})|S_t=s,A_t=a]
    % \end{aligned}
\end{equation}

\subsection{Bellman Optimality Equation}
The \tb{bellman optimality equation} are special consistency conditions that the optimal value functions must satisfy and that can, in principle, be solved for the optimal value functions, from which an optimal policy can be determined with relative ease. Bellman optimality equation for state value function can be expressed like \cref{eq:eq21}
\begin{equation} \label{eq:eq21}
    \begin{aligned}
        v_*(s) &= \max_{a}q_{\pi_*}(s,a)\\
        &=\max_a \mc{R}^a_{s} + \gamma \sum_{s' \in \mc{S}}\mc{P}^a_{ss'}v_*(s')
    \end{aligned}
\end{equation}
And bellman optimality equation for action value function can be expressed like \cref{eq:eq22}
\begin{equation} \label{eq:eq22}
    \begin{aligned}
        q_*(s,a) &= \mc{R}^a_{s} + \gamma \sum_{s' \in \mc{S}} \mc{P}^a_{ss'}v_*(s') \\
        &= \mc{R}^a_{s} + \gamma \sum_{s' \in \mc{S}} \mc{P}^a_{ss'} \max_{a'} q_*(s',a')
    \end{aligned}
\end{equation}



Whereas the \ti{optimal value functions} for states and state-action pairs are \ti{unique} for states and state-action pairs are unique, there can be \ti{many optimal policies}.

\subsection{Optimal Policy}
There is always at least one policy that is better than or equal to all other policies. This is an \tb{optimal policy}. All optimal policies achieve the optimal state value function, $\bm{v_{\pi_*}(s) = v_*(s)}$ for all state $s$ and the optimal action value function $\bm{q_{\pi_*}(s,a) = q_*(s,a)}$ for all state and action pair $(s,a)$. In other words, \ti{any policy that is greedy} with respect to the optimal value function $v_*$ is an \ti{optimal policy}.

An \ti{optimal policy} $\pi_*$ can be found by maximising over $q_*(s,a)$,
\begin{equation}
    \pi_*(a|s) = 
    \begin{cases}
        1 & \text{if } a = \argmax_{a \in \mc{A}}q_*(s,a) \\
        0 & \ti{otherwise}
    \end{cases}
\end{equation}
if we know $q_*(s,a)$ we immediate have the optimal policy.
\newpage



\section{Dynamic Programming}
In this chapter, we only consider about deterministic policies, although all properties and equations can also be able to applied for stochastic policy $\pi(a|s)$.
\subsection{Policy Evaluation}
Suppose there exists a MDP $\braket{\mc{S,A,P,R}, \gamma}$ and a given policy $\pi$. How we can get value function for specific policy? Throught \tb{Policy evaluation}, also called as \tb{prediction problem}, we can evaluate policy.

If we know the environment's dynamics, then we can consider \cref{eq:eq6} as a system of $|\mc{S}|$ linear equations of $|S|$ unknowns. In other words, by solving that $|\mc{S}|$ number of linear equations we can get value function for a policy $\pi$. However solving such tremendous linear system involves tedious calculations and can lead to lack of computation resources. For our purpose, iterative solution methods are most suitable. \tb{Iterative update process (Jacobi's method)} for state value function can be expressed by \cref{eq:eq23}.
\begin{equation} \label{eq:eq23}
    \begin{aligned}
        v_{k+1}(s) &= \mbb{E}_\pi [R_{t+1} + \gamma v_k(S_{t+1}) | S_t = s] \\
        &= \sum_{a \in \mc{A}(s)} \pi(a|s)\left(\mc{R}^a_s + \gamma\sum_{s' \in \mc{S}}\mc{P}^a_{ss'}v_{k}(s') \right)
    \end{aligned}
\end{equation}

Note that not only Jacobi's method, but also Gauss-Seidal, SOR(Successive over relaxation) and other iterative methods can be used for policy evaluation (prediction problem).

\subsection{Policy Improvement}
Suppose we have determined the value function $v_\pi$ for a policy for an arbitrary policy $\pi$. Assume another policy $\pi'$ satisfies inequation $q_\pi(s, \pi'(s)) \geq v_\pi(s)$, for all $s \in \mc{S}$. Then following inequation \cref{eq:eq24} is satisfied.
\begin{equation} \label{eq:eq24}
    \begin{aligned}
        v_\pi(s) &\leq q_\pi(s, \pi'(s))\\
        &=\mbb{E}[R_{t+1} + \gamma v_\pi(S_{t+1}) | S_t=s, A_t=\pi'(s)]\\
        &=\mbb{E}_{\pi'}[R_{t+1} + \gamma v_\pi (S_{t+1}) | S_{t}=s]\\
        &\leq \mbb{E}_\pi'[R_{t+1} + \gamma q_\pi(S_{t+1}, \pi'(S_{t+1}))|S_t=s]\\
        &= \mbb{E}_{\pi'}[R_{t+1} + \gamma\mbb{E}_{\pi'}[R_{t+2} + \gamma v_\pi(S_{t+1})|S_{t+1}]|S_t=s]\\
        &= \mbb{E}_{\pi'}[R_{t+1}+\gamma R_{t+2} + \gamma^2 v_\pi (S_{t+2}) |S_t=s] \quad \text{(by law of iterated expectation)}\\
        &\vdots\\
        &\leq \mbb{E}_{\pi'}[G_{t+1}|S_t=s]\\
        &= v_{\pi'}(s)
    \end{aligned}
\end{equation}

And for every policy $\pi$, we can always find better or equal policy $\pi'$. Once we get value function $v_\pi$ for $\pi$, $\pi'$ defined by \cref{eq:eq25} is always better than $\pi$.
\begin{equation} \label{eq:eq25}
    \pi'(s) = \argmax_a q_\pi(s,a)
\end{equation}
This implies new greedy policy for value function of original policy is always better than original policy and The process of deriving a new policy using the aforementioned method is called \tb{policy improvement}.

If new policy $\pi'$ derived from original policy $\pi$ satisfies bellman optimality equation (\cref{eq:eq26}), then both $\pi'$ and $\pi$ are optimal policy.
\begin{equation} \label{eq:eq26}
    v_{\pi'}(s) = \max_a \mbb{E}[R_{t+1} + \gamma v_{\pi'}(S_{t+1}) | S_t=s, A_t=a]
\end{equation}

\subsection{Policy Iteration}
We can recursively update the value function and policy in sequence manner like \cref{eq:eq27}.
\begin{equation} \label{eq:eq27}
    \pi_0 \xrightarrow{\mrm{E}} v_{\pi_0} \xrightarrow{\mrm{I}} \pi_1 \xrightarrow{\mrm{E}} v_{\pi_1} \xrightarrow{\mrm{I}} \dots \xrightarrow{\mrm{E}} \pi_*
\end{equation}
Terminate when newly updated policy $\pi_{k+1}$ is eqaul to $\pi_k$ and this policy $\pi_{k+1}$ becomes optimal policy $\pi_*$. This process used to obtain optimal policy $\pi_*$ is called \tb{policy iteration}.

\subsection{Value Iteration}
Waiting to complete policy evaluation at each step of policy iteration can lead to wasted time. In fact, the policy evaluation step in policy iteration can be truncated in several ways without losing the convergence guarantees of policy iteration. \tb{Value iteration} truncates the policy evaluation step to just \ti{one step}, ensuring fast convergence. The update equation for value iteration can be written as a simple one-line equation (\cref{eq:eq28}) that combines policy improvement and the one-step truncated policy evaluation step.
\begin{equation} \label{eq:eq28}
    v_{k+1}(s) = \max_a \mbb{E} [R_{t+1} + \gamma v_k(S_{t+1}) | S_t=s, A_t=a]
\end{equation}
\newpage



\section{Model-free Prediction}
\tb{On-policy} methods attempt to evaluate or improve the policy that is used to make decisions, whereas \tb{off-policy} methods evaluate or improve a policy different from that used to generate the data.
\subsection{On-policy Monte Carlo Prediction}
Suppose we don't know about environments. Can we evaluate policy although environments transition probability and following rewards are unknown? In this case, \ti{sampling} sequences of states, actions, and rewards from actual or simulated interaction with an environment can be used to estimate actual value function.

\tb{Monte carlo} methods utilizes averaging sample returns under the episodic tasks, that well-defined returns are available, to estimate value functions. In monte carlo prediction, each occurrence of state $s$ in an episode is called a \ti{visit} to $s$. \tb{First-visit MC method} estimates $v_\pi(s)$ as the average of the returns following first visit to s. \cref{alg:alg1} describes whole process of First visit MC prediction.

\begin{algorithm}
\SetAlgoLined  % default
% \SetAlgoNoLine % no line
\Input{a policy $\pi$ to be evaulated}
\Init{$V(s) \in \mbb{R}$, arbitrarily, for all $s \in \mc{S} \n Returns(s) \gets \text{an empty list, for all } s \in \mc{S}$
}


\While{sample episode}{
    Generate an episode following $\pi$: $S_0, A_0, R_1, S_1, A_1, R_2, \dots, S_{T-1}, A_{T-1}, R_{T}$

    $G \gets 0$\

    \For{$t = T-1, \dots, 0$}{
        $G \gets \gamma G + R_{t+1}$\

        \If{$S_t$ has not been visited in $S_0, S_1, \dots, S_{t-1}$}{
            $\text{Returns}(S_t) \gets \text{Returns}(S_t) \cup \{G\}$\
            
            $V(S_t) \gets \text{average}(\text{Returns}(S_t))$\
            
        }
    }
}

\Output{estimated value function $V(s)$, for all $s \in \mc{S}$}

\caption{On-policy first-visit MC value function prediction}
\label{alg:alg1}
\end{algorithm}

First visit MC converges to $v_\pi(s)$ as the number of visits to $s$ goes infinity. In MC methods, each estimate for each state are independent. In other words, estimate for one state is not derived by the estimate of any other state (not \ti{bootstrap}). Each return is considered as an independent, identically distributed estimate of $v_\pi(s)$ with finite variance.

We can express this algorithm in incremental manner. Both \cref{alg:alg1} and \cref{alg:alg2} are essentially same algorithm.
\begin{algorithm}
\Input{a policy $\pi$ to be evaluated}
\Init{$V(s) \in \mbb{R}$, arbitrarily, for all $s \in \mc{S}$ \n
$N(s) \in \mbb{N} \cup {0}, N(s) \gets 0$, for all $s \in \mc{S}$
}

\While{sample episode}{
    Generate an episode following $\pi$: $S_0, A_0, R_1, S_1, A_1, R_2, \dots, S_{T-1}, A_{T-1}, R_{T}$

    $G \gets 0$\

    \For{$t = T-1, \dots, 0$}{
        $G \gets \gamma G + R_{t+1}$\

        \If{$S_t$ has not been visited in $S_0, S_1, \dots, S_{t-1}$}{
            $N(S_t) \gets N(S_t) + 1$\

            $V(S_t) \gets V(S_t) + \frac{1}{N(S_t)}(G-V(S_t))$
        }
    }
}

\Output{estimated value function $V(s)$, for all $s \in \mc{S}$}

\caption{On-Policy first-visit incremental MC value function prediction}
\label{alg:alg2}
\end{algorithm}
 
Incremental monte carlo algorithm for the \tb{non-stationary problem} \cref{alg:alg3} is slightly different from \cref{alg:alg2}. In non-stationary environment, true underlying value function changes over time. So, it is important to forget too previous estimated values in non-stationary problem and weights recent estimated values. \cref{alg:alg3} uses \ti{constant-$\alpha$} as a constant step-size parameter to forget too old stored values. In \cref{alg:alg2} estimated values in each states are ordinal averaged, other side, in \cref{alg:alg3}, estimaved values in each states are \tb{exponential recency-weighted averaged}.

\begin{algorithm}[H]
    \Input{a policy $\pi$ to be evaluated\n$\alpha \in \mbb{R}^+$, constant stepsize parameter}
    \Init{$V(s) \in \mbb{R}$, arbitrarily, for all $s \in \mc{S}$
    }
    
    \While{sample episode}{
        Generate an episode following $\pi$: $S_0, A_0, R_1, S_1, A_1, R_2, \dots, S_{T-1}, A_{T-1}, R_{T}$
    
        $G \gets 0$\
    
        \For{$t = T-1, \dots, 0$}{
            $G \gets \gamma G + R_{t+1}$\
    
            \If{$S_t$ has not been visited in $S_0, S_1, \dots, S_{t-1}$}{
                $V(S_t) \gets V(S_t) + \alpha(G-V(S_t))$
            }
        }
    }

    \Output{estimated value function $V(s)$, for all $s \in \mc{S}$}

\caption{On-policy first-visit incremental constant-$\alpha$ MC value function prediction}
\label{alg:alg3}
\end{algorithm}

\subsection{Off-policy Monte Carlo Prediction}
In off-policy control, the \tb{target policy}, which is being learned, is different from the \tb{behavior policy}, which is used to generate behavior. 
In this case we say that learning is from data \ti{off} the target policy, and the overall process is termed \tb{off-policy learning}.
In off-policy learning, we should weight the return $G_t$ while considering the difference between the sample distributions of the target policy and the behavior policy, called \tb{importance sampling}.
Let denote target policy as $\pi$ and behavior policy as $\mu$, then weighted return can be expressed by \cref{eq:eq40}
\begin{equation} \label{eq:eq40}
    G^{\pi / \mu}_{t} = \frac{\pi(A_t|S_t)\pi(A_{t+1}|S_{t+1})}{\mu(A_t|S_t)\mu(A_{t+1}|S_{t+1})}\dots\frac{\pi(A_T|S_T)}{\mu(A_{T-1}|S_{T-1})}G_t
\end{equation}
And updating value function is progressed by \cref{eq:eq41}.
\begin{equation} \label{eq:eq41}
    V(S_t) \gets V(S_t) + \alpha (G^{\pi / \mu}_t - V(S_t))
\end{equation}
This equation can not be used when $\mu$ is zero although $\pi$ is non-zero.
Importance sampling can dramatically increase variance.

\begin{algorithm}[H]
    \Input{an arbitrary target policy $\pi$}
    \Init{$Q(s,a) \in \mbb{R}$ (arbitrary)\n
    $N(s,a) \gets 0$
    }

    \While{each episode}{
        $\mu \gets$ any policy with coverage of $\pi$\

        Generate an episode following $\mu: S_0, A_0, R_1, \dots , S_{T-1},A_{T-1},R_T$\

        $G \gets 0; W \gets 1;$\

        \For{each time step of episode, $t=T-1, T-2,\dots, 0$}{
            \If{$W = 0$}{
                \tb{Break}
            }
            \If{$(S_t, A_t)$ pair has not been visited in $(S_0, A_0), (S_1, A_1), \dots, (S_{t-1}, A_{t-1})$}{
                $N(S_t, A_t) \gets N(S_t, A_t) + 1$\

                $Q(S_t, A_t) \gets Q(S_t,A_t) + \frac{1}{N(S_t,A_t)}[WG - Q(S_t,A_t)]$
            }
            $W \gets W \frac{\pi(A_t|S_t)}{\mu(A_t|S_t)}$
        }
    }
\caption{Off-policy first-visit MC prediction (Ordinal importance sampling)}
\label{alg:alg7}
\end{algorithm}

\cref{alg:alg7} shows the procedure that action value function is evaluated by off-policy with ordinal importance sampling manner.
Whereas \cref{alg:alg8} shows the procedure by every-visit with weighted important sampling manner. 

\begin{algorithm}[H]
    \Input{an arbitrary target policy $\pi$}
    \Init{$Q(s,a) \in \mbb{R}$ (arbitrary)\n
    $C(s,a) \gets 0$
    }

    \While{each episode}{
        $\mu \gets$ any policy with coverage of $\pi$\

        Generate an episode following $\mu: S_0, A_0, R_1, \dots , S_{T-1},A_{T-1},R_T$\

        $G \gets 0; W \gets 1;$\

        \For{each time step of episode, $t=T-1, T-2,\dots, 0$}{
            \If{$W = 0$}{
                \tb{Break}
            }{}
            $G \gets \gamma G + R_{t+1}$\

            $C(S_t, A_t) \gets C(S_t,A_t)+W$\

            $Q(S_t,A_t) \gets Q(S_t,A_t) + \frac{W}{C(S_t,A_t)}[G - Q(S_t,A_t)]$\

            $W \gets W \frac{\pi(A_t|S_t)}{\mu(A_t|S_t)}$
        }
    }
\caption{Off-policy every-visit MC prediction (weighted important sampling)}
\label{alg:alg8}
\end{algorithm}

Both of these dimensions—namely, every-visit vs. first-visit and ordinary vs. weighted importance sampling—correspond to existing algorithm variants. 
Therefore, an appropriate combination can be chosen depending on the situation and objectives.

% \tb{\textcolor{red}{need to fill importance sampling MC in here.}}

\subsection{Temporal Difference Prediction}
Monte Carlo methods help estimate value function even in situations where there is no knowledge about the environment (model-free). But drawback of monte carlo methods is that must wait until the episode is finished.

By combining the property of dynamic programming with monte carlo method, we can adress the shortcomings of monte carlo methods, which require waiting for end of an episode.

In simple manner, we can express non-stationary monte carlo update by \cref{eq:eq29}.
\begin{equation} \label{eq:eq29}
    V(S_t) \gets V(S_t) + \alpha (G_t - V(S_t))
\end{equation}
we need to wait for the episode to be finished in order to update value function in \cref{eq:eq29}. However, in \cref{eq:eq30}, we only need to wait for a single step to finish.
\begin{equation} \label{eq:eq30}
    V(S_t) \gets V(S_t) + \alpha (R_{t+1} + \gamma V(S_{t+1}) - V(S_t))
\end{equation}
\cref{eq:eq30} updates estimated value function using the observed reward $R_{t+1}$ and the estimate $V(S_{t+1})$. This method is called \tb{TD(0)} or \tb{one-step TD} because it only requires one step further estimated values. $R_{t+1} + \gamma V(S_{t+1})$ means one step further estimated values and TD(0) algorithm updates toward one step further estimated values by adding $R_{t+1}+\gamma V(S_{t+1})$ with multiplied by $\alpha$. Because TD(0) bases its update in part on an existing estimate, we say that it is a \tb{bootstrapping} method, like DP. $\delta_t = R_{t+1} + \gamma V(S_{t+1}) - V(S_t)$ is called \tb{TD(0) error}. \cref{alg:alg4} shows whole process of TD(0) algorithm.

\begin{algorithm}[H]
\Input{a policy $\pi$ to be evaluated}
\Init{$V(s) \in \mbb{R}$, arbitrarily, for all $s \in \mc{S}$ \n $\alpha \in \mbb{R}^+$, constant stepsize parameter
}

\ForEach{each episode}{
    Initialize $S \in \mc{S}$\

    \While{$S$ is non-terminal state}{
        $A \gets \text{ action given by } \pi \text{ for } S$\

        Take action $A$, observe $R, S'$\

        $V(S) \gets V(S) + \alpha [R + \gamma V(S') - V(S)]$\

        $S \gets S'$
    }
}

\Output{estimated value function $V(s)$, for all $s \in \mc{S}$}
\caption{On-policy TD(0) V prediction}
\label{alg:alg4}
\end{algorithm}

In monte carlo methods, target $G_t=R_{t+1}+\gamma R_{t+2}+ \dots + \gamma^{T-t-1}R_T$ is \tb{unbiased estimates} of $v_\pi(S_t)$. 
However, in TD(0), target $R_{t+1}+\gamma V(S_{t+1})$ is biased estimated of $v_\pi(S_t)$, in that sense \bm{$\mbb{E}[R_{t+1} + \gamma V(S_{t+1})|S_t] \neq \mbb{E}[G_{t+1}|S_t] = v_\pi(S_t)$}. 
MC has high \ti{variance} because return $R_{t+1}$ depends on \ti{many} random choices, transitions, rewards but TD(0) has \ti{lower bias} than MC because TD(0) target only depends on \ti{one} random action, transition, rewards. 
MC is \ti{robust} to initial value, whereas TD(0) is \ti{sensitive} to initial value.

In this subsection, prediction for action value runction is not discussed. It will be dealed with TD control in \cref{sec:model-free control} because Q-prediction algorithm (policy evaluation) is naturally combined and discussed with policy improvement.

\subsection{Batch MC, TD(0)}
Suppose there is available only a finite episode or timestep. In this case, there is lack of experience to converge $v_\pi$. To handle this situation, one method is repeatedly updating using finite experiences.

Given an approximation value functoin $V$, for every time step $t$, increments are computed and $V$ is updated once by the sum of the increments. And with new value function $V'$ we repeat this update process again until value function converges. This is called \tb{batch updating}. 

Batch monte carlo methods always find the estimates that minimize \ti{mean square error} on the training set, whereas batch TD(0) always finds the estimates that would be exactly correct for the \ti{maximum likelihood} model of the markov process.

Batch monte carlo methods tries to minimize equation \cref{eq:eq31} whereas batch TD(0) methods tries to match with solution of maximum likelihood of markov model by \cref{eq:eq32}. In other words, TD(0) converges to solution of maximum likelihood MDP $\braket{\mc{S,A,\hat{P}, \hat{R}},\gamma}$ which best fits the training data.

\begin{equation} \label{eq:eq31}
    \sum^K_{k=1} \sum^{T_k}_{t=1} \left(G^k_t - V(S^k_t)\right)^2
\end{equation}

\begin{equation} \label{eq:eq32}
    \begin{aligned}
        \hat{\mc{P}}^a_{s,s'} &= \frac{1}{N(s,a)} \sum^K_{k=1} \sum^{T_k}_{t=1}\mathbf{1}(s^k_t,a^k_t,s^k_{t+1} = s, a, s')\\
        \hat{\mc{R}^a_s} &= \frac{1}{N(s,a)} \sum^K_{k=1} \sum^{T_k}_{t=1}\mathbf{1}(s^t_k,a^t_k = s,a)r^k_t
    \end{aligned}
\end{equation}

\subsection{Generalized Temporal Difference: N-step Bootstrapping}
In monte carlo methods, \tb{target of the update} $G_t$ is defined by \cref{eq:eq33}, which is unbiased sample of truth return value. Whereas TD(0) uses $G_{t:t+1}$ (\cref{eq:eq34}) as target of the update, which is one-step further estimates for true return, combined with on one-step forward rewards $R_{t+1}$ and estimated value function $V(\cdot)$. Recall that update value of TD(0) is defined by $G_{t:t+1} - V(S_t)$.
\begin{equation} \label{eq:eq33}
    G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots + \gamma^{T-t-1} R_T
\end{equation}
\begin{equation} \label{eq:eq34}
    G_{t:t+1} = R_{t+1} + \gamma V(S_{t+1})
\end{equation}

% In n-step boot strapping, where $n = \lambda$, TD($\lambda$) generally means that update with $\delta^\lambda_t = G^\lambda_t - V(S_t)$. $G^\lambda_t$ is defined by \cref{eq:eq35}
% \begin{equation} \label{eq:eq35}
%     G^\lambda_t = R_{t+1} + \gamma R_{t+2} + \dots \gamma^\lambda R_{t+\lambda+1} + \gamma^{\lambda + 1} V(S_{t+\lambda+1})
% \end{equation}

In n-step bootstrapping, target of update is \tb{n-step return} $G_{t:t+n}$ (\cref{eq:eq35}) and $V(\cdot)$ is updated with $G_{t:t+n} - \gamma^n V(S_{t+n})$. Update formula for \tb{n-step bootstrapping} (\tb{n-step TD}) is defined by \cref{eq:eq36}. Note that n-step returns for $n > 1$ involve future rewards and states that are not available at the time of transition from $t$ to $t+1$.
\begin{equation} \label{eq:eq35}
    G_{t:t+n} = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots \gamma^{n-1} R_{n} + \gamma^n V(S_{t+n})
\end{equation}

\begin{equation} \label{eq:eq36}
    V(S_t) \gets V(S_t) + \alpha\left(G_{t:t+n} - V(S_t)\right)
\end{equation}

In n-step return, following \cref{eq:eq38} is satisfied. This is called the \ti{error reduction property} of n-step returns. Because of the reduction property, one can show formally that all n-step TD methods converge to the correct predictions under appropriate technical conditions.
\begin{equation} \label{eq:eq38}
    \max_s\left|\mbb{E}_\pi [G_{t:t+n}|S_t=s]-v_\pi(s)\right| \leq \gamma^n \max_s\left|V_{t+n-1}(s) - v_\pi(s)\right|
\end{equation}


\subsection{TD($\lambda$)}
To leverage both benefits of MC and TD(0), intermediate bootstrapping steps between MC and TD(0) can be combined, named as \tb{TD($\lambda$)}. $\lambda$ return $G^\lambda_t$ combines all n-step returns $G_{t:t+n}$ by \cref{eq:eq37}, where lower step return is more considered. If $\lambda = 0$, then return is same with of TD(0) and if $\lambda = 1$, then return is same with of MC.
\begin{equation} \label{eq:eq37}
    G^\lambda_t = (1-\lambda) \sum_{n=1}^\infty \lambda^{n-1}G_{t:t+n}, \quad 0 \leq \lambda \leq 1
\end{equation}


\section{Model-free Control} \label{sec:model-free control}

\begin{definition} \label{def:def1}
    We say a sequence of policies $(\pi_k)$ is \emph{Greedy in the Limit with Infinite Exploration (GLIE)} if it satisfies the following conditions:

\begin{itemize}
    \item \tb{All state-action pairs are explored infinitely many times.} 
    \[
        \lim_{k \to \infty} N_k(s, a) = \infty
    \]

    \item \tb{The policy converges to a greedy policy.}
    \[
        \lim_{k \to \infty} \pi_k(a|s)
        = 1\Bigl(a = \argmax_{a' \in \mathcal{A}} Q_k(s, a')\Bigr).
    \]
\end{itemize}
\end{definition}

\begin{theorem} \label{thm:thm2}
    For any $\epsilon$-greedy policy $\pi$, the $\epsilon$-greedy policy $\pi'$ with respect to $q_\pi$ is an improvement, i.e. $v_{\pi'} \geq v_\pi$
\end{theorem}

\begin{proof}
\begin{equation} \label{eq:eq39}
    \begin{aligned}
        q_\pi(s, \pi'(s))
        &= \sum_{a \in \mc{A}} \pi'(a|s)q_\pi(s,a) \\
        &= \epsilon/m \sum_{a \in \mc{A}} q_\pi(s,a) + (1-\epsilon) \max_{a \in \mc{A}}q_\pi(s,a) \quad (m=|\mc{A}(s)|) \\
        &\geq \epsilon / m \sum_{a \in \mc{A}}q_\pi(s,a) + (1-\epsilon)\sum_{a \in \mc{A}}\frac{\pi(a|s) - \epsilon / m}{1 - \epsilon}q_\pi(s,a)\\ 
        &\quad \left(\because \sum_{a \in \mc{A}}\frac{\pi(a|s) - \epsilon /m}{1-\epsilon} =1, \pi(a|s) - \epsilon/m \geq 0, \forall s,a \right)\\
        &= \sum_{a \in \mc{A}}\pi(a|s)q_\pi(s,a) \\
        &= v_\pi(s)
    \end{aligned}
\end{equation}
\end{proof}

Note that $q_\pi(s, \pi'(s))$ is \tb{one-step lookahead} for action value function $q_\pi$. 
\cref{thm:thm2} shows that if one-step lookahead action value function is greater than or equal to previous $q_\pi$, then changing policy in every step from $\pi$ to $pi'$ is improvement (i.e. $v_{\pi'}(s) \geq v_\pi(s)$)
\subsection{On-policy MC Control}
% On-policy MC control methods can be divided by two different algorithms. One is exploring starts, which is usually used where agent can start in random initial state. 
% Another is without exploring starts, which is start with fixed initial state $S_0$.
We can find both on-policy and off-policy control methods on both MC and TD. First we deal with on-policy MC control methods.
In on-policy control methods, the policy is generally \ti{soft}, meaning that $\pi(a|s) > 0$ for all $s \in \mc{S}$, but gradually shifted closer and closer to a deterministic optimal policy. 
\tb{$\epsilon$-greedy} policy is method that choose an action that has maximal estimated action value but with probability $\epsilon$ they instead select an action at random. 
Algorithm \cref{alg:alg5} improves policy as $\epsilon$-greedy.

\begin{algorithm} 
\Init{$\pi(s) \in \mc{A}(s)$, arbitrarily, for all $s \in \mc{S}$ \n $Q(s,a) \in \mbb{R}$, arbitrarily, for all $s \in \mc{S}, a \in \mc{A}(s)$
\n
$N(s, a) \gets 0$, for all $s \in \mc{S}$ and for all $a \in \mc{A}(s)$
}\
\While{True}{
    Sample $k$th episode using $\pi$ : $S_0, A_0, R_0, S_1, A_1, ..., S_{T_1}, A_{T-1}, R_T$ \ 
    $G \gets 0$

    \For{$t = T-1, \dots, 0$}{
        $G \gets \gamma G + R_{t+1}$
    
        \If{$(S_t, A_t)$ pair has not been visited in $(S_0, A_0), (S_1, A_1), \dots, (S_{t-1}, A_{t-1})$}{
            $N(S_t, A_t) \gets N(S_t, A_t) + 1$\

            $Q(S_t, A_t) \gets Q(S_t, A_t) + \frac{1}{N(S_t, A_t)}(G - Q(S_t, A_t))$

        }
    }
    $\pi \gets \epsilon\text{-greedy}(Q)$
}
\caption{On-policy Monte Carlo control (first-visit method)}
\label{alg:alg5}
\end{algorithm}

In algorithm \cref{alg:alg5}, policy improvement and policy evaluation occur alternately, one after the other. 
% But does the $\epsilon$-greedy update really satisfy the policy improvement theorem? 
According to \ti{Generalized Policy Iteration (GPI)}, the improved policy $\pi'$ should satisfy $\pi' \geq \pi$, where $\pi$ is the policy before the update. 
In this case, improved $\epsilon$-greedy policy $\pi'$ is greater than original policy $\pi$.
% \cref{eq:eq39} shows the fact that $v_{\pi'}(s) \geq v_\pi(s)$.
% In the inequality, the general fact that "the maximum value in a probability distribution function is always greater than or equal to any probability-weighted average" is used.



\subsection{Off-policy MC Control}
In off-policy MC control, TD target $R + \gamma V(S')$ is weighted by importance sampling. 
Let denote $\pi$ as target policy and $\mu$ as behavior policy. 
Only need is a single importance sampling correction, expressed by \cref{eq:eq48}.
This leads much lower variance than MC importance sampling because step length is much more less than MC. And behavior policy $\mu$ only needs to be similar with target policy $\pi$ over a single step, which is convenient. 

\begin{equation} \label{eq:eq48}
    V(S_t) \gets V(S_t) + \alpha \left(\frac{\pi(A_t|S_t)}{\mu(A_t|S_t)}(R_{t+1}+\gamma V(S_{t+1}))-V(s_t)\right)
\end{equation}

\begin{algorithm}[H]
    \Input{$\alpha \in (0,1]$}
    \Init{$Q(s,a) \in \mbb{R}$, arbitrarily, for all $s \in \mc{S}, a \in \mc{A}(s)$
    \n
    $C(s, a) \gets 0$, for all $s \in \mc{S}$ and for all $a \in \mc{A}(s)$\n
    $\pi(s) \gets \argmax_a Q(s,a)$, for all $s \in \mc{S}, a \in \mc{A}(s)$ (with ties broken consistently for maintaining deterministic)\n
    }\
    \ForEach{episode}{
        $\mu \gets$ any soft policy (e.g. $\epsilon$-greedy)\

        generate an episode using $\mu:S_0, A_0, R_0, S_1, A_1, ..., S_{T_1}, A_{T-1}, R_T$ \

        $G \gets 0; W \gets 1;$
    
        \For{$t = T-1, \dots, 0$}{
            $G \gets \gamma G + R_{t+1}$\

            $C(S_t,A_t) \gets C(S_t,A_t) + W$

            $Q(S_t,A_t) \gets Q(S_t,A_t) + \alpha\frac{W}{C(S_t,A_t)}[G - Q(S_t,A_t)]$\

            $\pi(S_t) \gets \argmax_a Q(S_t,a)$ (with ties broken consistently)\

            \If{$A_t \neq \pi(S_t)$}{
                \tb{Break}
            }
            $W \gets W\frac{1}{\mu(A_t|S_t)}$ ($\pi(A_t|S_t) = 1$)
        }
    }
    \Output{$\pi(\cdot)$ (with ties broken consistently)}
\caption{Off-policy Monte Carlo control (every-visit method)}
\label{alg:alg9}
\end{algorithm}

\cref{alg:alg9} describes off-policy control algorithm \ti{in terms of action value function}. 
In \cref{alg:alg9}, target policy is greedy, so that importance sampling ratio is $\frac{1}{\mu(A_t|S_t)}$.
It uses weighted importance sampling methods and one can replace it to ordinal importance sampling. 

\subsection{On-policy TD control: Sarsa}
In on-policy TD control, we learn action value function by \cref{eq:eq42}.
\begin{equation} \label{eq:eq42}
    Q(S_t,A_t) \gets Q(S_t,A_t) + \alpha (R_{t+1} + \gamma Q(S_{t+1},A_{t+1}) - Q(S_t,A_t))
\end{equation}
In this equation, we only need $(S_t,A_t,R_{t+1}, S_{t+1}, A_{t+1})$ in each iteration, so this technique is called \tb{sarsa}.
\cref{alg:alg6} shows the whole process of \tb{on-policy TD(0) action value function control}.
It uses only one policy, typically $\epsilon$-greedy, to visit and update state-action pair.

\begin{algorithm}
    \Input{
        step size $\alpha \in (0,1]$, small $\epsilon > 0$
    }
    \Init{
        $Q(s,a)$, for all $s \in \mc{S}^+, a \in \mc{A}(s)$, arbitrary except that $Q(\text{terminal-state}, \cdot)$
    }

    \ForEach{each episode}{
        Initialize $S \in \mc{S}$\

        Choose $A$ from $S$ using policy derived from $Q$ (e.g. $\epsilon$-greedy)\

        \While{$S$ is non-terminal state}{
            Take action $A$, observe $R, S'$\

            Choose $A'$ from $S'$ using policy derived from $Q$ (e.g. $\epsilon$-greedy)\

            $Q(S,A) \gets Q(S,A) + \alpha [R + \gamma Q(S',A') - Q(S,A)]$\ \label{eq:eq43}

            $S \gets S'; A \gets A'$
        }
    }
    \Output{
        $\pi \gets$ greedy policy for $Q(\cdot, \cdot)$
    }
\caption{On-policy TD(0) control (sarsa)}
\label{alg:alg6}
\end{algorithm}
This sarsa algorithm converges under conditions in \cref{thm:thm1}.
In MC control, policy is updated in every episode, whereas in \cref{alg:alg6},TD control, policy is updated \tb{in every step}.

\begin{theorem} \label{thm:thm1}
    Sarsa converges to the optimal action-value function,
    \[ Q(s,a) \to q_*(s,a), \]
    under the following conditions:
    \begin{itemize}
        \item {\bf GLIE sequence of policies} \(\pi_t(a|s)\). 
        \item {\bf Robbins-Monro sequence of step sizes} \(\alpha_t\) such that
        \[
            \sum_{t=1}^{\infty} \alpha_t = \infty
            \quad \text{and} \quad
            \sum_{t=1}^{\infty} \alpha_t^2 < \infty.
        \]
    \end{itemize}
\end{theorem}

GLIE(Greedy in the Limit with Infinite Exploration) is defined by \cref{def:def1}.

We can also discuss \tb{n-step sarsa} algorithm. By changing \cref{eq:eq43} of algorithm \cref{alg:alg6} as \cref{eq:eq44}, We can simply get n-step sarsa control.
\begin{equation} \label{eq:eq44}
    Q(S_t,A_t) \gets Q(S_t,A_t) + \alpha (q^n_t - Q(S_t,A_t))
\end{equation}
Where $q^n_t$, the n-step Q-return, is defined in \cref{eq:eq46}
\begin{equation} \label{eq:eq46}
    q^n_t = R_{t+1} + \gamma R_{t+2}+ \gamma^2 R_{t+3} + \dots + \gamma^{n-1} R_{t+n} + \gamma^n Q(S_{t+n}, A_{t+n})
\end{equation}

% We can also define \tb{Sarsa($\lambda$)}, which combines all n-step Q-returns $q^n_t$. By changing \cref{eq:eq43} of algorithm \cref{alg:alg6} as \cref{eq:eq45}, we can get sarsa($\lambda$).
% \begin{equation} \label{eq:eq45}
%     \begin{gathered}
%     q^\lambda_t = (1-\lambda)\sum_{n=1}^\infty \lambda^{n-1}q^n_t \\
%     Q(S_t,A_t) \gets Q(S_t,A_t) + \alpha (q^\lambda_t - Q(S_t, A_t))
%     \end{gathered}
% \end{equation}

\subsection{Off-policy TD control: Q-learning}
Q-learning is the representative off-policy TD(0) control algorithm. 
In Q-learning, the target policy $\pi$ is \ti{greedy} with respect to the current Q-function (estimated for optimal  $p^*$), while the behavior policy $\mu$ follows an \ti{$\epsilon$-greedy} strategy.

\begin{algorithm}
    \Input{
        step size $\alpha \in (0,1]$, small $\epsilon > 0$
    }
    \Init{
        $Q(s,a)$, for all $s \in \mc{S}^+, a \in \mc{A}(s)$, arbitrary except that $Q(\text{terminal-state}, \cdot)$
    }

    \ForEach{each episode}{
        Initialize $S$\

        \While{$S$ is non-terminal state}{
            Choose $A$ from $S$ using policy derived from $Q$ (e.g. $\epsilon$-greedy)\

            Take action $A$, observe $R, S'$\

            $Q(S,A) \gets Q(S,A) + \alpha[R + \gamma \max_a Q(S',a) - Q(S,A)]$\ \label{eq:eq47}

            $S \gets S'$
        }
    }
    \Output{
        $\pi \gets$ greedy policy for $Q(\cdot, \cdot)$
    }
\caption{Off-policy TD(0) Control: Q-learning}
\label{alg:alg10}
\end{algorithm}

In Algorithm \cref{alg:alg10}, importance sampling is not required. 
This is because the target state-action pair—obtained by greedily selecting an action from the current Q estimates (see Equation \cref{eq:eq47})—is used directly for the update. 
Importance sampling is normally used to correct for differences between the distribution of samples generated by the behavior policy and that of the target policy. 
However, in Q-learning the target update always uses the greedy action, so the samples from the behavior policy are not reweighted.

In Q-learning, \ti{policy evaluation and policy improvement occur at every iteration}. 
At each step, the updated Q-function is used to derive a greedy (or $\epsilon$-greedy) policy that selects the next action a for the successor state $S'$ as well as the current action A for state S. 
Since the Q-function that underlies the greedy policy is updated at every iteration, the policy is effectively improved with each update. 
Moreover, policy evaluation is carried out at every iteration via the update in \cref{eq:eq47}.
Therefore, the Q-learning algorithm in \cref{alg:alg10} can be regarded as a form of generalized policy iteration (GPI) and is inherently a control algorithm.

\textcolor{red}{Need to implement n-step TD}

\section{Value Function Approximation}
In large MDPs, there are too many states and actions to store in memory.
It leads too slow to learn the value of each state individually. 
To solve this problem, in large MDPs, estimate value function with \ti{function approximator} such as \tb{neural network} or \tb{linear combinations of features}. 
This approximater estimates the true value function such as $\hat{v}(s, \mb{w}) \approx v_\pi(s)$ or $\hat{q}(s,a,\mb{w}) \approx q_\pi(s,a)$.
In both neural network and linear combinations of features methods, parameter $\mb{w}$ is updated using MC or TD learning by \tb{gradient descent}.

We need to find parameter vector $\mb{w}$ minimising mean-squared error between approximate value function $\hat{v}(s, \mb{w})$ and true value function $v_\pi(s)$.
By gradient descent for objective function $J(\mb{w})$ defined in \cref{eq:eq49}.
\begin{equation} \label{eq:eq49}
    J(\mb{w}) = \mbb{E}_\pi[(v_\pi(s) - \hat{v}(s, \mb{w}))^2]
\end{equation}
In gradient descent method, $\Delta \mb{w}$ is defined by \cref{eq:eq50}.
\begin{equation} \label{eq:eq50}
\begin{aligned}
    \Delta \mb{w} &= -\frac{1}{2} \alpha \nabla_\mb{w} J(\mb{w})\\
    &= \alpha \mbb{E}_\pi [(v_\pi(s) - \hat{v}(s, \mb{w}))\nabla_\mb{w} \hat{v}(s,\mb{w})]
\end{aligned}
\end{equation}
In stochastic gradient descent method, $\Delta \mb{w}$ is defined slightly differently (\cref{eq:eq51}).
\begin{equation} \label{eq:eq51}
    \Delta \mb{w} = \alpha (v_\pi(s) - \hat{v}(s, \mb{w})) \nabla_\mb{w} \hat{v}(s, \mb{w})
\end{equation}
And also target $v_\pi(s)$ is substituted according to each methods. For MC, target becomes to be return $G_t$. (\cref{eq:eq54})
\begin{equation} \label{eq:eq54}
    \Delta \mb{w} = \alpha (G_t - \hat{v}(s, \mb{w})) \nabla_\mb{w} \hat{v}(s, \mb{w})
\end{equation}
For TD(0), the target becomes $r + \gamma \hat{v}(s', \mb{w})$. (\cref{eq:eq55})
\begin{equation} \label{eq:eq55}
    \Delta \mb{w} = \alpha (r + \gamma \hat{v}(s', \mb{w}) - \hat{v}(s, \mb{w})) \nabla_\mb{w} \hat{v}(s, \mb{w})
\end{equation}
FOr TD($\lambda$), the target is the $\lambda$-return $G^\lambda_t$. (\cref{eq:eq56})
\begin{equation} \label{eq:eq56}
    \Delta \mb{w} = \alpha (G^\lambda_t - \hat{v}(s, \mb{w})) \nabla_\mb{w} \hat{v}(s, \mb{w})
\end{equation}


\subsection{Linear Function Approximation}
In linear function approximation methods, state is represented by a \tb{featuer vector}, $\mb{x}(s) = \begin{pmatrix}
\mb{x}_1(s) \\ \vdots \\ \mb{x}_n(s)
\end{pmatrix}$. Vaue function approximation is defined by $\hat{v}(s,\mb{w}) = \mb{x}(s)^\top \mb{w}$.
In linear function approximation, function approximator $\hat{v}(s,\mb{w})$ of \cref{eq:eq49} is replaced by $\mb{x}^\top\mb{w}$, so defined by \cref{eq:eq52}
\begin{equation} \label{eq:eq52}
    J(\mb{w}) = \mbb{E}_\pi[(v_\pi(s) - \mb{x}(s)^\top \mb{w})^2]
\end{equation}

In linear function approximation, stochastic gradient method guarantees convergence on \tb{global optimum}. 
Update rule is defined by \cref{eq:eq53}
\begin{equation} \label{eq:eq53}\
    \begin{aligned}
    \nabla_\mb{w} \hat{v}(s,\mb{w}) &= \mb{x}(s) \\
    \Delta \mb{w} &= \alpha (v_\pi(s) - \hat{v}(s, \mb{w}))\mb{x}(s)
    \end{aligned}
\end{equation}

In MC, $v_\pi(s)$ of \cref{eq:eq53} becomes to $G_t$. (\cref{eq:eq57})
\begin{equation} \label{eq:eq57}
    \Delta \mb{w} = \alpha (G_t - \hat{v}(s, \mb{w}))\mb{x}(s)
\end{equation}.
In TD(0), $v_\pi(s)$ of \cref{eq:eq53} becomes to $r$. (\cref{eq:eq58})
\begin{equation} \label{eq:eq58}
    \Delta \mb{w} = \alpha (r + \gamma \hat{v}(s', \mb{w}) - \hat{v}(s, \mb{w}))\mb{x}(s)
\end{equation}
In TD($\lambda$), $v_\pi(s)$ is replaced with $G^\lambda_t$. (\cref{eq:eq59})
\begin{equation} \label{eq:eq59} 
    \Delta \mb{w} = \alpha (G^\lambda_t - \hat{v}(s, \mb{w})\mb{x}(s))
\end{equation}
\textcolor{red}{Need to implement backward view of TD($\lambda$)}.

\tb{control with action value function approximation} method adopts approximating policy evaluation $\hat{q}(\cdot, \cdot, \mb{w}) \approx q_\pi$ as policy evaluation and $\epsilon$-greedy policy imrovement as policy improvement.

Similar with value function approximation, approximate for the action value function is written as $\hat{q}(s, a, \mb{w}) \approx q_\pi(s,a)$.

Objective function $J(\mb{w})$ with action value function approximation $\hat{q}(s, a, \mb{w})$ is defined by \cref{eq:eq60}.

\begin{equation} \label{eq:eq60}
    J(\mb{w}) = \mbb{E}_\pi[(q_\pi(s, a) - \hat{q}(s, a, \mb{w}))^2]
\end{equation}

State and action pair is represented by a feature vector.

\begin{equation} \label{eq:eq62}
    \mb{x}(s,a) = \begin{pmatrix}
        \mb{x}_1(s,a)\\\vdots\\\mb{x}_n(s,a)
    \end{pmatrix}
\end{equation}

Update rule of SGD for action value function is defined by \cref{eq:eq61}
\begin{equation} \label{eq:eq61}
    \begin{aligned}
        -\frac{1}{2} \nabla_\mb{w} J(\mb{w}) &= (q_\pi(s,a) - \hat{q}(s, a, \mb{w})) \nabla_\mb{w}\hat{q}(s, a, \mb{w}) \\
        \Delta \mb{w} &= \alpha (q_\pi(s, a) - \hat{q}(s, a, \mb{w})) \nabla_\mb{w} \hat{q}(s, a, \mb{w})\\
        \Delta \mb{w} &= \alpha (q_\pi(s, a) - \hat{q}(s, a, \mb{w}))\mb{x}(s,a)
    \end{aligned}
\end{equation}

Prediction for action value function approximation methods replaces target $q_\pi(s,a)$ to other terms matched with each methods.
In MC, target $q_\pi(s,a)$ in \cref{eq:eq61} is replaced by $G_t$.
\begin{equation} \label{eq:eq63}
    \begin{aligned}
        \Delta \mb{w} &= \alpha (q_\pi(s, a) - \hat{q}(s, a, \mb{w})) \nabla_\mb{w} \hat{q}(s, a, \mb{w})\\
        \Delta \mb{w} &= \alpha (G_t - \hat{q}(s, a, \mb{w}))\mb{x}(s,a)
    \end{aligned}
\end{equation}
In TD(0), target $q_\pi(s,a)$ in \cref{eq:eq61} is replaced by $r + \gamma \hat{q}(s', a', \mb{w})$.
\begin{equation} \label{eq:eq64}
    \begin{aligned}
        \Delta \mb{w} &= \alpha (q_\pi(s, a) - \hat{q}(s, a, \mb{w})) \nabla_\mb{w} \hat{q}(s, a, \mb{w})\\
        \Delta \mb{w} &= \alpha (r + \gamma \hat{q}(s', a', \mb{w})- \hat{q}(s, a, \mb{w}))\mb{x}(s,a)
    \end{aligned}
\end{equation}
In TD($\lambda$), target $q_\pi(s,a)$ is replaced by action-value $\lambda$-return $q^\lambda_t$.
\begin{equation} \label{eq:eq65}
    \begin{aligned}
        \Delta \mb{w} &= \alpha (q_\pi(s, a) - \hat{q}(s, a, \mb{w})) \nabla_\mb{w} \hat{q}(s, a, \mb{w})\\
        \Delta \mb{w} &= \alpha(q^\lambda_t - \hat{q}(s, a, \mb{w}))\mb{x}(s,a)
    \end{aligned}
\end{equation}
\textcolor{red}{Need to implement backward view of TD($
\lambda$).}

\subsection{Batch Method}
For the experience $\mc{D} = \{\braket{s_1,v_1^\pi}, \braket{s_2,v_2^\pi}, \dots, \braket{s_T, v_T^\pi}\}$, to find best fitting parameter $\mb{w}$, \tb{least squares} algorithms find parameters $\mb{w}$ minimising sum-squared error between $\hat{v}(s_t, \mb{w})$ and target value $v_\pi(s)$.
\begin{equation} \label{eq:eq66}
    \begin{aligned}
    \text{LS}(\mb{w}) &= \sum_{t=1}^T (v^\pi_t - \hat{v}(s, \mb{w}))^2\\
    &= \mbb{E}_\mc{D} [(v^\pi - \hat{v}(s,\mb{w}))^2]
    \end{aligned}
\end{equation}

SGD with experience with experience replay algorithm repeats below processes.
\begin{enumerate}
    \item Sample state, value pair from experience
    $$\braket{s, v^\pi} \sim \mc{D}$$
    \item Apply SGD update
    $$\Delta \mb{w} = \alpha(v^\pi - \hat{v}(s, \mb{w}))\nabla_\mb{w} \hat{v}(s, \mb{w})$$
\end{enumerate}
This algorithm converges to least square solution. $$\mb{w}^\pi = \argmin_\mb{w} \text{LS}(\mb{w})$$

\tb{Deep Q-learning} algorithm uses experience replay and fixed Q-targets.
Store transition $(s,a,r,s')$ in $\mc{D}$.

Objective function $\mc{L}_i(w_i)$ is defined by \cref{eq:eq67}.
\begin{equation} \label{eq:eq67}
    \mc{L}_i(w_i) = \mbb{E}_{s,a,r,s' \sim \mc{D}_i} \left[\left(r + \gamma \max_{a'}Q(s',a',w_i^-)-Q(s,a,w_i)^2\right)\right]
\end{equation}
In this equation, subscript $i$ represents the iteration index. The update paramter $w_i$, the fixed parameter $w_i^-$ and the experience replay $\mc{D}_i$ changes according to update iteration index. 
$\mc{D}_i$ appends new experience and remove old experience as the index increases.
In each iteration, fixed parameter $w_i^-$ is used to calculate loss $\mc{L}_i$ to stabilize the target. fixed parameter $w_i^-$ is periodically updated as $w_i^- \gets w_i$.


\end{document}
