@article{agarwalReinforcementLearningTheorya,
  title = {Reinforcement {{Learning}}: {{Theory}} and {{Algorithms}} (Kakade {{RL Theory}})},
  author = {Agarwal, Alekh and Jiang, Nan and Kakade, Sham M and Sun, Wen},
  langid = {english},
  file = {/home/unist/Zotero/storage/NJIXKX3J/Agarwal et al. - Reinforcement Learning Theory and Algorithms.pdf}
}

@misc{anDirectPreferencebasedPolicy2023a,
  title = {Direct {{Preference-based Policy Optimization}} without {{Reward Modeling}}},
  author = {An, Gaon and Lee, Junhyeok and Zuo, Xingdong and Kosaka, Norio and Kim, Kyung-Min and Song, Hyun Oh},
  year = {2023},
  month = oct,
  number = {arXiv:2301.12842},
  eprint = {2301.12842},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2301.12842},
  url = {http://arxiv.org/abs/2301.12842},
  urldate = {2025-04-06},
  abstract = {Preference-based reinforcement learning (PbRL) is an approach that enables RL agents to learn from preference, which is particularly useful when formulating a reward function is challenging. Existing PbRL methods generally involve a two-step procedure: they first learn a reward model based on given preference data and then employ off-the-shelf reinforcement learning algorithms using the learned reward model. However, obtaining an accurate reward model solely from preference information, especially when the preference is from human teachers, can be difficult. Instead, we propose a PbRL algorithm that directly learns from preference without requiring any reward modeling. To achieve this, we adopt a contrastive learning framework to design a novel policy scoring metric that assigns a high score to policies that align with the given preferences. We apply our algorithm to offline RL tasks with actual human preference labels and show that our algorithm outperforms or is on par with the existing PbRL methods. Notably, on high-dimensional control tasks, our algorithm surpasses offline RL methods that learn with ground-truth reward information. Finally, we show that our algorithm can be successfully applied to fine-tune large language models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/unist/Zotero/storage/NBGQUFSY/An et al. - 2023 - Direct Preference-based Policy Optimization without Reward Modeling.pdf;/home/unist/Zotero/storage/QICYBVP5/2301.html}
}

@article{bagnellCovariantPolicySearch,
  title = {Covariant {{Policy Search}}},
  author = {Bagnell, J Andrew and Schneider, Jeff},
  journal = {Robotics Institute Carnegie-Mellon University Pittsburgh, PA 15213},
  abstract = {We investigate the problem of non-covariant behavior of policy gradient reinforcement learning algorithms. The policy gradient approach is amenable to analysis by information geometric methods. This leads us to propose a natural metric on controller parameterization that results from considering the manifold of probability distributions over paths induced by a stochastic controller. Investigation of this approach leads to a covariant gradient ascent rule. Interesting properties of this rule are discussed, including its relation with actor-critic style reinforcement learning algorithms. The algorithms discussed here are computationally quite efficient and on some interesting problems lead to dramatic performance improvement over noncovariant rules.},
  langid = {english},
  file = {/home/unist/Zotero/storage/RKJ6UV2X/Bagnell and Schneider - Robotics Institute Carnegie-Mellon University Pittsburgh, PA 15213.pdf}
}

@misc{baLayerNormalization2016a,
  title = {Layer {{Normalization}}},
  author = {Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E.},
  year = {2016},
  month = jul,
  number = {arXiv:1607.06450},
  eprint = {1607.06450},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1607.06450},
  url = {http://arxiv.org/abs/1607.06450},
  urldate = {2025-05-26},
  abstract = {Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feed-forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/unist/Zotero/storage/PYL5A8C6/Ba et al. - 2016 - Layer Normalization.pdf;/home/unist/Zotero/storage/CQ6RE63B/1607.html}
}

@misc{BeamerGuidea,
  title = {Beamer {{Guide}}},
  file = {/home/unist/Zotero/storage/SZ67FIVC/Beamer Guide.pdf}
}

@misc{beattieDeepMindLab2016a,
  title = {{{DeepMind Lab}}},
  author = {Beattie, Charles and Leibo, Joel Z. and Teplyashin, Denis and Ward, Tom and Wainwright, Marcus and K{\"u}ttler, Heinrich and Lefrancq, Andrew and Green, Simon and Vald{\'e}s, V{\'i}ctor and Sadik, Amir and Schrittwieser, Julian and Anderson, Keith and York, Sarah and Cant, Max and Cain, Adam and Bolton, Adrian and Gaffney, Stephen and King, Helen and Hassabis, Demis and Legg, Shane and Petersen, Stig},
  year = {2016},
  month = dec,
  number = {arXiv:1612.03801},
  eprint = {1612.03801},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1612.03801},
  url = {http://arxiv.org/abs/1612.03801},
  urldate = {2025-04-02},
  abstract = {DeepMind Lab is a first-person 3D game platform designed for research and development of general artificial intelligence and machine learning systems. DeepMind Lab can be used to study how autonomous artificial agents may learn complex tasks in large, partially observed, and visually diverse worlds. DeepMind Lab has a simple and flexible API enabling creative task-designs and novel AI-designs to be explored and quickly iterated upon. It is powered by a fast and widely recognised game engine, and tailored for effective use by the research community.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/home/unist/Zotero/storage/58SDLZH8/Beattie et al. - 2016 - DeepMind Lab.pdf;/home/unist/Zotero/storage/ATHFCCSL/1612.html}
}

@article{bertsekasGradientConvergenceGradienta,
  title = {Gradient {{Convergence}} in {{Gradient Methods}} with {{Errors}}},
  author = {Bertsekas, Dimitri P and Tsitsiklis, John N},
  volume = {10},
  number = {3},
  pages = {627--642},
  abstract = {We consider the gradient method xt+1 = xt + {$\gamma$}t(st + wt), where st is a descent direction of a function f : {\dbend}n {$\rightarrow$} {\dbend} and wt is a deterministic or stochastic error. We assume that ∇f is Lipschitz continuous, that the stepsize {$\gamma$}t diminishes to 0, and that st and wt satisfy standard conditions. We show that either f (xt) {$\rightarrow$} -{$\infty$} or f (xt) converges to a finite value and ∇f (xt) {$\rightarrow$} 0 (with probability 1 in the stochastic case), and in doing so, we remove various boundedness conditions that are assumed in existing results, such as boundedness from below of f , boundedness of ∇f (xt), or boundedness of xt.},
  langid = {english},
  file = {/home/unist/Zotero/storage/AQWJ6WP6/Bertsekas and Tsitsiklis - GRADIENT CONVERGENCE IN GRADIENT METHODS WITH ERRORS.pdf}
}

@article{bhatnagarNaturalActorCritic2009a,
  title = {Natural Actor--Critic Algorithms},
  author = {Bhatnagar, Shalabh and Sutton, Richard S. and Ghavamzadeh, Mohammad and Lee, Mark},
  year = {2009},
  month = nov,
  journal = {Automatica},
  volume = {45},
  number = {11},
  pages = {2471--2482},
  issn = {00051098},
  doi = {10.1016/j.automatica.2009.07.008},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0005109809003549},
  urldate = {2025-04-15},
  abstract = {We present four new reinforcement learning algorithms based on actor--critic, function approximation, and natural gradient ideas, and we provide their convergence proofs. Actor--critic reinforcement learning methods are online approximations to policy iteration in which the valuefunction parameters are estimated using temporal difference learning and the policy parameters are updated by stochastic gradient descent. Methods based on policy gradients in this way are of special interest because of their compatibility with function approximation methods, which are needed to handle large or infinite state spaces. The use of temporal difference learning in this way is of special interest because in many applications it dramatically reduces the variance of the gradient estimates. The use of the natural gradient is of interest because it can produce better conditioned parameterizations and has been shown to further reduce variance in some cases. Our results extend prior two-timescale convergence results for actor--critic methods by Konda and Tsitsiklis by using temporal difference learning in the actor and by incorporating natural gradients. Our results extend prior empirical studies of natural actor--critic methods by Peters, Vijayakumar and Schaal by providing the first convergence proofs and the first fully incremental algorithms. We present empirical results verifying the convergence of our algorithms.},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {/home/unist/Zotero/storage/QNJCZXZU/Bhatnagar et al. - 2009 - Natural actor–critic algorithms.pdf}
}

@inproceedings{bjorckUnderstandingBatchNormalization2018a,
  title = {Understanding {{Batch Normalization}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Bjorck, Nils and Gomes, Carla P and Selman, Bart and Weinberger, Kilian Q},
  year = {2018},
  volume = {31},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper/2018/hash/36072923bfc3cf47745d704feb489480-Abstract.html},
  urldate = {2025-05-26},
  abstract = {Batch normalization (BN) is a technique to normalize activations in intermediate layers of deep neural networks. Its tendency to improve accuracy and speed up training have established BN as a favorite technique in deep learning. Yet, despite its enormous success, there remains little consensus on the exact reason and mechanism behind these improvements. In this paper we take a step towards a better understanding of BN, following an empirical approach. We conduct several experiments, and show that BN primarily enables training with larger learning rates, which is the cause for faster convergence and better generalization. For networks without BN we demonstrate how large gradient updates can result in diverging loss and activations growing uncontrollably with network depth, which limits possible learning rates. BN avoids this problem by constantly correcting activations to be zero-mean and of unit standard deviation, which enables larger gradient steps, yields faster convergence and may help bypass sharp local minima. We further show various ways in which gradients and activations of deep unnormalized networks are ill-behaved. We contrast our results against recent findings in random matrix theory, shedding new light on classical initialization schemes and their consequences.},
  file = {/home/unist/Zotero/storage/SIH9QSIZ/Bjorck et al. - 2018 - Understanding Batch Normalization.pdf}
}

@misc{blackRealTimeExecutionAction2025,
  title = {Real-{{Time Execution}} of {{Action Chunking Flow Policies}}},
  author = {Black, Kevin and Galliker, Manuel Y. and Levine, Sergey},
  year = {2025},
  month = jun,
  number = {arXiv:2506.07339},
  eprint = {2506.07339},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2506.07339},
  url = {http://arxiv.org/abs/2506.07339},
  urldate = {2025-08-21},
  abstract = {Modern AI systems, especially those interacting with the physical world, increasingly require real-time performance. However, the high latency of state-of-the-art generalist models, including recent vision-language action models (VLAs), poses a significant challenge. While action chunking has enabled temporal consistency in high-frequency control tasks, it does not fully address the latency problem, leading to pauses or out-of-distribution jerky movements at chunk boundaries. This paper presents a novel inference-time algorithm that enables smooth asynchronous execution of action chunking policies. Our method, real-time chunking (RTC), is applicable to any diffusion- or flow-based VLA out of the box with no re-training. It generates the next action chunk while executing the current one, "freezing" actions guaranteed to execute and "inpainting" the rest. To test RTC, we introduce a new benchmark of 12 highly dynamic tasks in the Kinetix simulator, as well as evaluate 6 challenging real-world bimanual manipulation tasks. Results demonstrate that RTC is fast, performant, and uniquely robust to inference delay, significantly improving task throughput and enabling high success rates in precise tasks \${\textbackslash}unicode\{x2013\}\$ such as lighting a match \${\textbackslash}unicode\{x2013\}\$ even in the presence of significant latency. See https://pi.website/research/real\_time\_chunking for videos.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/home/unist/Zotero/storage/5P4UCTIH/Black et al. - 2025 - Real-Time Execution of Action Chunking Flow Policies.pdf;/home/unist/Zotero/storage/2M8ZMXE9/2506.html}
}

@misc{borsaUniversalSuccessorFeatures2018a,
  title = {Universal {{Successor Features Approximators}}},
  author = {Borsa, Diana and Barreto, Andr{\'e} and Quan, John and Mankowitz, Daniel and Munos, R{\'e}mi and van Hasselt, Hado and Silver, David and Schaul, Tom},
  year = {2018},
  month = dec,
  number = {arXiv:1812.07626},
  eprint = {1812.07626},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1812.07626},
  url = {http://arxiv.org/abs/1812.07626},
  urldate = {2025-04-02},
  abstract = {The ability of a reinforcement learning (RL) agent to learn about many reward functions at the same time has many potential benefits, such as the decomposition of complex tasks into simpler ones, the exchange of information between tasks, and the reuse of skills. We focus on one aspect in particular, namely the ability to generalise to unseen tasks. Parametric generalisation relies on the interpolation power of a function approximator that is given the task description as input; one of its most common form are universal value function approximators (UVFAs). Another way to generalise to new tasks is to exploit structure in the RL problem itself. Generalised policy improvement (GPI) combines solutions of previous tasks into a policy for the unseen task; this relies on instantaneous policy evaluation of old policies under the new reward function, which is made possible through successor features (SFs). Our proposed universal successor features approximators (USFAs) combine the advantages of all of these, namely the scalability of UVFAs, the instant inference of SFs, and the strong generalisation of GPI. We discuss the challenges involved in training a USFA, its generalisation properties and demonstrate its practical benefits and transfer abilities on a large-scale domain in which the agent has to navigate in a first-person perspective three-dimensional environment.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/unist/Zotero/storage/EE9J2MKF/Borsa et al. - 2018 - Universal Successor Features Approximators.pdf;/home/unist/Zotero/storage/Z4F7EGGP/1812.html}
}

@inproceedings{bortkiewiczAcceleratingGoalConditionedReinforcement2024,
  title = {Accelerating {{Goal-Conditioned Reinforcement Learning Algorithms}} and {{Research}}},
  booktitle = {The {{Thirteenth International Conference}} on {{Learning Representations}}},
  author = {Bortkiewicz, Micha{\l} and Pa{\l}ucki, W{\l}adys{\l}aw and Myers, Vivek and Dziarmaga, Tadeusz and Arczewski, Tomasz and Kuci{\'n}ski, {\L}ukasz and Eysenbach, Benjamin},
  year = {2024},
  month = oct,
  url = {https://openreview.net/forum?id=4gaySj8kvX},
  urldate = {2025-09-01},
  abstract = {Self-supervision has the potential to transform reinforcement learning (RL), paralleling the breakthroughs it has enabled in other areas of machine learning. While self-supervised learning in other domains aims to find patterns in a fixed dataset, self-supervised goal-conditioned reinforcement learning (GCRL) agents discover *new* behaviors by learning from the goals achieved during unstructured interaction with the environment. However, these methods have failed to see similar success, both due to a lack of data from slow environment simulations as well as a lack of stable algorithms. We take a step toward addressing both of these issues by releasing a high-performance codebase and benchmark (`JaxGCRL`) for self-supervised GCRL, enabling researchers to train agents for millions of environment steps in minutes on a single GPU. By utilizing GPU-accelerated replay buffers, environments, and a stable contrastive RL algorithm, we reduce training time by up to \$22{\textbackslash}times\$. Additionally, we assess key design choices in contrastive RL, identifying those that most effectively stabilize and enhance training performance. With this approach, we provide a foundation for future research in self-supervised GCRL, enabling researchers to quickly iterate on new ideas and evaluate them in diverse and challenging environments. Code: [https://anonymous.4open.science/r/JaxGCRL-2316/README.md](https://anonymous.4open.science/r/JaxGCRL-2316/README.md)},
  langid = {english},
  file = {/home/unist/Zotero/storage/YQJBSH2I/Bortkiewicz et al. - 2024 - Accelerating Goal-Conditioned Reinforcement Learning Algorithms and Research.pdf}
}

@misc{chenDecisionTransformerReinforcement2021a,
  title = {Decision {{Transformer}}: {{Reinforcement Learning}} via {{Sequence Modeling}}},
  shorttitle = {Decision {{Transformer}}},
  author = {Chen, Lili and Lu, Kevin and Rajeswaran, Aravind and Lee, Kimin and Grover, Aditya and Laskin, Michael and Abbeel, Pieter and Srinivas, Aravind and Mordatch, Igor},
  year = {2021},
  month = jun,
  number = {arXiv:2106.01345},
  eprint = {2106.01345},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2106.01345},
  url = {http://arxiv.org/abs/2106.01345},
  urldate = {2025-03-26},
  abstract = {We introduce a framework that abstracts Reinforcement Learning (RL) as a sequence modeling problem. This allows us to draw upon the simplicity and scalability of the Transformer architecture, and associated advances in language modeling such as GPT-x and BERT. In particular, we present Decision Transformer, an architecture that casts the problem of RL as conditional sequence modeling. Unlike prior approaches to RL that fit value functions or compute policy gradients, Decision Transformer simply outputs the optimal actions by leveraging a causally masked Transformer. By conditioning an autoregressive model on the desired return (reward), past states, and actions, our Decision Transformer model can generate future actions that achieve the desired return. Despite its simplicity, Decision Transformer matches or exceeds the performance of state-of-the-art model-free offline RL baselines on Atari, OpenAI Gym, and Key-to-Door tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/unist/Zotero/storage/QAPQE7P5/Chen et al. - 2021 - Decision Transformer Reinforcement Learning via Sequence Modeling.pdf;/home/unist/Zotero/storage/95BS9LWX/2106.html}
}

@misc{chenUnderstandingDomainRandomization2022,
  title = {Understanding {{Domain Randomization}} for {{Sim-to-real Transfer}}},
  author = {Chen, Xiaoyu and Hu, Jiachen and Jin, Chi and Li, Lihong and Wang, Liwei},
  year = {2022},
  month = mar,
  number = {arXiv:2110.03239},
  eprint = {2110.03239},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2110.03239},
  url = {http://arxiv.org/abs/2110.03239},
  urldate = {2025-08-25},
  abstract = {Reinforcement learning encounters many challenges when applied directly in the real world. Sim-to-real transfer is widely used to transfer the knowledge learned from simulation to the real world. Domain randomization -- one of the most popular algorithms for sim-to-real transfer -- has been demonstrated to be effective in various tasks in robotics and autonomous driving. Despite its empirical successes, theoretical understanding on why this simple algorithm works is limited. In this paper, we propose a theoretical framework for sim-to-real transfers, in which the simulator is modeled as a set of MDPs with tunable parameters (corresponding to unknown physical parameters such as friction). We provide sharp bounds on the sim-to-real gap -- the difference between the value of policy returned by domain randomization and the value of an optimal policy for the real world. We prove that sim-to-real transfer can succeed under mild conditions without any real-world training samples. Our theory also highlights the importance of using memory (i.e., history-dependent policies) in domain randomization. Our proof is based on novel techniques that reduce the problem of bounding the sim-to-real gap to the problem of designing efficient learning algorithms for infinite-horizon MDPs, which we believe are of independent interest.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/unist/Zotero/storage/MK5EZG7B/Chen et al. - 2022 - Understanding Domain Randomization for Sim-to-real Transfer.pdf;/home/unist/Zotero/storage/UZGYNDCN/2110.html}
}

@misc{choOutcomedirectedReinforcementLearning2023,
  title = {Outcome-Directed {{Reinforcement Learning}} by {{Uncertainty}} \& {{Temporal Distance-Aware Curriculum Goal Generation}}},
  author = {Cho, Daesol and Lee, Seungjae and Kim, H. Jin},
  year = {2023},
  month = feb,
  number = {arXiv:2301.11741},
  eprint = {2301.11741},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2301.11741},
  url = {http://arxiv.org/abs/2301.11741},
  urldate = {2025-08-26},
  abstract = {Current reinforcement learning (RL) often suffers when solving a challenging exploration problem where the desired outcomes or high rewards are rarely observed. Even though curriculum RL, a framework that solves complex tasks by proposing a sequence of surrogate tasks, shows reasonable results, most of the previous works still have difficulty in proposing curriculum due to the absence of a mechanism for obtaining calibrated guidance to the desired outcome state without any prior domain knowledge. To alleviate it, we propose an uncertainty \& temporal distance-aware curriculum goal generation method for the outcome-directed RL via solving a bipartite matching problem. It could not only provide precisely calibrated guidance of the curriculum to the desired outcome states but also bring much better sample efficiency and geometry-agnostic curriculum goal proposal capability compared to previous curriculum RL methods. We demonstrate that our algorithm significantly outperforms these prior methods in a variety of challenging navigation tasks and robotic manipulation tasks in a quantitative and qualitative way.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/home/unist/Zotero/storage/DIA9JD86/Cho et al. - 2023 - Outcome-directed Reinforcement Learning by Uncertainty & Temporal Distance-Aware Curriculum Goal Gen.pdf;/home/unist/Zotero/storage/R5MI9NDV/2301.html}
}

@misc{choPolicylabeledPreferenceLearning2025a,
  title = {Policy-Labeled {{Preference Learning}}: {{Is Preference Enough}} for {{RLHF}}?},
  shorttitle = {Policy-Labeled {{Preference Learning}}},
  author = {Cho, Taehyun and Ju, Seokhun and Han, Seungyub and Kim, Dohyeong and Lee, Kyungjae and Lee, Jungwoo},
  year = {2025},
  month = may,
  number = {arXiv:2505.06273},
  eprint = {2505.06273},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2505.06273},
  url = {http://arxiv.org/abs/2505.06273},
  urldate = {2025-08-19},
  abstract = {To design rewards that align with human goals, Reinforcement Learning from Human Feedback (RLHF) has emerged as a prominent technique for learning reward functions from human preferences and optimizing policies via reinforcement learning algorithms. However, existing RLHF methods often misinterpret trajectories as being generated by an optimal policy, causing inaccurate likelihood estimation and suboptimal learning. Inspired by Direct Preference Optimization framework which directly learns optimal policy without explicit reward, we propose policy-labeled preference learning (PPL), to resolve likelihood mismatch issues by modeling human preferences with regret, which reflects behavior policy information. We also provide a contrastive KL regularization, derived from regret-based principles, to enhance RLHF in sequential decision making. Experiments in high-dimensional continuous control tasks demonstrate PPL's significant improvements in offline RLHF performance and its effectiveness in online settings.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/unist/Zotero/storage/HJUDDZQH/Cho et al. - 2025 - Policy-labeled Preference Learning Is Preference Enough for RLHF.pdf;/home/unist/Zotero/storage/F5VD2L7B/2505.html}
}

@article{cichockiFamiliesAlphaBeta2010a,
  title = {Families of {{Alpha- Beta-}} and {{Gamma- Divergences}}: {{Flexible}} and {{Robust Measures}} of {{Similarities}}},
  shorttitle = {Families of {{Alpha- Beta-}} and {{Gamma- Divergences}}},
  author = {Cichocki, Andrzej and Amari, Shun-ichi},
  year = {2010},
  month = jun,
  journal = {Entropy},
  volume = {12},
  number = {6},
  pages = {1532--1568},
  publisher = {Molecular Diversity Preservation International},
  issn = {1099-4300},
  doi = {10.3390/e12061532},
  url = {https://www.mdpi.com/1099-4300/12/6/1532},
  urldate = {2025-08-19},
  abstract = {In this paper, we extend and overview wide families of Alpha-, Beta- and Gamma-divergences and discuss their fundamental properties. In literature usually only one single asymmetric (Alpha, Beta or Gamma) divergence is considered. We show in this paper that there exist families of such divergences with the same consistent properties. Moreover, we establish links and correspondences among these divergences by applying suitable nonlinear transformations. For example, we can generate the Beta-divergences directly from Alpha-divergences and vice versa. Furthermore, we show that a new wide class of Gamma-divergences can be generated not only from the family of Beta-divergences but also from a family of Alpha-divergences. The paper bridges these divergences and shows also their links to Tsallis and R{\'e}nyi entropies. Most of these divergences have a natural information theoretic interpretation.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {Csiszar-Morimoto and Bregman divergences,extended Itakura-Saito like divergences,generalized divergences,Similarity measures,Tsallis and Renyi entropies},
  file = {/home/unist/Zotero/storage/XPLA643E/Cichocki and Amari - 2010 - Families of Alpha- Beta- and Gamma- Divergences Flexible and Robust Measures of Similarities.pdf}
}

@book{CS229LectureNotesa,
  title = {{{CS229 Lecture Notes}}},
  file = {/home/unist/Zotero/storage/B7G7SFQC/main_notes.pdf}
}

@misc{CS229REINFORCEa,
  title = {{{CS229 REINFORCE}}},
  file = {/home/unist/Zotero/storage/VVNJW8VS/cs229-notes14.pdf}
}

@misc{CS598Statisticala,
  title = {{{CS}} 598 {{Statistical Reinforcement Learning}}},
  url = {https://nanjiang.cs.illinois.edu/cs598/},
  urldate = {2025-05-07},
  file = {/home/unist/Zotero/storage/3HH5DTTM/cs598.html}
}

@misc{CS6789Foundationsa,
  title = {{{CS}} 6789 {{Foundations}} of {{RL}}},
  url = {https://wensun.github.io/CS6789\_fall\_2021.html},
  urldate = {2025-05-07},
  file = {/home/unist/Zotero/storage/FI5I85F2/CS6789_fall_2021.html}
}

@misc{curtisFlowbasedDomainRandomization2025a,
  title = {Flow-Based {{Domain Randomization}} for {{Learning}} and {{Sequencing Robotic Skills}}},
  author = {Curtis, Aidan and Li, Eric and Noseworthy, Michael and Gothoskar, Nishad and Chitta, Sachin and Li, Hui and Kaelbling, Leslie Pack and Carey, Nicole},
  year = {2025},
  month = may,
  number = {arXiv:2502.01800},
  eprint = {2502.01800},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2502.01800},
  url = {http://arxiv.org/abs/2502.01800},
  urldate = {2025-08-22},
  abstract = {Domain randomization in reinforcement learning is an established technique for increasing the robustness of control policies trained in simulation. By randomizing environment properties during training, the learned policy can become robust to uncertainties along the randomized dimensions. While the environment distribution is typically specified by hand, in this paper we investigate automatically discovering a sampling distribution via entropy-regularized reward maximization of a normalizing-flow-based neural sampling distribution. We show that this architecture is more flexible and provides greater robustness than existing approaches that learn simpler, parameterized sampling distributions, as demonstrated in six simulated and one real-world robotics domain. Lastly, we explore how these learned sampling distributions, combined with a privileged value function, can be used for out-of-distribution detection in an uncertainty-aware multi-step manipulation planner.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/home/unist/Zotero/storage/ANH7BUEM/Curtis et al. - 2025 - Flow-based Domain Randomization for Learning and Sequencing Robotic Skills.pdf;/home/unist/Zotero/storage/NB8C45LR/2502.html}
}

@misc{damMonteCarloTreeSearch2023a,
  title = {Monte-{{Carlo}} Tree Search with Uncertainty Propagation via Optimal Transport},
  author = {Dam, Tuan and Stenger, Pascal and Schneider, Lukas and Pajarinen, Joni and D'Eramo, Carlo and Maillard, Odalric-Ambrym},
  year = {2023},
  month = sep,
  number = {arXiv:2309.10737},
  eprint = {2309.10737},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2309.10737},
  url = {http://arxiv.org/abs/2309.10737},
  urldate = {2025-08-19},
  abstract = {This paper introduces a novel backup strategy for Monte-Carlo Tree Search (MCTS) designed for highly stochastic and partially observable Markov decision processes. We adopt a probabilistic approach, modeling both value and action-value nodes as Gaussian distributions. We introduce a novel backup operator that computes value nodes as the Wasserstein barycenter of their action-value children nodes; thus, propagating the uncertainty of the estimate across the tree to the root node. We study our novel backup operator when using a novel combination of \$L{\textasciicircum}1\$-Wasserstein barycenter with \${\textbackslash}alpha\$-divergence, by drawing a notable connection to the generalized mean backup operator. We complement our probabilistic backup operator with two sampling strategies, based on optimistic selection and Thompson sampling, obtaining our Wasserstein MCTS algorithm. We provide theoretical guarantees of asymptotic convergence to the optimal policy, and an empirical evaluation on several stochastic and partially observable environments, where our approach outperforms well-known related baselines.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/home/unist/Zotero/storage/Q8J5YJAD/Dam et al. - 2023 - Monte-Carlo tree search with uncertainty propagation via optimal transport.pdf;/home/unist/Zotero/storage/Z9ETA2YF/2309.html}
}

@article{dauphinNeglectedHessianComponent2024a,
  title = {Neglected {{Hessian}} Component Explains Mysteries in Sharpness Regularization},
  author = {Dauphin, Yann N. and Agarwala, Atish and Mobahi, Hossein},
  year = {2024},
  month = dec,
  journal = {Advances in Neural Information Processing Systems},
  volume = {37},
  pages = {131920--131945},
  url = {https://proceedings.neurips.cc/paper\_files/paper/2024/hash/ee3ce0121939f42098cdefd3ea025bf1-Abstract-Conference.html},
  urldate = {2025-04-02},
  langid = {english},
  file = {/home/unist/Zotero/storage/UM6SBKYJ/Dauphin et al. - 2024 - Neglected Hessian component explains mysteries in sharpness regularization.pdf}
}

@inproceedings{degrisModelFreeReinforcementLearning2012a,
  title = {Model-{{Free}} Reinforcement Learning with Continuous Action in Practice},
  booktitle = {2012 {{American Control Conference}} ({{ACC}})},
  author = {Degris, T. and Pilarski, P. M. and Sutton, R. S.},
  year = {2012},
  month = jun,
  pages = {2177--2182},
  publisher = {IEEE},
  address = {Montreal, QC},
  doi = {10.1109/ACC.2012.6315022},
  url = {http://ieeexplore.ieee.org/document/6315022/},
  urldate = {2025-04-09},
  abstract = {Reinforcement learning methods are often considered as a potential solution to enable a robot to adapt to changes in real time to an unpredictable environment. However, with continuous action, only a few existing algorithms are practical for real-time learning. In such a setting, most effective methods have used a parameterized policy structure, often with a separate parameterized value function. The goal of this paper is to assess such actor--critic methods to form a fully specified practical algorithm. Our specific contributions include 1) developing the extension of existing incremental policy-gradient algorithms to use eligibility traces, 2) an empirical comparison of the resulting algorithms using continuous actions, 3) the evaluation of a gradient-scaling technique that can significantly improve performance. Finally, we apply our actor--critic algorithm to learn on a robotic platform with a fast sensorimotor cycle (10ms). Overall, these results constitute an important step towards practical real-time learning control with continuous action.},
  isbn = {978-1-4577-1096-4 978-1-4577-1095-7 978-1-4577-1094-0 978-1-4673-2102-0},
  langid = {english},
  file = {/home/unist/Zotero/storage/9CPD78M4/Degris et al. - 2012 - Model-Free reinforcement learning with continuous action in practice.pdf}
}

@misc{durkanNeuralSplineFlows2019,
  title = {Neural {{Spline Flows}}},
  author = {Durkan, Conor and Bekasov, Artur and Murray, Iain and Papamakarios, George},
  year = {2019},
  month = dec,
  number = {arXiv:1906.04032},
  eprint = {1906.04032},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1906.04032},
  url = {http://arxiv.org/abs/1906.04032},
  urldate = {2025-08-25},
  abstract = {A normalizing flow models a complex probability density as an invertible transformation of a simple base density. Flows based on either coupling or autoregressive transforms both offer exact density evaluation and sampling, but rely on the parameterization of an easily invertible elementwise transformation, whose choice determines the flexibility of these models. Building upon recent work, we propose a fully-differentiable module based on monotonic rational-quadratic splines, which enhances the flexibility of both coupling and autoregressive transforms while retaining analytic invertibility. We demonstrate that neural spline flows improve density estimation, variational inference, and generative modeling of images.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/unist/Zotero/storage/T6UNDKP7/Durkan et al. - 2019 - Neural Spline Flows.pdf;/home/unist/Zotero/storage/N6PAR8KR/1906.html}
}

@article{FeaturebasedMethodsLargea,
  title = {Feature-Based Methods for Large Scale Dynamic Programming},
  abstract = {We develop a methodological framework and present a few different ways in which dynamic programming and compact representations can be combined to solve large scale stochastic control problems. In particular, we develop algorithms that employ two types of feature-based compact representations; that is, representations that involve feature extraction and a relatively simple approximation architecture. We prove the convergence of these algorithms and provide bounds on the approximation error. As an example, one of these algorithms is used to generate a strategy for the game of Tetris. Furthermore, we provide a counterexample illustrating the difficulties of integrating compact representations with dynamic programming, which exemplifies the shortcomings of certain simple approaches.},
  langid = {english},
  file = {/home/unist/Zotero/storage/V36F79W8/Feature-based methods for large scale dynamic programming.pdf}
}

@misc{finnModelAgnosticMetaLearningFast2017a,
  title = {Model-{{Agnostic Meta-Learning}} for {{Fast Adaptation}} of {{Deep Networks}}},
  shorttitle = {{{MAML}}},
  author = {Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
  year = {2017},
  month = jul,
  number = {arXiv:1703.03400},
  eprint = {1703.03400},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1703.03400},
  url = {http://arxiv.org/abs/1703.03400},
  urldate = {2025-05-10},
  abstract = {We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two few-shot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/home/unist/Zotero/storage/AK9HFD95/Finn et al. - 2017 - Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.pdf;/home/unist/Zotero/storage/TNMFUJJH/1703.html}
}

@misc{foretSharpnessAwareMinimizationEfficiently2021a,
  title = {Sharpness-{{Aware Minimization}} for {{Efficiently Improving Generalization}}},
  author = {Foret, Pierre and Kleiner, Ariel and Mobahi, Hossein and Neyshabur, Behnam},
  year = {2021},
  month = apr,
  number = {arXiv:2010.01412},
  eprint = {2010.01412},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2010.01412},
  url = {http://arxiv.org/abs/2010.01412},
  urldate = {2025-05-23},
  abstract = {In today's heavily overparameterized models, the value of the training loss provides few guarantees on model generalization ability. Indeed, optimizing only the training loss value, as is commonly done, can easily lead to suboptimal model quality. Motivated by prior work connecting the geometry of the loss landscape and generalization, we introduce a novel, effective procedure for instead simultaneously minimizing loss value and loss sharpness. In particular, our procedure, Sharpness-Aware Minimization (SAM), seeks parameters that lie in neighborhoods having uniformly low loss; this formulation results in a min-max optimization problem on which gradient descent can be performed efficiently. We present empirical results showing that SAM improves model generalization across a variety of benchmark datasets (e.g., CIFAR-10, CIFAR-100, ImageNet, finetuning tasks) and models, yielding novel state-of-the-art performance for several. Additionally, we find that SAM natively provides robustness to label noise on par with that provided by state-of-the-art procedures that specifically target learning with noisy labels. We open source our code at {\textbackslash}url\{https://github.com/google-research/sam\}.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/unist/Zotero/storage/3UUS2IEN/Foret et al. - 2021 - Sharpness-Aware Minimization for Efficiently Improving Generalization.pdf;/home/unist/Zotero/storage/883U73J4/2010.html}
}

@misc{foxTamingNoiseReinforcement2017a,
  title = {Taming the {{Noise}} in {{Reinforcement Learning}} via {{Soft Updates}}},
  author = {Fox, Roy and Pakman, Ari and Tishby, Naftali},
  year = {2017},
  month = mar,
  number = {arXiv:1512.08562},
  eprint = {1512.08562},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1512.08562},
  url = {http://arxiv.org/abs/1512.08562},
  urldate = {2025-04-19},
  abstract = {Model-free reinforcement learning algorithms, such as Q-learning, perform poorly in the early stages of learning in noisy environments, because much effort is spent unlearning biased estimates of the state-action value function. The bias results from selecting, among several noisy estimates, the apparent optimum, which may actually be suboptimal. We propose G-learning, a new off-policy learning algorithm that regularizes the value estimates by penalizing deterministic policies in the beginning of the learning process. We show that this method reduces the bias of the value-function estimation, leading to faster convergence to the optimal value and the optimal policy. Moreover, G-learning enables the natural incorporation of prior domain knowledge, when available. The stochastic nature of G-learning also makes it avoid some exploration costs, a property usually attributed only to on-policy algorithms. We illustrate these ideas in several examples, where G-learning results in significant improvements of the convergence rate and the cost of the learning process.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Information Theory,Computer Science - Machine Learning,Mathematics - Information Theory},
  file = {/home/unist/Zotero/storage/MFDYRFNE/Fox et al. - 2017 - Taming the Noise in Reinforcement Learning via Soft Updates.pdf;/home/unist/Zotero/storage/TE48NLKG/1512.html}
}

@misc{fujimotoAddressingFunctionApproximation2018a,
  title = {Addressing {{Function Approximation Error}} in {{Actor-Critic Methods}}},
  author = {Fujimoto, Scott and van Hoof, Herke and Meger, David},
  year = {2018},
  month = oct,
  number = {arXiv:1802.09477},
  eprint = {1802.09477},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1802.09477},
  url = {http://arxiv.org/abs/1802.09477},
  urldate = {2025-04-09},
  abstract = {In value-based reinforcement learning methods such as deep Q-learning, function approximation errors are known to lead to overestimated value estimates and suboptimal policies. We show that this problem persists in an actor-critic setting and propose novel mechanisms to minimize its effects on both the actor and the critic. Our algorithm builds on Double Q-learning, by taking the minimum value between a pair of critics to limit overestimation. We draw the connection between target networks and overestimation bias, and suggest delaying policy updates to reduce per-update error and further improve performance. We evaluate our method on the suite of OpenAI gym tasks, outperforming the state of the art in every environment tested.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/unist/Zotero/storage/5YPGYVXI/Fujimoto et al. - 2018 - Addressing Function Approximation Error in Actor-Critic Methods.pdf;/home/unist/Zotero/storage/VK8URX53/1802.html}
}

@misc{fujimotoGeneralPurposeModelFreeReinforcement2025a,
  title = {Towards {{General-Purpose Model-Free Reinforcement Learning}}},
  author = {Fujimoto, Scott and D'Oro, Pierluca and Zhang, Amy and Tian, Yuandong and Rabbat, Michael},
  year = {2025},
  month = jan,
  number = {arXiv:2501.16142},
  eprint = {2501.16142},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2501.16142},
  url = {http://arxiv.org/abs/2501.16142},
  urldate = {2025-08-10},
  abstract = {Reinforcement learning (RL) promises a framework for near-universal problem-solving. In practice however, RL algorithms are often tailored to specific benchmarks, relying on carefully tuned hyperparameters and algorithmic choices. Recently, powerful model-based RL methods have shown impressive general results across benchmarks but come at the cost of increased complexity and slow run times, limiting their broader applicability. In this paper, we attempt to find a unifying model-free deep RL algorithm that can address a diverse class of domains and problem settings. To achieve this, we leverage model-based representations that approximately linearize the value function, taking advantage of the denser task objectives used by model-based RL while avoiding the costs associated with planning or simulated trajectories. We evaluate our algorithm, MR.Q, on a variety of common RL benchmarks with a single set of hyperparameters and show a competitive performance against domain-specific and general baselines, providing a concrete step towards building general-purpose model-free deep RL algorithms.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/unist/Zotero/storage/DKVAUG8W/Fujimoto et al. - 2025 - Towards General-Purpose Model-Free Reinforcement Learning.pdf;/home/unist/Zotero/storage/448SE5LG/2501.html}
}

@misc{fujimotoMinimalistApproachOffline2021a,
  title = {A {{Minimalist Approach}} to {{Offline Reinforcement Learning}}},
  author = {Fujimoto, Scott and Gu, Shixiang Shane},
  year = {2021},
  month = dec,
  number = {arXiv:2106.06860},
  eprint = {2106.06860},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2106.06860},
  url = {http://arxiv.org/abs/2106.06860},
  urldate = {2025-04-15},
  abstract = {Offline reinforcement learning (RL) defines the task of learning from a fixed batch of data. Due to errors in value estimation from out-of-distribution actions, most offline RL algorithms take the approach of constraining or regularizing the policy with the actions contained in the dataset. Built on pre-existing RL algorithms, modifications to make an RL algorithm work offline comes at the cost of additional complexity. Offline RL algorithms introduce new hyperparameters and often leverage secondary components such as generative models, while adjusting the underlying RL algorithm. In this paper we aim to make a deep RL algorithm work while making minimal changes. We find that we can match the performance of state-of-the-art offline RL algorithms by simply adding a behavior cloning term to the policy update of an online RL algorithm and normalizing the data. The resulting algorithm is a simple to implement and tune baseline, while more than halving the overall run time by removing the additional computational overhead of previous methods.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/unist/Zotero/storage/3IZKZJCK/Fujimoto and Gu - 2021 - A Minimalist Approach to Offline Reinforcement Learning.pdf;/home/unist/Zotero/storage/M3HLJZBZ/2106.html}
}

@misc{fujimotoOffPolicyDeepReinforcement2019,
  title = {Off-{{Policy Deep Reinforcement Learning}} without {{Exploration}}},
  author = {Fujimoto, Scott and Meger, David and Precup, Doina},
  year = {2019},
  month = aug,
  number = {arXiv:1812.02900},
  eprint = {1812.02900},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1812.02900},
  url = {http://arxiv.org/abs/1812.02900},
  urldate = {2025-09-08},
  abstract = {Many practical applications of reinforcement learning constrain agents to learn from a fixed batch of data which has already been gathered, without offering further possibility for data collection. In this paper, we demonstrate that due to errors introduced by extrapolation, standard off-policy deep reinforcement learning algorithms, such as DQN and DDPG, are incapable of learning with data uncorrelated to the distribution under the current policy, making them ineffective for this fixed batch setting. We introduce a novel class of off-policy algorithms, batch-constrained reinforcement learning, which restricts the action space in order to force the agent towards behaving close to on-policy with respect to a subset of the given data. We present the first continuous control deep reinforcement learning algorithm which can learn effectively from arbitrary, fixed batch data, and empirically demonstrate the quality of its behavior in several tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/unist/Zotero/storage/RKVJ4GPD/Fujimoto et al. - 2019 - Off-Policy Deep Reinforcement Learning without Exploration.pdf;/home/unist/Zotero/storage/7AEC67ZS/1812.html}
}

@misc{fujimotoOffPolicyDeepReinforcement2019b,
  title = {Off-{{Policy Deep Reinforcement Learning}} without {{Exploration}}},
  author = {Fujimoto, Scott and Meger, David and Precup, Doina},
  year = {2019},
  month = aug,
  number = {arXiv:1812.02900},
  eprint = {1812.02900},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1812.02900},
  url = {http://arxiv.org/abs/1812.02900},
  urldate = {2025-04-15},
  abstract = {Many practical applications of reinforcement learning constrain agents to learn from a fixed batch of data which has already been gathered, without offering further possibility for data collection. In this paper, we demonstrate that due to errors introduced by extrapolation, standard off-policy deep reinforcement learning algorithms, such as DQN and DDPG, are incapable of learning with data uncorrelated to the distribution under the current policy, making them ineffective for this fixed batch setting. We introduce a novel class of off-policy algorithms, batch-constrained reinforcement learning, which restricts the action space in order to force the agent towards behaving close to on-policy with respect to a subset of the given data. We present the first continuous control deep reinforcement learning algorithm which can learn effectively from arbitrary, fixed batch data, and empirically demonstrate the quality of its behavior in several tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,off-policy,Statistics - Machine Learning},
  file = {/home/unist/Zotero/storage/4LHR58EW/Fujimoto et al. - 2019 - Off-Policy Deep Reinforcement Learning without Exploration.pdf;/home/unist/Zotero/storage/HKQFUW8N/1812.html}
}

@misc{fujimotoOffPolicyDeepReinforcement2019c,
  title = {Off-{{Policy Deep Reinforcement Learning}} without {{Exploration}}},
  author = {Fujimoto, Scott and Meger, David and Precup, Doina},
  year = {2019},
  month = aug,
  number = {arXiv:1812.02900},
  eprint = {1812.02900},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1812.02900},
  url = {http://arxiv.org/abs/1812.02900},
  urldate = {2025-08-18},
  abstract = {Many practical applications of reinforcement learning constrain agents to learn from a fixed batch of data which has already been gathered, without offering further possibility for data collection. In this paper, we demonstrate that due to errors introduced by extrapolation, standard off-policy deep reinforcement learning algorithms, such as DQN and DDPG, are incapable of learning with data uncorrelated to the distribution under the current policy, making them ineffective for this fixed batch setting. We introduce a novel class of off-policy algorithms, batch-constrained reinforcement learning, which restricts the action space in order to force the agent towards behaving close to on-policy with respect to a subset of the given data. We present the first continuous control deep reinforcement learning algorithm which can learn effectively from arbitrary, fixed batch data, and empirically demonstrate the quality of its behavior in several tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/unist/Zotero/storage/5TVWPQKX/Fujimoto et al. - 2019 - Off-Policy Deep Reinforcement Learning without Exploration.pdf;/home/unist/Zotero/storage/NMUFBXNK/1812.html}
}

@misc{fujimotoWhyShouldTrust2022a,
  title = {Why {{Should I Trust You}}, {{Bellman}}? {{The Bellman Error}} Is a {{Poor Replacement}} for {{Value Error}}},
  shorttitle = {Why {{Should I Trust You}}, {{Bellman}}?},
  author = {Fujimoto, Scott and Meger, David and Precup, Doina and Nachum, Ofir and Gu, Shixiang Shane},
  year = {2022},
  month = jun,
  number = {arXiv:2201.12417},
  eprint = {2201.12417},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2201.12417},
  url = {http://arxiv.org/abs/2201.12417},
  urldate = {2025-04-15},
  abstract = {In this work, we study the use of the Bellman equation as a surrogate objective for value prediction accuracy. While the Bellman equation is uniquely solved by the true value function over all state-action pairs, we find that the Bellman error (the difference between both sides of the equation) is a poor proxy for the accuracy of the value function. In particular, we show that (1) due to cancellations from both sides of the Bellman equation, the magnitude of the Bellman error is only weakly related to the distance to the true value function, even when considering all state-action pairs, and (2) in the finite data regime, the Bellman equation can be satisfied exactly by infinitely many suboptimal solutions. This means that the Bellman error can be minimized without improving the accuracy of the value function. We demonstrate these phenomena through a series of propositions, illustrative toy examples, and empirical analysis in standard benchmark domains.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/unist/Zotero/storage/PMJ8S6AF/Fujimoto et al. - 2022 - Why Should I Trust You, Bellman The Bellman Error is a Poor Replacement for Value Error.pdf;/home/unist/Zotero/storage/2GULVDRW/2201.html}
}

@misc{guMambaLinearTimeSequence2024a,
  title = {Mamba: {{Linear-Time Sequence Modeling}} with {{Selective State Spaces}}},
  shorttitle = {Mamba},
  author = {Gu, Albert and Dao, Tri},
  year = {2024},
  month = may,
  number = {arXiv:2312.00752},
  eprint = {2312.00752},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2312.00752},
  url = {http://arxiv.org/abs/2312.00752},
  urldate = {2025-05-09},
  abstract = {Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5\${\textbackslash}times\$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/unist/Zotero/storage/BGGF3PLI/Gu and Dao - 2024 - Mamba Linear-Time Sequence Modeling with Selective State Spaces.pdf;/home/unist/Zotero/storage/QHPVME92/2312.html}
}

@inproceedings{guQPropSampleEfficientPolicy2017a,
  title = {Q-{{Prop}}: {{Sample-Efficient Policy Gradient}} with {{An Off-Policy Critic}}},
  shorttitle = {Q-{{Prop}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Gu, Shixiang and Lillicrap, Timothy and Ghahramani, Zoubin and Turner, Richard E. and Levine, Sergey},
  year = {2017},
  month = feb,
  url = {https://openreview.net/forum?id=SJ3rcZcxl},
  urldate = {2025-04-11},
  abstract = {Model-free deep reinforcement learning (RL) methods have been successful in a wide variety of simulated domains. However, a major obstacle facing deep RL in the real world is their high sample complexity. Batch policy gradient methods offer stable learning, but at the cost of high variance, which often requires large batches. TD-style methods, such as off-policy actor-critic and Q-learning, are more sample-efficient but biased, and often require costly hyperparameter sweeps to stabilize. In this work, we aim to develop methods that combine the stability of policy gradients with the efficiency of off-policy RL. We present Q-Prop, a policy gradient method that uses a Taylor expansion of the off-policy critic as a control variate. Q-Prop is both sample efficient and stable, and effectively combines the benefits of on-policy and off-policy methods. We analyze the connection between Q-Prop and existing model-free algorithms, and use control variate theory to derive two variants of Q-Prop with conservative and aggressive adaptation. We show that conservative Q-Prop provides substantial gains in sample efficiency over trust region policy optimization (TRPO) with generalized advantage estimation (GAE), and improves stability over deep deterministic policy gradient (DDPG), the state-of-the-art on-policy and off-policy methods, on OpenAI Gym's MuJoCo continuous control environments.},
  langid = {english},
  file = {/home/unist/Zotero/storage/R9UW4CBP/Gu et al. - 2017 - Q-Prop Sample-Efficient Policy Gradient with An Off-Policy Critic.pdf}
}

@misc{haarnojaReinforcementLearningDeep2017a,
  title = {Reinforcement {{Learning}} with {{Deep Energy-Based Policies}}},
  shorttitle = {Soft {{Q-Learning}}},
  author = {Haarnoja, Tuomas and Tang, Haoran and Abbeel, Pieter and Levine, Sergey},
  year = {2017},
  month = jul,
  number = {arXiv:1702.08165},
  eprint = {1702.08165},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1702.08165},
  url = {http://arxiv.org/abs/1702.08165},
  urldate = {2025-04-03},
  abstract = {We propose a method for learning expressive energy-based policies for continuous states and actions, which has been feasible only in tabular domains before. We apply our method to learning maximum entropy policies, resulting into a new algorithm, called soft Q-learning, that expresses the optimal policy via a Boltzmann distribution. We use the recently proposed amortized Stein variational gradient descent to learn a stochastic sampling network that approximates samples from this distribution. The benefits of the proposed algorithm include improved exploration and compositionality that allows transferring skills between tasks, which we confirm in simulated experiments with swimming and walking robots. We also draw a connection to actor-critic methods, which can be viewed performing approximate inference on the corresponding energy-based model.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,soft policy Iteration},
  file = {/home/unist/Zotero/storage/T37DEW86/Haarnoja et al. - 2017 - Reinforcement Learning with Deep Energy-Based Policies.pdf;/home/unist/Zotero/storage/X6M5ZTNU/1702.html}
}

@misc{haarnojaSoftActorCriticOffPolicy2018a,
  title = {Soft {{Actor-Critic}}: {{Off-Policy Maximum Entropy Deep Reinforcement Learning}} with a {{Stochastic Actor}}},
  shorttitle = {Soft {{Actor-Critic}}},
  author = {Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
  year = {2018},
  month = aug,
  number = {arXiv:1801.01290},
  eprint = {1801.01290},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1801.01290},
  url = {http://arxiv.org/abs/1801.01290},
  urldate = {2025-04-07},
  abstract = {Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,sac,soft actor critic,Statistics - Machine Learning},
  file = {/home/unist/Zotero/storage/LZW79ZMY/Haarnoja et al. - 2018 - Soft Actor-Critic Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor.pdf;/home/unist/Zotero/storage/6D7228EK/1801.html}
}

@misc{hafnerMasteringDiverseDomains2024a,
  title = {Mastering {{Diverse Domains}} through {{World Models}}},
  author = {Hafner, Danijar and Pasukonis, Jurgis and Ba, Jimmy and Lillicrap, Timothy},
  year = {2024},
  month = apr,
  number = {arXiv:2301.04104},
  eprint = {2301.04104},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2301.04104},
  url = {http://arxiv.org/abs/2301.04104},
  urldate = {2025-04-15},
  abstract = {Developing a general algorithm that learns to solve tasks across a wide range of applications has been a fundamental challenge in artificial intelligence. Although current reinforcement learning algorithms can be readily applied to tasks similar to what they have been developed for, configuring them for new application domains requires significant human expertise and experimentation. We present DreamerV3, a general algorithm that outperforms specialized methods across over 150 diverse tasks, with a single configuration. Dreamer learns a model of the environment and improves its behavior by imagining future scenarios. Robustness techniques based on normalization, balancing, and transformations enable stable learning across domains. Applied out of the box, Dreamer is the first algorithm to collect diamonds in Minecraft from scratch without human data or curricula. This achievement has been posed as a significant challenge in artificial intelligence that requires exploring farsighted strategies from pixels and sparse rewards in an open world. Our work allows solving challenging control problems without extensive experimentation, making reinforcement learning broadly applicable.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,model-based,Statistics - Machine Learning,world model},
  file = {/home/unist/Zotero/storage/SYUSAHLW/Hafner et al. - 2024 - Mastering Diverse Domains through World Models.pdf;/home/unist/Zotero/storage/CNCCQVR8/2301.html}
}

@article{hafnerReinforcementLearningFeedback2011a,
  title = {Reinforcement Learning in Feedback Control},
  author = {Hafner, Roland and Riedmiller, Martin},
  year = {2011},
  month = jul,
  journal = {Machine Learning},
  volume = {84},
  number = {1},
  pages = {137--169},
  issn = {1573-0565},
  doi = {10.1007/s10994-011-5235-x},
  url = {https://doi.org/10.1007/s10994-011-5235-x},
  urldate = {2025-04-16},
  abstract = {Technical process control is a highly interesting area of application serving a high practical impact. Since classical controller design is, in general, a demanding job, this area constitutes a highly attractive domain for the application of learning approaches---in particular, reinforcement learning (RL) methods. RL provides concepts for learning controllers that, by cleverly exploiting information from interactions with the process, can acquire high-quality control behaviour from scratch.},
  langid = {english},
  keywords = {Artificial Intelligence,Benchmarks,Feedback control,Nonlinear control,Reinforcement learning},
  file = {/home/unist/Zotero/storage/LV4D9K76/Hafner and Riedmiller - 2011 - Reinforcement learning in feedback control.pdf}
}

@misc{hasseltDeepReinforcementLearning2015a,
  title = {Deep {{Reinforcement Learning}} with {{Double Q-learning}}},
  shorttitle = {{{DDQN}}},
  author = {van Hasselt, Hado and Guez, Arthur and Silver, David},
  year = {2015},
  month = dec,
  number = {arXiv:1509.06461},
  eprint = {1509.06461},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1509.06461},
  url = {http://arxiv.org/abs/1509.06461},
  urldate = {2025-03-26},
  abstract = {The popular Q-learning algorithm is known to overestimate action values under certain conditions. It was not previously known whether, in practice, such overestimations are common, whether they harm performance, and whether they can generally be prevented. In this paper, we answer all these questions affirmatively. In particular, we first show that the recent DQN algorithm, which combines Q-learning with a deep neural network, suffers from substantial overestimations in some games in the Atari 2600 domain. We then show that the idea behind the Double Q-learning algorithm, which was introduced in a tabular setting, can be generalized to work with large-scale function approximation. We propose a specific adaptation to the DQN algorithm and show that the resulting algorithm not only reduces the observed overestimations, as hypothesized, but that this also leads to much better performance on several games.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Double},
  file = {/home/unist/Zotero/storage/66EL2PEZ/Hasselt et al. - 2015 - Deep Reinforcement Learning with Double Q-learning.pdf;/home/unist/Zotero/storage/ACKMC4KU/1509.html}
}

@inproceedings{hasseltDoubleQlearning2010a,
  title = {Double {{Q-learning}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Hasselt, Hado},
  year = {2010},
  volume = {23},
  publisher = {Curran Associates, Inc.},
  url = {https://papers.nips.cc/paper\_files/paper/2010/hash/091d584fced301b442654dd8c23b3fc9-Abstract.html},
  urldate = {2025-08-01},
  abstract = {In some stochastic environments the well-known reinforcement learning algorithm Q-learning performs very poorly. This poor performance is caused by large overestimations of action values. These overestimations result from a positive bias that is introduced because Q-learning uses the maximum action value as an approximation for the maximum expected action value. We introduce an alternative way to approximate the maximum expected value for any set of random variables. The obtained double estimator method is shown to sometimes underestimate rather than overestimate the maximum expected value. We apply the double estimator to Q-learning to construct Double Q-learning, a new off-policy reinforcement learning algorithm. We show the new algorithm converges to the optimal policy and that it performs well in some settings in which Q-learning performs poorly due to its overestimation.},
  keywords = {Double Q-Learning},
  file = {/home/unist/Zotero/storage/EA6S8FM2/Hasselt - 2010 - Double Q-learning.pdf}
}

@misc{hazanProvablyEfficientMaximum2019a,
  title = {Provably {{Efficient Maximum Entropy Exploration}}},
  author = {Hazan, Elad and Kakade, Sham M. and Singh, Karan and Soest, Abby Van},
  year = {2019},
  month = jan,
  number = {arXiv:1812.02690},
  eprint = {1812.02690},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1812.02690},
  url = {http://arxiv.org/abs/1812.02690},
  urldate = {2025-04-04},
  abstract = {Suppose an agent is in a (possibly unknown) Markov Decision Process in the absence of a reward signal, what might we hope that an agent can efficiently learn to do? This work studies a broad class of objectives that are defined solely as functions of the state-visitation frequencies that are induced by how the agent behaves. For example, one natural, intrinsically defined, objective problem is for the agent to learn a policy which induces a distribution over state space that is as uniform as possible, which can be measured in an entropic sense. We provide an efficient algorithm to optimize such such intrinsically defined objectives, when given access to a black box planning oracle (which is robust to function approximation). Furthermore, when restricted to the tabular setting where we have sample based access to the MDP, our proposed algorithm is provably efficient, both in terms of its sample and computational complexities. Key to our algorithmic methodology is utilizing the conditional gradient method (a.k.a. the Frank-Wolfe algorithm) which utilizes an approximate MDP solver.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/unist/Zotero/storage/Q9Z74TRP/Hazan et al. - 2019 - Provably Efficient Maximum Entropy Exploration.pdf;/home/unist/Zotero/storage/3JHI5CIL/1812.html}
}

@misc{heDelvingDeepRectifiers2015a,
  title = {Delving {{Deep}} into {{Rectifiers}}: {{Surpassing Human-Level Performance}} on {{ImageNet Classification}}},
  shorttitle = {Delving {{Deep}} into {{Rectifiers}}},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year = {2015},
  month = feb,
  number = {arXiv:1502.01852},
  eprint = {1502.01852},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1502.01852},
  url = {http://arxiv.org/abs/1502.01852},
  urldate = {2025-05-26},
  abstract = {Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on our PReLU networks (PReLU-nets), we achieve 4.94\% top-5 test error on the ImageNet 2012 classification dataset. This is a 26\% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66\%). To our knowledge, our result is the first to surpass human-level performance (5.1\%, Russakovsky et al.) on this visual recognition challenge.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,He initialization,PReLU},
  file = {/home/unist/Zotero/storage/NX3LUDAT/He et al. - 2015 - Delving Deep into Rectifiers Surpassing Human-Level Performance on ImageNet Classification.pdf;/home/unist/Zotero/storage/P448I9BF/1502.html}
}

@misc{heMomentumContrastUnsupervised2020a,
  title = {Momentum {{Contrast}} for {{Unsupervised Visual Representation Learning}}},
  shorttitle = {{{MoCo}}},
  author = {He, Kaiming and Fan, Haoqi and Wu, Yuxin and Xie, Saining and Girshick, Ross},
  year = {2020},
  month = mar,
  number = {arXiv:1911.05722},
  eprint = {1911.05722},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1911.05722},
  url = {http://arxiv.org/abs/1911.05722},
  urldate = {2025-05-23},
  abstract = {We present Momentum Contrast (MoCo) for unsupervised visual representation learning. From a perspective on contrastive learning as dictionary look-up, we build a dynamic dictionary with a queue and a moving-averaged encoder. This enables building a large and consistent dictionary on-the-fly that facilitates contrastive unsupervised learning. MoCo provides competitive results under the common linear protocol on ImageNet classification. More importantly, the representations learned by MoCo transfer well to downstream tasks. MoCo can outperform its supervised pre-training counterpart in 7 detection/segmentation tasks on PASCAL VOC, COCO, and other datasets, sometimes surpassing it by large margins. This suggests that the gap between unsupervised and supervised representation learning has been largely closed in many vision tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/unist/Zotero/storage/JBX4FM4W/He et al. - 2020 - Momentum Contrast for Unsupervised Visual Representation Learning.pdf;/home/unist/Zotero/storage/3CEVPD3D/1911.html}
}

@misc{hoDenoisingDiffusionProbabilistic2020a,
  title = {Denoising {{Diffusion Probabilistic Models}}},
  author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  year = {2020},
  month = dec,
  number = {arXiv:2006.11239},
  eprint = {2006.11239},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2006.11239},
  url = {http://arxiv.org/abs/2006.11239},
  urldate = {2025-05-28},
  abstract = {We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at https://github.com/hojonathanho/diffusion},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/unist/Zotero/storage/ATUL28M7/Ho et al. - 2020 - Denoising Diffusion Probabilistic Models.pdf;/home/unist/Zotero/storage/AZURGWEM/2006.html}
}

@book{hoffmanLInearAlgebraHoffmana,
  title = {{{LInear Algebra}} Hoffman},
  author = {Hoffman, kenneth and {kunze}, ray},
  file = {/home/unist/Zotero/storage/JQBTULYF/Hoffman and kunze - Linear Algebra.pdf;/home/unist/Zotero/storage/PPDMWVHA/Linear Algebra Solutions manual (Hoffman and Kunze) (Z-Library).pdf}
}

@book{IntroductionProbabilitya,
  title = {Introduction to Probability},
  file = {/home/unist/Zotero/storage/N44M5U89/Introduction to probability.pdf}
}

@misc{ioffeBatchNormalizationAccelerating2015a,
  title = {Batch {{Normalization}}: {{Accelerating Deep Network Training}} by {{Reducing Internal Covariate Shift}}},
  shorttitle = {Batch {{Normalization}}},
  author = {Ioffe, Sergey and Szegedy, Christian},
  year = {2015},
  month = mar,
  number = {arXiv:1502.03167},
  eprint = {1502.03167},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1502.03167},
  url = {http://arxiv.org/abs/1502.03167},
  urldate = {2025-05-26},
  abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9\% top-5 validation error (and 4.8\% test error), exceeding the accuracy of human raters.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/unist/Zotero/storage/82ZWQZEP/Ioffe and Szegedy - 2015 - Batch Normalization Accelerating Deep Network Training by Reducing Internal Covariate Shift.pdf;/home/unist/Zotero/storage/6QYQWS5C/1502.html}
}

@misc{javedREINFORCEVsReparameterizationa,
  title = {{{REINFORCE}} vs {{Reparameterization Trick}}},
  author = {Javed, Syed Ashar},
  url = {https://stillbreeze.github.io/REINFORCE-vs-Reparameterization-trick/},
  urldate = {2025-05-28},
  abstract = {An introduction and comparison of two popular techniques for estimating gradients in machine learning models},
  file = {/home/unist/Zotero/storage/824ZDY3M/REINFORCE-vs-Reparameterization-trick.html}
}

@book{jonathanrichardIntroductionConjugateGradienta,
  title = {An {{Introduction}} to the {{Conjugate Gradient Method Without}} the {{Agonizing Pain}}},
  author = {Jonathan Richard, Shewchuk},
  keywords = {Conjugate Gradient},
  file = {/home/unist/Zotero/storage/VY8GMU5H/painless-conjugate-gradient.pdf}
}

@inproceedings{kakadeNaturalPolicyGradient2001a,
  title = {A {{Natural Policy Gradient}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Kakade, Sham M},
  year = {2001},
  volume = {14},
  publisher = {MIT Press},
  url = {https://proceedings.neurips.cc/paper\_files/paper/2001/hash/4b86abe48d358ecf194c56c69108433e-Abstract.html},
  urldate = {2025-04-16},
  abstract = {We provide a  natural gradient method that represents the steepest  descent  direction based on the underlying structure of the param(cid:173) eter space.  Although gradient methods cannot make large changes  in  the  values  of the  parameters,  we  show  that  the  natural  gradi(cid:173) ent is moving toward choosing a greedy optimal action rather than  just a  better action.  These greedy optimal  actions  are those  that  would  be  chosen  under  one  improvement  step  of  policy  iteration  with  approximate,  compatible  value  functions,  as  defined  by  Sut(cid:173) ton  et al.  [9].  We  then show drastic performance improvements in  simple MDPs and in the more challenging MDP of Tetris.},
  keywords = {natural policy gradient},
  file = {/home/unist/Zotero/storage/A9G9Q3HB/Kakade - 2001 - A Natural Policy Gradient.pdf}
}

@article{kianiUnitaryConvolutionsLearning2024a,
  title = {Unitary {{Convolutions}} for {{Learning}} on {{Graphs}} and {{Groups}}},
  author = {Kiani, Bobak T. and Fesser, Lukas and Weber, Melanie},
  year = {2024},
  month = dec,
  journal = {Advances in Neural Information Processing Systems},
  volume = {37},
  pages = {136922--136961},
  url = {https://proceedings.neurips.cc/paper\_files/paper/2024/hash/f775e2e0e7e12227adecbbf945f43546-Abstract-Conference.html},
  urldate = {2025-04-02},
  langid = {english},
  file = {/home/unist/Zotero/storage/AWPEY39X/Kiani et al. - 2024 - Unitary Convolutions for Learning on Graphs and Groups.pdf}
}

@misc{kimPenalizingInfeasibleActions2025a,
  title = {Penalizing {{Infeasible Actions}} and {{Reward Scaling}} in {{Reinforcement Learning}} with {{Offline Data}}},
  author = {Kim, Jeonghye and Shin, Yongjae and Jung, Whiyoung and Hong, Sunghoon and Yoon, Deunsol and Sung, Youngchul and Lee, Kanghoon and Lim, Woohyung},
  year = {2025},
  month = jul,
  number = {arXiv:2507.08761},
  eprint = {2507.08761},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2507.08761},
  url = {http://arxiv.org/abs/2507.08761},
  urldate = {2025-08-19},
  abstract = {Reinforcement learning with offline data suffers from Q-value extrapolation errors. To address this issue, we first demonstrate that linear extrapolation of the Q-function beyond the data range is particularly problematic. To mitigate this, we propose guiding the gradual decrease of Q-values outside the data range, which is achieved through reward scaling with layer normalization (RS-LN) and a penalization mechanism for infeasible actions (PA). By combining RS-LN and PA, we develop a new algorithm called PARS. We evaluate PARS across a range of tasks, demonstrating superior performance compared to state-of-the-art algorithms in both offline training and online fine-tuning on the D4RL benchmark, with notable success in the challenging AntMaze Ultra task.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/unist/Zotero/storage/PBT3YNEC/Kim et al. - 2025 - Penalizing Infeasible Actions and Reward Scaling in Reinforcement Learning with Offline Data.pdf;/home/unist/Zotero/storage/C7JJ9NBJ/2507.html}
}

@misc{kingmaAdamMethodStochastic2017a,
  title = {Adam: {{A Method}} for {{Stochastic Optimization}}},
  shorttitle = {Adam},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  year = {2017},
  month = jan,
  number = {arXiv:1412.6980},
  eprint = {1412.6980},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1412.6980},
  url = {http://arxiv.org/abs/1412.6980},
  urldate = {2025-04-01},
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/unist/Zotero/storage/EF7VECNS/Kingma and Ba - 2017 - Adam A Method for Stochastic Optimization.pdf;/home/unist/Zotero/storage/FBX2S9S6/1412.html}
}

@misc{kingmaAutoEncodingVariationalBayes2022a,
  title = {Auto-{{Encoding Variational Bayes}}},
  author = {Kingma, Diederik P. and Welling, Max},
  year = {2022},
  month = dec,
  number = {arXiv:1312.6114},
  eprint = {1312.6114},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1312.6114},
  url = {http://arxiv.org/abs/1312.6114},
  urldate = {2025-05-28},
  abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions are two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/unist/Zotero/storage/GNZ4DLKC/Kingma and Welling - 2022 - Auto-Encoding Variational Bayes.pdf;/home/unist/Zotero/storage/68DINDFQ/1312.html}
}

@article{kingmaIntroductionVariationalAutoencoders2019a,
  title = {An {{Introduction}} to {{Variational Autoencoders}}},
  author = {Kingma, Diederik P. and Welling, Max},
  year = {2019},
  journal = {Foundations and Trends{\textregistered} in Machine Learning},
  volume = {12},
  number = {4},
  eprint = {1906.02691},
  primaryclass = {cs},
  pages = {307--392},
  issn = {1935-8237, 1935-8245},
  doi = {10.1561/2200000056},
  url = {http://arxiv.org/abs/1906.02691},
  urldate = {2025-05-28},
  abstract = {Variational autoencoders provide a principled framework for learning deep latent-variable models and corresponding inference models. In this work, we provide an introduction to variational autoencoders and some important extensions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/unist/Zotero/storage/JVNX4VR9/Kingma and Welling - 2019 - An Introduction to Variational Autoencoders.pdf;/home/unist/Zotero/storage/SX4AXAUH/1906.html}
}

@inproceedings{klinkCurriculumReinforcementLearning2022,
  title = {Curriculum {{Reinforcement Learning}} via {{Constrained Optimal Transport}}},
  booktitle = {Proceedings of the 39th {{International Conference}} on {{Machine Learning}}},
  author = {Klink, Pascal and Yang, Haoyi and D'Eramo, Carlo and Peters, Jan and Pajarinen, Joni},
  year = {2022},
  month = jun,
  pages = {11341--11358},
  publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v162/klink22a.html},
  urldate = {2025-08-26},
  abstract = {Curriculum reinforcement learning (CRL) allows solving complex tasks by generating a tailored sequence of learning tasks, starting from easy ones and subsequently increasing their difficulty. Although the potential of curricula in RL has been clearly shown in a variety of works, it is less clear how to generate them for a given learning environment, resulting in a variety of methods aiming to automate this task. In this work, we focus on the idea of framing curricula as interpolations between task distributions, which has previously been shown to be a viable approach to CRL. Identifying key issues of existing methods, we frame the generation of a curriculum as a constrained optimal transport problem between task distributions. Benchmarks show that this way of curriculum generation can improve upon existing CRL methods, yielding high performance in a variety of tasks with different characteristics.},
  langid = {english},
  file = {/home/unist/Zotero/storage/II68SLX9/Klink et al. - 2022 - Curriculum Reinforcement Learning via Constrained Optimal Transport.pdf}
}

@misc{kostrikovOfflineReinforcementLearning2021,
  title = {Offline {{Reinforcement Learning}} with {{Implicit Q-Learning}}},
  author = {Kostrikov, Ilya and Nair, Ashvin and Levine, Sergey},
  year = {2021},
  month = oct,
  number = {arXiv:2110.06169},
  eprint = {2110.06169},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2110.06169},
  url = {http://arxiv.org/abs/2110.06169},
  urldate = {2025-09-08},
  abstract = {Offline reinforcement learning requires reconciling two conflicting aims: learning a policy that improves over the behavior policy that collected the dataset, while at the same time minimizing the deviation from the behavior policy so as to avoid errors due to distributional shift. This trade-off is critical, because most current offline reinforcement learning methods need to query the value of unseen actions during training to improve the policy, and therefore need to either constrain these actions to be in-distribution, or else regularize their values. We propose an offline RL method that never needs to evaluate actions outside of the dataset, but still enables the learned policy to improve substantially over the best behavior in the data through generalization. The main insight in our work is that, instead of evaluating unseen actions from the latest policy, we can approximate the policy improvement step implicitly by treating the state value function as a random variable, with randomness determined by the action (while still integrating over the dynamics to avoid excessive optimism), and then taking a state conditional upper expectile of this random variable to estimate the value of the best actions in that state. This leverages the generalization capacity of the function approximator to estimate the value of the best available action at a given state without ever directly querying a Q-function with this unseen action. Our algorithm alternates between fitting this upper expectile value function and backing it up into a Q-function. Then, we extract the policy via advantage-weighted behavioral cloning. We dub our method implicit Q-learning (IQL). IQL demonstrates the state-of-the-art performance on D4RL, a standard benchmark for offline reinforcement learning. We also demonstrate that IQL achieves strong performance fine-tuning using online interaction after offline initialization.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/unist/Zotero/storage/B6HNMGZ7/Kostrikov et al. - 2021 - Offline Reinforcement Learning with Implicit Q-Learning.pdf;/home/unist/Zotero/storage/6QIVMZZD/2110.html}
}

@misc{kumarConservativeQLearningOffline2020,
  title = {Conservative {{Q-Learning}} for {{Offline Reinforcement Learning}}},
  author = {Kumar, Aviral and Zhou, Aurick and Tucker, George and Levine, Sergey},
  year = {2020},
  month = aug,
  number = {arXiv:2006.04779},
  eprint = {2006.04779},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2006.04779},
  url = {http://arxiv.org/abs/2006.04779},
  urldate = {2025-09-04},
  abstract = {Effectively leveraging large, previously collected datasets in reinforcement learning (RL) is a key challenge for large-scale real-world applications. Offline RL algorithms promise to learn effective policies from previously-collected, static datasets without further interaction. However, in practice, offline RL presents a major challenge, and standard off-policy RL methods can fail due to overestimation of values induced by the distributional shift between the dataset and the learned policy, especially when training on complex and multi-modal data distributions. In this paper, we propose conservative Q-learning (CQL), which aims to address these limitations by learning a conservative Q-function such that the expected value of a policy under this Q-function lower-bounds its true value. We theoretically show that CQL produces a lower bound on the value of the current policy and that it can be incorporated into a policy learning procedure with theoretical improvement guarantees. In practice, CQL augments the standard Bellman error objective with a simple Q-value regularizer which is straightforward to implement on top of existing deep Q-learning and actor-critic implementations. On both discrete and continuous control domains, we show that CQL substantially outperforms existing offline RL methods, often learning policies that attain 2-5 times higher final return, especially when learning from complex and multi-modal data distributions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/unist/Zotero/storage/Z5Z3PCRU/Kumar et al. - 2020 - Conservative Q-Learning for Offline Reinforcement Learning.pdf;/home/unist/Zotero/storage/XNITY5EB/2006.html}
}

@misc{kunstnerLimitationsEmpiricalFisher2020a,
  title = {Limitations of the {{Empirical Fisher Approximation}} for {{Natural Gradient Descent}}},
  author = {Kunstner, Frederik and Balles, Lukas and Hennig, Philipp},
  year = {2020},
  month = jun,
  number = {arXiv:1905.12558},
  eprint = {1905.12558},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1905.12558},
  url = {http://arxiv.org/abs/1905.12558},
  urldate = {2025-04-18},
  abstract = {Natural gradient descent, which preconditions a gradient descent update with the Fisher information matrix of the underlying statistical model, is a way to capture partial second-order information. Several highly visible works have advocated an approximation known as the empirical Fisher, drawing connections between approximate second-order methods and heuristics like Adam. We dispute this argument by showing that the empirical Fisher---unlike the Fisher---does not generally capture second-order information. We further argue that the conditions under which the empirical Fisher approaches the Fisher (and the Hessian) are unlikely to be met in practice, and that, even on simple optimization problems, the pathologies of the empirical Fisher can have undesirable effects.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/unist/Zotero/storage/IF3WP8YB/Kunstner et al. - 2020 - Limitations of the Empirical Fisher Approximation for Natural Gradient Descent.pdf;/home/unist/Zotero/storage/5TX3GLMA/1905.html}
}

@book{langLinearAlgebraa,
  title = {Linear {{Algebra}}},
  author = {Lang, Serge},
  file = {/home/unist/Zotero/storage/EG8RXEZW/Lang - Linear Algebra.pdf}
}

@inproceedings{leeSEMDICEOffpolicyState2024a,
  title = {{{SEMDICE}}: {{Off-policy State Entropy Maximization}} via {{Stationary Distribution Correction Estimation}}},
  shorttitle = {{{SEMDICE}}},
  booktitle = {The {{Thirteenth International Conference}} on {{Learning Representations}}},
  author = {Lee, Jongmin and Sun, Meiqi and Abbeel, Pieter},
  year = {2024},
  month = oct,
  url = {https://openreview.net/forum?id=rJ5g8ueQaI\#},
  urldate = {2025-05-28},
  abstract = {In the unsupervised pre-training for reinforcement learning, the agent aims to learn a prior policy for downstream tasks without relying on task-specific reward functions. We focus on state entropy maximization (SEM), where the goal is to learn a policy that maximizes the entropy of the state's stationary distribution. In this paper, we introduce SEMDICE, a principled off-policy algorithm that computes an SEM policy from an arbitrary off-policy dataset, which optimizes the policy directly within the space of stationary distributions. SEMDICE computes a single, stationary Markov state-entropy-maximizing policy from an arbitrary off-policy dataset. Experimental results demonstrate that SEMDICE outperforms baseline algorithms in maximizing state entropy while achieving the best adaptation efficiency for downstream tasks among SEM-based unsupervised RL pre-training methods.},
  langid = {english},
  file = {/home/unist/Zotero/storage/52KLKGUN/Lee et al. - 2024 - SEMDICE Off-policy State Entropy Maximization via Stationary Distribution Correction Estimation.pdf}
}

@misc{leeSimBaSimplicityBias2025a,
  title = {{{SimBa}}: {{Simplicity Bias}} for {{Scaling Up Parameters}} in {{Deep Reinforcement Learning}}},
  shorttitle = {{{SimBa}}},
  author = {Lee, Hojoon and Hwang, Dongyoon and Kim, Donghu and Kim, Hyunseung and Tai, Jun Jet and Subramanian, Kaushik and Wurman, Peter R. and Choo, Jaegul and Stone, Peter and Seno, Takuma},
  year = {2025},
  month = may,
  number = {arXiv:2410.09754},
  eprint = {2410.09754},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.09754},
  url = {http://arxiv.org/abs/2410.09754},
  urldate = {2025-08-18},
  abstract = {Recent advances in CV and NLP have been largely driven by scaling up the number of network parameters, despite traditional theories suggesting that larger networks are prone to overfitting. These large networks avoid overfitting by integrating components that induce a simplicity bias, guiding models toward simple and generalizable solutions. However, in deep RL, designing and scaling up networks have been less explored. Motivated by this opportunity, we present SimBa, an architecture designed to scale up parameters in deep RL by injecting a simplicity bias. SimBa consists of three components: (i) an observation normalization layer that standardizes inputs with running statistics, (ii) a residual feedforward block to provide a linear pathway from the input to output, and (iii) a layer normalization to control feature magnitudes. By scaling up parameters with SimBa, the sample efficiency of various deep RL algorithms-including off-policy, on-policy, and unsupervised methods-is consistently improved. Moreover, solely by integrating SimBa architecture into SAC, it matches or surpasses state-of-the-art deep RL methods with high computational efficiency across DMC, MyoSuite, and HumanoidBench. These results demonstrate SimBa's broad applicability and effectiveness across diverse RL algorithms and environments.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/unist/Zotero/storage/LC6WVIUN/Lee et al. - 2025 - SimBa Simplicity Bias for Scaling Up Parameters in Deep Reinforcement Learning.pdf;/home/unist/Zotero/storage/9B6RRHFK/2410.html}
}

@misc{levineOfflineReinforcementLearning2020b,
  title = {Offline {{Reinforcement Learning}}: {{Tutorial}}, {{Review}}, and {{Perspectives}} on {{Open Problems}}},
  shorttitle = {Offline {{Reinforcement Learning}}},
  author = {Levine, Sergey and Kumar, Aviral and Tucker, George and Fu, Justin},
  year = {2020},
  month = nov,
  number = {arXiv:2005.01643},
  eprint = {2005.01643},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2005.01643},
  url = {http://arxiv.org/abs/2005.01643},
  urldate = {2025-04-04},
  abstract = {In this tutorial article, we aim to provide the reader with the conceptual tools needed to get started on research on offline reinforcement learning algorithms: reinforcement learning algorithms that utilize previously collected data, without additional online data collection. Offline reinforcement learning algorithms hold tremendous promise for making it possible to turn large datasets into powerful decision making engines. Effective offline reinforcement learning methods would be able to extract policies with the maximum possible utility out of the available data, thereby allowing automation of a wide range of decision-making domains, from healthcare and education to robotics. However, the limitations of current algorithms make this difficult. We will aim to provide the reader with an understanding of these challenges, particularly in the context of modern deep reinforcement learning methods, and describe some potential solutions that have been explored in recent work to mitigate these challenges, along with recent applications, and a discussion of perspectives on open problems in the field.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/unist/Zotero/storage/YH868YI7/Levine et al. - 2020 - Offline Reinforcement Learning Tutorial, Review, and Perspectives on Open Problems.pdf;/home/unist/Zotero/storage/WFZQ9EWZ/2005.html}
}

@misc{levineOfflineReinforcementLearning2020c,
  title = {Offline {{Reinforcement Learning}}: {{Tutorial}}, {{Review}}, and {{Perspectives}} on {{Open Problems}}},
  shorttitle = {Offline {{Reinforcement Learning}}},
  author = {Levine, Sergey and Kumar, Aviral and Tucker, George and Fu, Justin},
  year = {2020},
  month = nov,
  number = {arXiv:2005.01643},
  eprint = {2005.01643},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2005.01643},
  url = {http://arxiv.org/abs/2005.01643},
  urldate = {2025-08-20},
  abstract = {In this tutorial article, we aim to provide the reader with the conceptual tools needed to get started on research on offline reinforcement learning algorithms: reinforcement learning algorithms that utilize previously collected data, without additional online data collection. Offline reinforcement learning algorithms hold tremendous promise for making it possible to turn large datasets into powerful decision making engines. Effective offline reinforcement learning methods would be able to extract policies with the maximum possible utility out of the available data, thereby allowing automation of a wide range of decision-making domains, from healthcare and education to robotics. However, the limitations of current algorithms make this difficult. We will aim to provide the reader with an understanding of these challenges, particularly in the context of modern deep reinforcement learning methods, and describe some potential solutions that have been explored in recent work to mitigate these challenges, along with recent applications, and a discussion of perspectives on open problems in the field.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/unist/Zotero/storage/LZFX46HC/Levine et al. - 2020 - Offline Reinforcement Learning Tutorial, Review, and Perspectives on Open Problems.pdf;/home/unist/Zotero/storage/BU5YNXLR/2005.html}
}

@misc{lillicrapContinuousControlDeep2019a,
  title = {Continuous Control with Deep Reinforcement Learning},
  shorttitle = {{{DDPG}}},
  author = {Lillicrap, Timothy P. and Hunt, Jonathan J. and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
  year = {2019},
  month = jul,
  number = {arXiv:1509.02971},
  eprint = {1509.02971},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1509.02971},
  url = {http://arxiv.org/abs/1509.02971},
  urldate = {2025-04-09},
  abstract = {We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/unist/Zotero/storage/HLLBZPE4/Lillicrap et al. - 2019 - Continuous control with deep reinforcement learning.pdf;/home/unist/Zotero/storage/N3APHANI/1509.html}
}

@misc{liSurveyTransformersReinforcement2023a,
  title = {A {{Survey}} on {{Transformers}} in {{Reinforcement Learning}}},
  author = {Li, Wenzhe and Luo, Hao and Lin, Zichuan and Zhang, Chongjie and Lu, Zongqing and Ye, Deheng},
  year = {2023},
  month = sep,
  number = {arXiv:2301.03044},
  eprint = {2301.03044},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2301.03044},
  url = {http://arxiv.org/abs/2301.03044},
  urldate = {2025-05-09},
  abstract = {Transformer has been considered the dominating neural architecture in NLP and CV, mostly under supervised settings. Recently, a similar surge of using Transformers has appeared in the domain of reinforcement learning (RL), but it is faced with unique design choices and challenges brought by the nature of RL. However, the evolution of Transformers in RL has not yet been well unraveled. In this paper, we seek to systematically review motivations and progress on using Transformers in RL, provide a taxonomy on existing works, discuss each sub-field, and summarize future prospects.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/unist/Zotero/storage/FZQIHMAW/Li et al. - 2023 - A Survey on Transformers in Reinforcement Learning.pdf;/home/unist/Zotero/storage/H7SMSZVK/2301.html}
}

@misc{liuContinualReinforcementLearning2025a,
  title = {Continual {{Reinforcement Learning}} by {{Planning}} with {{Online World Models}}},
  author = {Liu, Zichen and Fu, Guoji and Du, Chao and Lee, Wee Sun and Lin, Min},
  year = {2025},
  month = jul,
  number = {arXiv:2507.09177},
  eprint = {2507.09177},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2507.09177},
  url = {http://arxiv.org/abs/2507.09177},
  urldate = {2025-08-19},
  abstract = {Continual reinforcement learning (CRL) refers to a naturalistic setting where an agent needs to endlessly evolve, by trial and error, to solve multiple tasks that are presented sequentially. One of the largest obstacles to CRL is that the agent may forget how to solve previous tasks when learning a new task, known as catastrophic forgetting. In this paper, we propose to address this challenge by planning with online world models. Specifically, we learn a Follow-The-Leader shallow model online to capture the world dynamics, in which we plan using model predictive control to solve a set of tasks specified by any reward functions. The online world model is immune to forgetting by construction with a proven regret bound of \${\textbackslash}mathcal\{O\}({\textbackslash}sqrt\{K{\textasciicircum}2D{\textbackslash}log(T)\})\$ under mild assumptions. The planner searches actions solely based on the latest online model, thus forming a FTL Online Agent (OA) that updates incrementally. To assess OA, we further design Continual Bench, a dedicated environment for CRL, and compare with several strong baselines under the same model-planning algorithmic framework. The empirical results show that OA learns continuously to solve new tasks while not forgetting old skills, outperforming agents built on deep world models with various continual learning techniques.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/unist/Zotero/storage/EPTSNXYL/Liu et al. - 2025 - Continual Reinforcement Learning by Planning with Online World Models.pdf;/home/unist/Zotero/storage/CVRW4FY4/2507.html}
}

@misc{liuDropoutReducesUnderfitting2023a,
  title = {Dropout {{Reduces Underfitting}}},
  author = {Liu, Zhuang and Xu, Zhiqiu and Jin, Joseph and Shen, Zhiqiang and Darrell, Trevor},
  year = {2023},
  month = may,
  number = {arXiv:2303.01500},
  eprint = {2303.01500},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.01500},
  url = {http://arxiv.org/abs/2303.01500},
  urldate = {2025-05-23},
  abstract = {Introduced by Hinton et al. in 2012, dropout has stood the test of time as a regularizer for preventing overfitting in neural networks. In this study, we demonstrate that dropout can also mitigate underfitting when used at the start of training. During the early phase, we find dropout reduces the directional variance of gradients across mini-batches and helps align the mini-batch gradients with the entire dataset's gradient. This helps counteract the stochasticity of SGD and limit the influence of individual batches on model training. Our findings lead us to a solution for improving performance in underfitting models - early dropout: dropout is applied only during the initial phases of training, and turned off afterwards. Models equipped with early dropout achieve lower final training loss compared to their counterparts without dropout. Additionally, we explore a symmetric technique for regularizing overfitting models - late dropout, where dropout is not used in the early iterations and is only activated later in training. Experiments on ImageNet and various vision tasks demonstrate that our methods consistently improve generalization accuracy. Our results encourage more research on understanding regularization in deep learning and our methods can be useful tools for future neural network training, especially in the era of large data. Code is available at https://github.com/facebookresearch/dropout.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/unist/Zotero/storage/82F24RS8/Liu et al. - 2023 - Dropout Reduces Underfitting.pdf;/home/unist/Zotero/storage/XA8AQ9HE/2303.html}
}

@misc{loshchilovDecoupledWeightDecay2019b,
  title = {Decoupled {{Weight Decay Regularization}}},
  author = {Loshchilov, Ilya and Hutter, Frank},
  year = {2019},
  month = jan,
  number = {arXiv:1711.05101},
  eprint = {1711.05101},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1711.05101},
  url = {http://arxiv.org/abs/1711.05101},
  urldate = {2025-05-26},
  abstract = {L\$\_2\$ regularization and weight decay regularization are equivalent for standard stochastic gradient descent (when rescaled by the learning rate), but as we demonstrate this is {\textbackslash}emph\{not\} the case for adaptive gradient algorithms, such as Adam. While common implementations of these algorithms employ L\$\_2\$ regularization (often calling it "weight decay" in what may be misleading due to the inequivalence we expose), we propose a simple modification to recover the original formulation of weight decay regularization by {\textbackslash}emph\{decoupling\} the weight decay from the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam and (ii) substantially improves Adam's generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter). Our proposed decoupled weight decay has already been adopted by many researchers, and the community has implemented it in TensorFlow and PyTorch; the complete source code for our experiments is available at https://github.com/loshchil/AdamW-and-SGDW},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Mathematics - Optimization and Control},
  file = {/home/unist/Zotero/storage/PT8XE9AC/Loshchilov and Hutter - 2019 - Decoupled Weight Decay Regularization.pdf;/home/unist/Zotero/storage/NTVQLDT3/1711.html}
}

@misc{loshchilovDecoupledWeightDecay2019c,
  title = {Decoupled {{Weight Decay Regularization}}},
  author = {Loshchilov, Ilya and Hutter, Frank},
  year = {2019},
  month = jan,
  number = {arXiv:1711.05101},
  eprint = {1711.05101},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1711.05101},
  url = {http://arxiv.org/abs/1711.05101},
  urldate = {2025-04-01},
  abstract = {L\$\_2\$ regularization and weight decay regularization are equivalent for standard stochastic gradient descent (when rescaled by the learning rate), but as we demonstrate this is {\textbackslash}emph\{not\} the case for adaptive gradient algorithms, such as Adam. While common implementations of these algorithms employ L\$\_2\$ regularization (often calling it "weight decay" in what may be misleading due to the inequivalence we expose), we propose a simple modification to recover the original formulation of weight decay regularization by {\textbackslash}emph\{decoupling\} the weight decay from the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam and (ii) substantially improves Adam's generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter). Our proposed decoupled weight decay has already been adopted by many researchers, and the community has implemented it in TensorFlow and PyTorch; the complete source code for our experiments is available at https://github.com/loshchil/AdamW-and-SGDW},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Mathematics - Optimization and Control},
  file = {/home/unist/Zotero/storage/47AKNFTI/Loshchilov and Hutter - 2019 - Decoupled Weight Decay Regularization.pdf;/home/unist/Zotero/storage/Y3WUNJAF/1711.html}
}

@misc{luoUnderstandingEffectiveReceptive2017a,
  title = {Understanding the {{Effective Receptive Field}} in {{Deep Convolutional Neural Networks}}},
  author = {Luo, Wenjie and Li, Yujia and Urtasun, Raquel and Zemel, Richard},
  year = {2017},
  month = jan,
  number = {arXiv:1701.04128},
  eprint = {1701.04128},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1701.04128},
  url = {http://arxiv.org/abs/1701.04128},
  urldate = {2025-05-28},
  abstract = {We study characteristics of receptive fields of units in deep convolutional networks. The receptive field size is a crucial issue in many visual tasks, as the output must respond to large enough areas in the image to capture information about large objects. We introduce the notion of an effective receptive field, and show that it both has a Gaussian distribution and only occupies a fraction of the full theoretical receptive field. We analyze the effective receptive field in several architecture designs, and the effect of nonlinear activations, dropout, sub-sampling and skip connections on it. This leads to suggestions for ways to address its tendency to be too small.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/unist/Zotero/storage/ALTEEKRV/Luo et al. - 2017 - Understanding the Effective Receptive Field in Deep Convolutional Neural Networks.pdf;/home/unist/Zotero/storage/YU5TTTTC/1701.html}
}

@misc{luWhatMakesGood2025a,
  title = {What {{Makes}} a {{Good Diffusion Planner}} for {{Decision Making}}?},
  author = {Lu, Haofei and Han, Dongqi and Shen, Yifei and Li, Dongsheng},
  year = {2025},
  month = mar,
  number = {arXiv:2503.00535},
  eprint = {2503.00535},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2503.00535},
  url = {http://arxiv.org/abs/2503.00535},
  urldate = {2025-08-14},
  abstract = {Diffusion models have recently shown significant potential in solving decision-making problems, particularly in generating behavior plans -- also known as diffusion planning. While numerous studies have demonstrated the impressive performance of diffusion planning, the mechanisms behind the key components of a good diffusion planner remain unclear and the design choices are highly inconsistent in existing studies. In this work, we address this issue through systematic empirical experiments on diffusion planning in an offline reinforcement learning (RL) setting, providing practical insights into the essential components of diffusion planning. We trained and evaluated over 6,000 diffusion models, identifying the critical components such as guided sampling, network architecture, action generation and planning strategy. We revealed that some design choices opposite to the common practice in previous work in diffusion planning actually lead to better performance, e.g., unconditional sampling with selection can be better than guided sampling and Transformer outperforms U-Net as denoising network. Based on these insights, we suggest a simple yet strong diffusion planning baseline that achieves state-of-the-art results on standard offline RL benchmarks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/unist/Zotero/storage/4Y5SS6PP/Lu et al. - 2025 - What Makes a Good Diffusion Planner for Decision Making.pdf;/home/unist/Zotero/storage/WDVJV6LN/2503.html}
}

@book{m.coverElementsInformationTheorya,
  title = {Elements of {{Information Theory}}},
  author = {M.cover, Thomas},
  file = {/home/unist/Zotero/storage/SYXBR5JJ/_.pdf}
}

@misc{machadoRevisitingArcadeLearning2017a,
  title = {Revisiting the {{Arcade Learning Environment}}: {{Evaluation Protocols}} and {{Open Problems}} for {{General Agents}}},
  shorttitle = {Revisiting the {{Arcade Learning Environment}}},
  author = {Machado, Marlos C. and Bellemare, Marc G. and Talvitie, Erik and Veness, Joel and Hausknecht, Matthew and Bowling, Michael},
  year = {2017},
  month = dec,
  number = {arXiv:1709.06009},
  eprint = {1709.06009},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1709.06009},
  url = {http://arxiv.org/abs/1709.06009},
  urldate = {2025-05-16},
  abstract = {The Arcade Learning Environment (ALE) is an evaluation platform that poses the challenge of building AI agents with general competency across dozens of Atari 2600 games. It supports a variety of different problem settings and it has been receiving increasing attention from the scientific community, leading to some high-profile success stories such as the much publicized Deep Q-Networks (DQN). In this article we take a big picture look at how the ALE is being used by the research community. We show how diverse the evaluation methodologies in the ALE have become with time, and highlight some key concerns when evaluating agents in the ALE. We use this discussion to present some methodological best practices and provide new benchmark results using these best practices. To further the progress in the field, we introduce a new version of the ALE that supports multiple game modes and provides a form of stochasticity we call sticky actions. We conclude this big picture look by revisiting challenges posed when the ALE was introduced, summarizing the state-of-the-art in various problems and highlighting problems that remain open.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/unist/Zotero/storage/LYGQJ66J/Machado et al. - 2017 - Revisiting the Arcade Learning Environment Evaluation Protocols and Open Problems for General Agent.pdf;/home/unist/Zotero/storage/GP4TZZ78/1709.html}
}

@inproceedings{maHighlyEfficientSelfAdaptive2024,
  title = {Highly {{Efficient Self-Adaptive Reward Shaping}} for {{Reinforcement Learning}}},
  booktitle = {The {{Thirteenth International Conference}} on {{Learning Representations}}},
  author = {Ma, Haozhe and Luo, Zhengding and Vo, Thanh Vinh and Sima, Kuankuan and Leong, Tze-Yun},
  year = {2024},
  month = oct,
  url = {https://openreview.net/forum?id=QOfWubPhdS},
  urldate = {2025-09-01},
  abstract = {Reward shaping is a reinforcement learning technique that addresses the sparse-reward problem by providing frequent, informative feedback. We propose an efficient self-adaptive reward-shaping mechanism that uses success rates derived from historical experiences as shaped rewards. The success rates are sampled from Beta distributions, which evolve from uncertainty to reliability as data accumulates. Initially, shaped rewards are stochastic to encourage exploration, gradually becoming more certain to promote exploitation and maintain a natural balance between exploration and exploitation. We apply Kernel Density Estimation (KDE) with Random Fourier Features (RFF) to derive Beta distributions, providing a computationally efficient solution for continuous and high-dimensional state spaces. Our method, validated on tasks with extremely sparse rewards, improves sample efficiency and convergence stability over relevant baselines.},
  langid = {english},
  file = {/home/unist/Zotero/storage/WM3LSLW2/Ma et al. - 2024 - Highly Efficient Self-Adaptive Reward Shaping for Reinforcement Learning.pdf}
}

@inproceedings{mahmoodWeightedImportanceSampling2014a,
  title = {Weighted Importance Sampling for Off-Policy Learning with Linear Function Approximation},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Mahmood, A. Rupam and {van Hasselt}, Hado and Sutton, Richard S.},
  year = {2014},
  volume = {27},
  publisher = {Curran Associates, Inc.},
  url = {https://papers.nips.cc/paper\_files/paper/2014/hash/1c64ee92596e8ea5050fc435a1d57459-Abstract.html},
  urldate = {2025-03-26},
  abstract = {Importance sampling is an essential component of off-policy model-free reinforcement learning algorithms. However, its most effective variant, {\textbackslash}emph\{weighted\} importance sampling, does not carry over easily to function approximation and, because of this, it is not utilized in existing off-policy learning algorithms. In this paper, we take two steps toward bridging this gap. First, we show that weighted importance sampling can be viewed as a special case of weighting the error of individual training samples, and that this weighting has theoretical and empirical benefits similar to those of weighted importance sampling. Second, we show that these benefits extend to a new weighted-importance-sampling version of off-policy LSTD(lambda). We show empirically that our new WIS-LSTD(lambda) algorithm can result in much more rapid and reliable convergence than conventional off-policy LSTD(lambda) (Yu 2010, Bertsekas \& Yu 2009).},
  file = {/home/unist/Zotero/storage/KCLR6NPY/Mahmood et al. - 2014 - Weighted importance sampling for off-policy learning with linear function approximation.pdf}
}

@misc{markPolicyAgnosticRL2024a,
  title = {Policy {{Agnostic RL}}: {{Offline RL}} and {{Online RL Fine-Tuning}} of {{Any Class}} and {{Backbone}}},
  shorttitle = {Policy {{Agnostic RL}}},
  author = {Mark, Max Sobol and Gao, Tian and Sampaio, Georgia Gabriela and Srirama, Mohan Kumar and Sharma, Archit and Finn, Chelsea and Kumar, Aviral},
  year = {2024},
  month = dec,
  number = {arXiv:2412.06685},
  eprint = {2412.06685},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2412.06685},
  url = {http://arxiv.org/abs/2412.06685},
  urldate = {2025-08-20},
  abstract = {Recent advances in learning decision-making policies can largely be attributed to training expressive policy models, largely via imitation learning. While imitation learning discards non-expert data, reinforcement learning (RL) can still learn from suboptimal data. However, instantiating RL training of a new policy class often presents a different challenge: most deep RL machinery is co-developed with assumptions on the policy class and backbone, resulting in poor performance when the policy class changes. For instance, SAC utilizes a low-variance reparameterization policy gradient for Gaussian policies, but this is unstable for diffusion policies and intractable for autoregressive categorical policies. To address this issue, we develop an offline RL and online fine-tuning approach called policy-agnostic RL (PA-RL) that can effectively train multiple policy classes, with varying architectures and sizes. We build off the basic idea that a universal supervised learning loss can replace the policy improvement step in RL, as long as it is applied on "optimized" actions. To obtain these optimized actions, we first sample multiple actions from a base policy, and run global optimization (i.e., re-ranking multiple action samples using the Q-function) and local optimization (i.e., running gradient steps on an action sample) to maximize the critic on these candidates. PA-RL enables fine-tuning diffusion and transformer policies with either autoregressive tokens or continuous action outputs, at different sizes, entirely via actor-critic RL. Moreover, PA-RL improves the performance and sample-efficiency by up to 2 times compared to existing offline RL and online fine-tuning methods. We show the first result that successfully fine-tunes OpenVLA, a 7B generalist robot policy, autonomously with Cal-QL, an online RL fine-tuning algorithm, improving from 40\% to 70\% in the real world in 40 minutes.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/unist/Zotero/storage/EIXCYKKL/Mark et al. - 2024 - Policy Agnostic RL Offline RL and Online RL Fine-Tuning of Any Class and Backbone.pdf;/home/unist/Zotero/storage/Y75W8T9W/2412.html}
}

@misc{martonMitigatingInformationLoss2025a,
  title = {Mitigating {{Information Loss}} in {{Tree-Based Reinforcement Learning}} via {{Direct Optimization}}},
  author = {Marton, Sascha and Grams, Tim and Vogt, Florian and L{\"u}dtke, Stefan and Bartelt, Christian and Stuckenschmidt, Heiner},
  year = {2025},
  month = mar,
  number = {arXiv:2408.08761},
  eprint = {2408.08761},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2408.08761},
  url = {http://arxiv.org/abs/2408.08761},
  urldate = {2025-08-18},
  abstract = {Reinforcement learning (RL) has seen significant success across various domains, but its adoption is often limited by the black-box nature of neural network policies, making them difficult to interpret. In contrast, symbolic policies allow representing decision-making strategies in a compact and interpretable way. However, learning symbolic policies directly within on-policy methods remains challenging. In this paper, we introduce SYMPOL, a novel method for SYMbolic tree-based on-POLicy RL. SYMPOL employs a tree-based model integrated with a policy gradient method, enabling the agent to learn and adapt its actions while maintaining a high level of interpretability. We evaluate SYMPOL on a set of benchmark RL tasks, demonstrating its superiority over alternative tree-based RL approaches in terms of performance and interpretability. Unlike existing methods, it enables gradient-based, end-to-end learning of interpretable, axis-aligned decision trees within standard on-policy RL algorithms. Therefore, SYMPOL can become the foundation for a new class of interpretable RL based on decision trees. Our implementation is available under: https://github.com/s-marton/sympol},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/unist/Zotero/storage/AJ8277IE/Marton et al. - 2025 - Mitigating Information Loss in Tree-Based Reinforcement Learning via Direct Optimization.pdf;/home/unist/Zotero/storage/65H6WXJU/2408.html}
}

@misc{mehtaActiveDomainRandomization2019,
  title = {Active {{Domain Randomization}}},
  author = {Mehta, Bhairav and Diaz, Manfred and Golemo, Florian and Pal, Christopher J. and Paull, Liam},
  year = {2019},
  month = jul,
  number = {arXiv:1904.04762},
  eprint = {1904.04762},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1904.04762},
  url = {http://arxiv.org/abs/1904.04762},
  urldate = {2025-08-25},
  abstract = {Domain randomization is a popular technique for improving domain transfer, often used in a zero-shot setting when the target domain is unknown or cannot easily be used for training. In this work, we empirically examine the effects of domain randomization on agent generalization. Our experiments show that domain randomization may lead to suboptimal, high-variance policies, which we attribute to the uniform sampling of environment parameters. We propose Active Domain Randomization, a novel algorithm that learns a parameter sampling strategy. Our method looks for the most informative environment variations within the given randomization ranges by leveraging the discrepancies of policy rollouts in randomized and reference environment instances. We find that training more frequently on these instances leads to better overall agent generalization. Our experiments across various physics-based simulated and real-robot tasks show that this enhancement leads to more robust, consistent policies.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/home/unist/Zotero/storage/PLH287NI/Mehta et al. - 2019 - Active Domain Randomization.pdf;/home/unist/Zotero/storage/39W4ZLXC/1904.html}
}

@inproceedings{mehtaActiveDomainRandomization2020a,
  title = {Active {{Domain Randomization}}},
  booktitle = {Proceedings of the {{Conference}} on {{Robot Learning}}},
  author = {Mehta, Bhairav and Diaz, Manfred and Golemo, Florian and Pal, Christopher J. and Paull, Liam},
  year = {2020},
  month = may,
  pages = {1162--1176},
  publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v100/mehta20a.html},
  urldate = {2025-08-25},
  abstract = {Domain randomization is a popular technique for improving domain transfer, often used in a zero-shot setting when the target domain is unknown or cannot easily be used for training. In this work, we empirically examine the effects of domain randomization on agent generalization. Our experiments show that domain randomization may lead to suboptimal, high-variance policies, which we attribute to the uniform sampling of environment parameters. We propose Active Domain Randomization, a novel algorithm that learns a parameter sampling strategy. Our method looks for the most informative environment variations within the given randomization ranges by leveraging the discrepancies of policy rollouts in randomized and reference environment instances. We find that training more frequently on these instances leads to better overall agent generalization. Our experiments across various physics-based simulated and real-robot tasks show that this enhancement leads to more robust, consistent policies.},
  langid = {english},
  file = {/home/unist/Zotero/storage/K42S65MW/Mehta et al. - 2020 - Active Domain Randomization.pdf}
}

@inproceedings{metelliPropagatingUncertaintyReinforcement2019a,
  title = {Propagating {{Uncertainty}} in {{Reinforcement Learning}} via {{Wasserstein Barycenters}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Metelli, Alberto Maria and Likmeta, Amarildo and Restelli, Marcello},
  year = {2019},
  volume = {32},
  publisher = {Curran Associates, Inc.},
  url = {https://papers.nips.cc/paper\_files/paper/2019/hash/f83630579d055dc5843ae693e7cdafe0-Abstract.html},
  urldate = {2025-08-19},
  abstract = {How does the uncertainty of the value function propagate when performing temporal difference learning? In this paper, we address this question by proposing a Bayesian framework in which we employ approximate posterior distributions to model the uncertainty of the value function and Wasserstein barycenters to propagate it across state-action pairs. Leveraging on these tools, we present an algorithm, Wasserstein Q-Learning (WQL), starting in the tabular case and then, we show how it can be extended to deal with continuous domains. Furthermore, we prove that, under mild assumptions, a slight variation of WQL enjoys desirable theoretical properties in the tabular setting. Finally, we present an experimental campaign to show the effectiveness of WQL on finite problems, compared to several RL algorithms, some of which are specifically designed for exploration, along with some preliminary results on Atari games.},
  file = {/home/unist/Zotero/storage/F5X7LK5U/Metelli et al. - 2019 - Propagating Uncertainty in Reinforcement Learning via Wasserstein Barycenters.pdf}
}

@article{mnihHumanlevelControlDeep2015a,
  title = {Human-Level Control through Deep Reinforcement Learning},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
  year = {2015},
  month = feb,
  journal = {Nature},
  volume = {518},
  number = {7540},
  pages = {529--533},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/nature14236},
  url = {https://www.nature.com/articles/nature14236},
  urldate = {2025-05-07},
  abstract = {An artificial agent is developed that learns to play~a diverse range of classic Atari 2600 computer games directly from sensory experience, achieving a~performance comparable to that of an expert human player; this work paves the way to building general-purpose learning algorithms that bridge the divide between perception and action.},
  copyright = {2015 Springer Nature Limited},
  langid = {english},
  keywords = {Computer science},
  file = {/home/unist/Zotero/storage/M7TS7JNV/Mnih et al. - 2015 - Human-level control through deep reinforcement learning.pdf}
}

@misc{mnihPlayingAtariDeep2013a,
  title = {Playing {{Atari}} with {{Deep Reinforcement Learning}}},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  year = {2013},
  month = dec,
  number = {arXiv:1312.5602},
  eprint = {1312.5602},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1312.5602},
  url = {http://arxiv.org/abs/1312.5602},
  urldate = {2025-03-19},
  abstract = {We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/unist/Zotero/storage/KKKAJSYQ/Mnih et al. - 2013 - Playing Atari with Deep Reinforcement Learning.pdf;/home/unist/Zotero/storage/6EGDUN37/1312.html}
}

@misc{mohamedMonteCarloGradient2020a,
  title = {Monte {{Carlo Gradient Estimation}} in {{Machine Learning}}},
  author = {Mohamed, Shakir and Rosca, Mihaela and Figurnov, Michael and Mnih, Andriy},
  year = {2020},
  month = sep,
  number = {arXiv:1906.10652},
  eprint = {1906.10652},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1906.10652},
  url = {http://arxiv.org/abs/1906.10652},
  urldate = {2025-05-29},
  abstract = {This paper is a broad and accessible survey of the methods we have at our disposal for Monte Carlo gradient estimation in machine learning and across the statistical sciences: the problem of computing the gradient of an expectation of a function with respect to parameters defining the distribution that is integrated; the problem of sensitivity analysis. In machine learning research, this gradient problem lies at the core of many learning problems, in supervised, unsupervised and reinforcement learning. We will generally seek to rewrite such gradients in a form that allows for Monte Carlo estimation, allowing them to be easily and efficiently used and analysed. We explore three strategies--the pathwise, score function, and measure-valued gradient estimators--exploring their historical development, derivation, and underlying assumptions. We describe their use in other fields, show how they are related and can be combined, and expand on their possible generalisations. Wherever Monte Carlo gradient estimators have been derived and deployed in the past, important advances have followed. A deeper and more widely-held understanding of this problem will lead to further advances, and it is these advances that we wish to support.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/home/unist/Zotero/storage/E2ETXFSM/Mohamed et al. - 2020 - Monte Carlo Gradient Estimation in Machine Learning.pdf;/home/unist/Zotero/storage/CIV3LRPI/1906.html}
}

@misc{moonDiscoveringHierarchicalAchievements2023a,
  title = {Discovering {{Hierarchical Achievements}} in {{Reinforcement Learning}} via {{Contrastive Learning}}},
  author = {Moon, Seungyong and Yeom, Junyoung and Park, Bumsoo and Song, Hyun Oh},
  year = {2023},
  month = nov,
  number = {arXiv:2307.03486},
  eprint = {2307.03486},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2307.03486},
  url = {http://arxiv.org/abs/2307.03486},
  urldate = {2025-04-06},
  abstract = {Discovering achievements with a hierarchical structure in procedurally generated environments presents a significant challenge. This requires an agent to possess a broad range of abilities, including generalization and long-term reasoning. Many prior methods have been built upon model-based or hierarchical approaches, with the belief that an explicit module for long-term planning would be advantageous for learning hierarchical dependencies. However, these methods demand an excessive number of environment interactions or large model sizes, limiting their practicality. In this work, we demonstrate that proximal policy optimization (PPO), a simple yet versatile model-free algorithm, outperforms previous methods when optimized with recent implementation practices. Moreover, we find that the PPO agent can predict the next achievement to be unlocked to some extent, albeit with limited confidence. Based on this observation, we introduce a novel contrastive learning method, called achievement distillation, which strengthens the agent's ability to predict the next achievement. Our method exhibits a strong capacity for discovering hierarchical achievements and shows state-of-the-art performance on the challenging Crafter environment in a sample-efficient manner while utilizing fewer model parameters.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/unist/Zotero/storage/S7IP3A48/Moon et al. - 2023 - Discovering Hierarchical Achievements in Reinforcement Learning via Contrastive Learning.pdf;/home/unist/Zotero/storage/UMAJ7XVC/2307.html}
}

@misc{moonRethinkingValueFunction2023a,
  title = {Rethinking {{Value Function Learning}} for {{Generalization}} in {{Reinforcement Learning}}},
  author = {Moon, Seungyong and Lee, JunYeong and Song, Hyun Oh},
  year = {2023},
  month = jan,
  number = {arXiv:2210.09960},
  eprint = {2210.09960},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2210.09960},
  url = {http://arxiv.org/abs/2210.09960},
  urldate = {2025-04-06},
  abstract = {Our work focuses on training RL agents on multiple visually diverse environments to improve observational generalization performance. In prior methods, policy and value networks are separately optimized using a disjoint network architecture to avoid interference and obtain a more accurate value function. We identify that a value network in the multi-environment setting is more challenging to optimize and prone to memorizing the training data than in the conventional single-environment setting. In addition, we find that appropriate regularization on the value network is necessary to improve both training and test performance. To this end, we propose Delayed-Critic Policy Gradient (DCPG), a policy gradient algorithm that implicitly penalizes value estimates by optimizing the value network less frequently with more training data than the policy network. This can be implemented using a single unified network architecture. Furthermore, we introduce a simple self-supervised task that learns the forward and inverse dynamics of environments using a single discriminator, which can be jointly optimized with the value network. Our proposed algorithms significantly improve observational generalization performance and sample efficiency on the Procgen Benchmark.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/unist/Zotero/storage/D5W45TIN/Moon et al. - 2023 - Rethinking Value Function Learning for Generalization in Reinforcement Learning.pdf;/home/unist/Zotero/storage/EXHRRCLW/2210.html}
}

@inproceedings{muratoreNeuralPosteriorDomain2022,
  title = {Neural {{Posterior Domain Randomization}}},
  booktitle = {Proceedings of the 5th {{Conference}} on {{Robot Learning}}},
  author = {Muratore, Fabio and Gruner, Theo and Wiese, Florian and Belousov, Boris and Gienger, Michael and Peters, Jan},
  year = {2022},
  month = jan,
  pages = {1532--1542},
  publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v164/muratore22a.html},
  urldate = {2025-08-25},
  abstract = {Combining domain randomization and reinforcement learning is a widely used approach to obtain control policies that can bridge the gap between simulation and reality. However, existing methods make limiting assumptions on the form of the domain parameter distribution which prevents them from utilizing the full power of domain randomization. Typically, a restricted family of probability distributions (e.g., normal or uniform) is chosen a priori for every parameter. Furthermore, straightforward approaches based on deep learning require differentiable simulators, which are either not available or can only simulate a limited class of systems. Such rigid assumptions diminish the applicability of domain randomization in robotics. Building upon recently proposed neural likelihood-free inference methods, we introduce Neural Posterior Domain Randomization (NPDR), an algorithm that alternates between learning a policy from a randomized simulator and adapting the posterior distribution over the simulator's parameters in a Bayesian fashion. Our approach only requires a parameterized simulator, coarse prior ranges, a policy (optionally with optimization routine), and a small set of real-world observations. Most importantly, the domain parameter distribution is not restricted to a specific family, parameters can be correlated, and the simulator does not have to be differentiable. We show that the presented method is able to efficiently adapt the posterior over the domain parameters to closer match the observed dynamics. Moreover, we demonstrate that NPDR can learn transferable policies using fewer real-world rollouts than comparable algorithms.},
  langid = {english},
  file = {/home/unist/Zotero/storage/TASBPF8C/Muratore et al. - 2022 - Neural Posterior Domain Randomization.pdf}
}

@misc{odonoghueCombiningPolicyGradient2017a,
  title = {Combining Policy Gradient and {{Q-learning}}},
  author = {O'Donoghue, Brendan and Munos, Remi and Kavukcuoglu, Koray and Mnih, Volodymyr},
  year = {2017},
  month = apr,
  number = {arXiv:1611.01626},
  eprint = {1611.01626},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1611.01626},
  url = {http://arxiv.org/abs/1611.01626},
  urldate = {2025-04-12},
  abstract = {Policy gradient is an efficient technique for improving a policy in a reinforcement learning setting. However, vanilla online variants are on-policy only and not able to take advantage of off-policy data. In this paper we describe a new technique that combines policy gradient with off-policy Q-learning, drawing experience from a replay buffer. This is motivated by making a connection between the fixed points of the regularized policy gradient algorithm and the Q-values. This connection allows us to estimate the Q-values from the action preferences of the policy, to which we apply Q-learning updates. We refer to the new technique as 'PGQL', for policy gradient and Q-learning. We also establish an equivalency between action-value fitting techniques and actor-critic algorithms, showing that regularized policy gradient techniques can be interpreted as advantage function learning algorithms. We conclude with some numerical examples that demonstrate improved data efficiency and stability of PGQL. In particular, we tested PGQL on the full suite of Atari games and achieved performance exceeding that of both asynchronous advantage actor-critic (A3C) and Q-learning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Mathematics - Optimization and Control,pgo,Statistics - Machine Learning},
  file = {/home/unist/Zotero/storage/EABLU43D/O'Donoghue et al. - 2017 - Combining policy gradient and Q-learning.pdf;/home/unist/Zotero/storage/YP5NWBPI/1611.html}
}

@misc{oordRepresentationLearningContrastive2019a,
  title = {Representation {{Learning}} with {{Contrastive Predictive Coding}}},
  author = {van den Oord, Aaron and Li, Yazhe and Vinyals, Oriol},
  year = {2019},
  month = jan,
  number = {arXiv:1807.03748},
  eprint = {1807.03748},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1807.03748},
  url = {http://arxiv.org/abs/1807.03748},
  urldate = {2025-05-24},
  abstract = {While supervised learning has enabled great progress in many applications, unsupervised learning has not seen such widespread adoption, and remains an important and challenging endeavor for artificial intelligence. In this work, we propose a universal unsupervised learning approach to extract useful representations from high-dimensional data, which we call Contrastive Predictive Coding. The key insight of our model is to learn such representations by predicting the future in latent space by using powerful autoregressive models. We use a probabilistic contrastive loss which induces the latent space to capture information that is maximally useful to predict future samples. It also makes the model tractable by using negative sampling. While most prior work has focused on evaluating representations for a particular modality, we demonstrate that our approach is able to learn useful representations achieving strong performance on four distinct domains: speech, images, text and reinforcement learning in 3D environments.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/unist/Zotero/storage/KDDPQAE3/Oord et al. - 2019 - Representation Learning with Contrastive Predictive Coding.pdf;/home/unist/Zotero/storage/N984DCXQ/1807.html}
}

@misc{openaiSolvingRubiksCube2019,
  title = {Solving {{Rubik}}'s {{Cube}} with a {{Robot Hand}}},
  author = {OpenAI and Akkaya, Ilge and Andrychowicz, Marcin and Chociej, Maciek and Litwin, Mateusz and McGrew, Bob and Petron, Arthur and Paino, Alex and Plappert, Matthias and Powell, Glenn and Ribas, Raphael and Schneider, Jonas and Tezak, Nikolas and Tworek, Jerry and Welinder, Peter and Weng, Lilian and Yuan, Qiming and Zaremba, Wojciech and Zhang, Lei},
  year = {2019},
  month = oct,
  number = {arXiv:1910.07113},
  eprint = {1910.07113},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1910.07113},
  url = {http://arxiv.org/abs/1910.07113},
  urldate = {2025-08-25},
  abstract = {We demonstrate that models trained only in simulation can be used to solve a manipulation problem of unprecedented complexity on a real robot. This is made possible by two key components: a novel algorithm, which we call automatic domain randomization (ADR) and a robot platform built for machine learning. ADR automatically generates a distribution over randomized environments of ever-increasing difficulty. Control policies and vision state estimators trained with ADR exhibit vastly improved sim2real transfer. For control policies, memory-augmented models trained on an ADR-generated distribution of environments show clear signs of emergent meta-learning at test time. The combination of ADR with our custom robot platform allows us to solve a Rubik's cube with a humanoid robot hand, which involves both control and state estimation problems. Videos summarizing our results are available: https://openai.com/blog/solving-rubiks-cube/},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning},
  file = {/home/unist/Zotero/storage/7ZJ8HD3Y/OpenAI et al. - 2019 - Solving Rubik's Cube with a Robot Hand.pdf;/home/unist/Zotero/storage/P3S9MXWD/1910.html}
}

@book{p.murphyProbabilisticMachineLearningb,
  title = {Probabilistic {{Machine Learning Advanced}}},
  author = {P.Murphy, Kevin},
  file = {/home/unist/Zotero/storage/Y3VZLW2U/P.Murphy - Probabilistic Machine Learning Advanced.pdf}
}

@book{p.murphyProbabilisticMachineLearningc,
  title = {Probabilistic {{Machine Learning}} an {{Introduction}}},
  author = {P.Murphy, Kevin},
  file = {/home/unist/Zotero/storage/F26VY5ZR/book1.pdf}
}

@misc{parkFlowQLearning2025a,
  title = {Flow {{Q-Learning}}},
  author = {Park, Seohong and Li, Qiyang and Levine, Sergey},
  year = {2025},
  month = may,
  number = {arXiv:2502.02538},
  eprint = {2502.02538},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2502.02538},
  url = {http://arxiv.org/abs/2502.02538},
  urldate = {2025-08-19},
  abstract = {We present flow Q-learning (FQL), a simple and performant offline reinforcement learning (RL) method that leverages an expressive flow-matching policy to model arbitrarily complex action distributions in data. Training a flow policy with RL is a tricky problem, due to the iterative nature of the action generation process. We address this challenge by training an expressive one-step policy with RL, rather than directly guiding an iterative flow policy to maximize values. This way, we can completely avoid unstable recursive backpropagation, eliminate costly iterative action generation at test time, yet still mostly maintain expressivity. We experimentally show that FQL leads to strong performance across 73 challenging state- and pixel-based OGBench and D4RL tasks in offline RL and offline-to-online RL. Project page: https://seohong.me/projects/fql/},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/unist/Zotero/storage/L8L8YAZD/Park et al. - 2025 - Flow Q-Learning.pdf;/home/unist/Zotero/storage/G2GPZMII/2502.html}
}

@inproceedings{parrAnalysisLinearModels2008a,
  title = {An Analysis of Linear Models, Linear Value-Function Approximation, and Feature Selection for Reinforcement Learning},
  booktitle = {Proceedings of the 25th International Conference on {{Machine}} Learning - {{ICML}} '08},
  author = {Parr, Ronald and Li, Lihong and Taylor, Gavin and {Painter-Wakefield}, Christopher and Littman, Michael L.},
  year = {2008},
  pages = {752--759},
  publisher = {ACM Press},
  address = {Helsinki, Finland},
  doi = {10.1145/1390156.1390251},
  url = {http://portal.acm.org/citation.cfm?doid=1390156.1390251},
  urldate = {2025-08-14},
  abstract = {We show that linear value-function approximation is equivalent to a form of linear model approximation. We then derive a relationship between the model-approximation error and the Bellman error, and show how this relationship can guide feature selection for model improvement and/or value-function improvement. We also show how these results give insight into the behavior of existing feature-selection algorithms.},
  copyright = {https://www.acm.org/publications/policies/copyright\_policy\#Background},
  isbn = {978-1-60558-205-4},
  langid = {english},
  file = {/home/unist/Zotero/storage/YLL4Y8UK/Parr et al. - 2008 - An analysis of linear models, linear value-function approximation, and feature selection for reinfor.pdf}
}

@book{PMAa,
  title = {{{PMA}}},
  file = {/home/unist/Zotero/storage/K2ASUH32/Mathematical Analysis Guide by Kit-Wing Yu.pdf;/home/unist/Zotero/storage/R7TDX8IH/PMA.pdf}
}

@misc{pouplinSynergyLLMsRL2025a,
  title = {The {{Synergy}} of {{LLMs}} \& {{RL Unlocks Offline Learning}} of {{Generalizable Language-Conditioned Policies}} with {{Low-fidelity Data}}},
  author = {Pouplin, Thomas and Kobalczyk, Katarzyna and Sun, Hao and van der Schaar, Mihaela},
  year = {2025},
  month = jun,
  number = {arXiv:2412.06877},
  eprint = {2412.06877},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2412.06877},
  url = {http://arxiv.org/abs/2412.06877},
  urldate = {2025-08-19},
  abstract = {Developing autonomous agents capable of performing complex, multi-step decision-making tasks specified in natural language remains a significant challenge, particularly in realistic settings where labeled data is scarce and real-time experimentation is impractical. Existing reinforcement learning (RL) approaches often struggle to generalize to unseen goals and states, limiting their applicability. In this paper, we introduce TEDUO, a novel training pipeline for offline language-conditioned policy learning in symbolic environments. Unlike conventional methods, TEDUO operates on readily available, unlabeled datasets and addresses the challenge of generalization to previously unseen goals and states. Our approach harnesses large language models (LLMs) in a dual capacity: first, as automatization tools augmenting offline datasets with richer annotations, and second, as generalizable instruction-following agents. Empirical results demonstrate that TEDUO achieves data-efficient learning of robust language-conditioned policies, accomplishing tasks beyond the reach of conventional RL frameworks or out-of-the-box LLMs alone.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/unist/Zotero/storage/Q2NU7GKF/Pouplin et al. - 2025 - The Synergy of LLMs & RL Unlocks Offline Learning of Generalizable Language-Conditioned Policies wit.pdf}
}

@misc{pressUsingOutputEmbedding2017a,
  title = {Using the {{Output Embedding}} to {{Improve Language Models}}},
  author = {Press, Ofir and Wolf, Lior},
  year = {2017},
  month = feb,
  number = {arXiv:1608.05859},
  eprint = {1608.05859},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1608.05859},
  url = {http://arxiv.org/abs/1608.05859},
  urldate = {2025-05-27},
  abstract = {We study the topmost weight matrix of neural network language models. We show that this matrix constitutes a valid word embedding. When training language models, we recommend tying the input embedding and this output embedding. We analyze the resulting update rules and show that the tied embedding evolves in a more similar way to the output embedding than to the input embedding in the untied model. We also offer a new method of regularizing the output embedding. Our methods lead to a significant reduction in perplexity, as we are able to show on a variety of neural network language models. Finally, we show that weight tying can reduce the size of neural translation models to less than half of their original size without harming their performance.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/unist/Zotero/storage/KM2VKP4H/Press and Wolf - 2017 - Using the Output Embedding to Improve Language Models.pdf;/home/unist/Zotero/storage/S2DQ5E84/1608.html}
}

@article{rawlikStochasticOptimalControla,
  title = {On {{Stochastic Optimal Control}} and {{Reinforcement Learning}} by {{Approximate Inference}}},
  author = {Rawlik, Konrad and Toussaint, Marc and Vijayakumar, Sethu},
  abstract = {We present a reformulation of the stochastic optimal control problem in terms of KL divergence minimisation, not only providing a unifying perspective of previous approaches in this area, but also demonstrating that the formalism leads to novel practical approaches to the control problem. Specifically, a natural relaxation of the dual formulation gives rise to exact iterative solutions to the finite and infinite horizon stochastic optimal control problem, while direct application of Bayesian inference methods yields instances of risk sensitive control. We furthermore study corresponding formulations in the reinforcement learning setting and present model free algorithms for problems with both discrete and continuous state and action spaces. Evaluation of the proposed methods on the standard Gridworld and Cart-Pole benchmarks verifies the theoretical insights and shows that the proposed methods improve upon current approaches.},
  langid = {english},
  file = {/home/unist/Zotero/storage/M56WGRRA/Rawlik et al. - On Stochastic Optimal Control and Reinforcement Learning by Approximate Inference.pdf}
}

@book{ReinforcementLearninga,
  title = {Reinforcement {{Learning}}},
  file = {/home/unist/Zotero/storage/B5ZGLI7Y/adsf.pdf}
}

@incollection{rubinTradingValueInformation2012a,
  title = {Trading {{Value}} and {{Information}} in {{MDPs}}},
  booktitle = {Decision {{Making}} with {{Imperfect Decision Makers}}},
  author = {Rubin, Jonathan and Shamir, Ohad and Tishby, Naftali},
  editor = {Kacprzyk, Janusz and Jain, Lakhmi C. and Guy, Tatiana Valentine and K{\'a}rn{\'y}, Miroslav and Wolpert, David H.},
  year = {2012},
  volume = {28},
  pages = {57--74},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-24647-0_3},
  url = {http://link.springer.com/10.1007/978-3-642-24647-0\_3},
  urldate = {2025-04-19},
  abstract = {Interactions between an organism and its environment are commonly treated in the framework of Markov Decision Processes (MDP). While standard MDP is aimed at maximizing expected future rewards (value), the circular flow of information between the agent and its environment is generally ignored. In particular, the information gained from the environment by means of perception and the information involved in the process of action selection are not treated in the standard MDP setting. In this paper, we focus on the control information and show how it can be combined with the reward measure in a unified way. Both of these measures satisfy the familiar Bellman recursive equations, and their linear combination (the free-energy) provides an interesting new optimization criterion. The tradeoff between value and information, explored using our INFO-RL algorithm, provides a principled justification for stochastic (soft) policies. We use computational learning theory to show that these optimal policies are also robust to uncertainties in the reward values.},
  isbn = {978-3-642-24646-3 978-3-642-24647-0},
  langid = {english},
  file = {/home/unist/Zotero/storage/PRMWWFHV/Rubin et al. - 2012 - Trading Value and Information in MDPs.pdf}
}

@misc{ruderOverviewGradientDescent2017a,
  title = {An Overview of Gradient Descent Optimization Algorithms},
  author = {Ruder, Sebastian},
  year = {2017},
  month = jun,
  number = {arXiv:1609.04747},
  eprint = {1609.04747},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1609.04747},
  url = {http://arxiv.org/abs/1609.04747},
  urldate = {2025-05-26},
  abstract = {Gradient descent optimization algorithms, while increasingly popular, are often used as black-box optimizers, as practical explanations of their strengths and weaknesses are hard to come by. This article aims to provide the reader with intuitions with regard to the behaviour of different algorithms that will allow her to put them to use. In the course of this overview, we look at different variants of gradient descent, summarize challenges, introduce the most common optimization algorithms, review architectures in a parallel and distributed setting, and investigate additional strategies for optimizing gradient descent.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/unist/Zotero/storage/YDF7CNLK/Ruder - 2017 - An overview of gradient descent optimization algorithms.pdf;/home/unist/Zotero/storage/RUF7GMTY/1609.html}
}

@misc{salimansWeightNormalizationSimple2016a,
  title = {Weight {{Normalization}}: {{A Simple Reparameterization}} to {{Accelerate Training}} of {{Deep Neural Networks}}},
  shorttitle = {Weight {{Normalization}}},
  author = {Salimans, Tim and Kingma, Diederik P.},
  year = {2016},
  month = jun,
  number = {arXiv:1602.07868},
  eprint = {1602.07868},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1602.07868},
  url = {http://arxiv.org/abs/1602.07868},
  urldate = {2025-05-29},
  abstract = {We present weight normalization: a reparameterization of the weight vectors in a neural network that decouples the length of those weight vectors from their direction. By reparameterizing the weights in this way we improve the conditioning of the optimization problem and we speed up convergence of stochastic gradient descent. Our reparameterization is inspired by batch normalization but does not introduce any dependencies between the examples in a minibatch. This means that our method can also be applied successfully to recurrent models such as LSTMs and to noise-sensitive applications such as deep reinforcement learning or generative models, for which batch normalization is less well suited. Although our method is much simpler, it still provides much of the speed-up of full batch normalization. In addition, the computational overhead of our method is lower, permitting more optimization steps to be taken in the same amount of time. We demonstrate the usefulness of our method on applications in supervised image recognition, generative modelling, and deep reinforcement learning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/home/unist/Zotero/storage/WBIVLUW7/Salimans and Kingma - 2016 - Weight Normalization A Simple Reparameterization to Accelerate Training of Deep Neural Networks.pdf}
}

@article{sallansReinforcementLearningFactoreda,
  title = {Reinforcement {{Learning}} with {{Factored States}} and {{Actions}}},
  author = {Sallans, Brian and At, Oefai and Hinton, Geoffrey E},
  abstract = {A novel approximation method is presented for approximating the value function and selecting good actions for Markov decision processes with large state and action spaces. The method approximates state-action values as negative free energies in an undirected graphical model called a product of experts. The model parameters can be learned efficiently because values and derivatives can be efficiently computed for a product of experts. Actions can be found even in large factored action spaces by the use of Markov chain Monte Carlo sampling. Simulation results show that the product of experts approximation can be used to solve large problems. In one simulation it is used to find actions in action spaces of size 240.},
  langid = {english},
  file = {/home/unist/Zotero/storage/N6VCPNFL/Sallans et al. - Reinforcement Learning with Factored States and Actions.pdf}
}

@misc{santosNumberTrialsMatters2025a,
  title = {The {{Number}} of {{Trials Matters}} in {{Infinite-Horizon General-Utility Markov Decision Processes}}},
  author = {Santos, Pedro P. and Sardinha, Alberto and Melo, Francisco S.},
  year = {2025},
  month = jul,
  number = {arXiv:2409.15128},
  eprint = {2409.15128},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2409.15128},
  url = {http://arxiv.org/abs/2409.15128},
  urldate = {2025-08-20},
  abstract = {The general-utility Markov decision processes (GUMDPs) framework generalizes the MDPs framework by considering objective functions that depend on the frequency of visitation of state-action pairs induced by a given policy. In this work, we contribute with the first analysis on the impact of the number of trials, i.e., the number of randomly sampled trajectories, in infinite-horizon GUMDPs. We show that, as opposed to standard MDPs, the number of trials plays a key-role in infinite-horizon GUMDPs and the expected performance of a given policy depends, in general, on the number of trials. We consider both discounted and average GUMDPs, where the objective function depends, respectively, on discounted and average frequencies of visitation of state-action pairs. First, we study policy evaluation under discounted GUMDPs, proving lower and upper bounds on the mismatch between the finite and infinite trials formulations for GUMDPs. Second, we address average GUMDPs, studying how different classes of GUMDPs impact the mismatch between the finite and infinite trials formulations. Third, we provide a set of empirical results to support our claims, highlighting how the number of trajectories and the structure of the underlying GUMDP influence policy evaluation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/unist/Zotero/storage/HLLWFBCI/Santos et al. - 2025 - The Number of Trials Matters in Infinite-Horizon General-Utility Markov Decision Processes.pdf;/home/unist/Zotero/storage/83RQEV9J/2409.html}
}

@misc{schaulPrioritizedExperienceReplay2016a,
  title = {Prioritized {{Experience Replay}}},
  author = {Schaul, Tom and Quan, John and Antonoglou, Ioannis and Silver, David},
  year = {2016},
  month = feb,
  number = {arXiv:1511.05952},
  eprint = {1511.05952},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1511.05952},
  url = {http://arxiv.org/abs/1511.05952},
  urldate = {2025-03-26},
  abstract = {Experience replay lets online reinforcement learning agents remember and reuse experiences from the past. In prior work, experience transitions were uniformly sampled from a replay memory. However, this approach simply replays transitions at the same frequency that they were originally experienced, regardless of their significance. In this paper we develop a framework for prioritizing experience, so as to replay important transitions more frequently, and therefore learn more efficiently. We use prioritized experience replay in Deep Q-Networks (DQN), a reinforcement learning algorithm that achieved human-level performance across many Atari games. DQN with prioritized experience replay achieves a new state-of-the-art, outperforming DQN with uniform replay on 41 out of 49 games.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/unist/Zotero/storage/FBNB9IFR/Schaul et al. - 2016 - Prioritized Experience Replay.pdf;/home/unist/Zotero/storage/W9H6U3QT/1511.html}
}

@misc{schulmanHighDimensionalContinuousControl2018a,
  title = {High-{{Dimensional Continuous Control Using Generalized Advantage Estimation}}},
  shorttitle = {{{GAE}}},
  author = {Schulman, John and Moritz, Philipp and Levine, Sergey and Jordan, Michael and Abbeel, Pieter},
  year = {2018},
  month = oct,
  number = {arXiv:1506.02438},
  eprint = {1506.02438},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1506.02438},
  url = {http://arxiv.org/abs/1506.02438},
  urldate = {2025-05-19},
  abstract = {Policy gradient methods are an appealing approach in reinforcement learning because they directly optimize the cumulative reward and can straightforwardly be used with nonlinear function approximators such as neural networks. The two main challenges are the large number of samples typically required, and the difficulty of obtaining stable and steady improvement despite the nonstationarity of the incoming data. We address the first challenge by using value functions to substantially reduce the variance of policy gradient estimates at the cost of some bias, with an exponentially-weighted estimator of the advantage function that is analogous to TD(lambda). We address the second challenge by using trust region optimization procedure for both the policy and the value function, which are represented by neural networks. Our approach yields strong empirical results on highly challenging 3D locomotion tasks, learning running gaits for bipedal and quadrupedal simulated robots, and learning a policy for getting the biped to stand up from starting out lying on the ground. In contrast to a body of prior work that uses hand-crafted policy representations, our neural network policies map directly from raw kinematics to joint torques. Our algorithm is fully model-free, and the amount of simulated experience required for the learning tasks on 3D bipeds corresponds to 1-2 weeks of real time.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics,Computer Science - Systems and Control,GAE,generalized advantage estimation},
  file = {/home/unist/Zotero/storage/VHREDSKG/Schulman et al. - 2018 - High-Dimensional Continuous Control Using Generalized Advantage Estimation.pdf;/home/unist/Zotero/storage/FDDLSHNP/1506.html}
}

@misc{schulmanProximalPolicyOptimization2017,
  title = {Proximal {{Policy Optimization Algorithms}}},
  author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  year = {2017},
  month = aug,
  number = {arXiv:1707.06347},
  eprint = {1707.06347},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1707.06347},
  url = {http://arxiv.org/abs/1707.06347},
  urldate = {2025-08-27},
  abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/unist/Zotero/storage/V4KENI9N/Schulman et al. - 2017 - Proximal Policy Optimization Algorithms.pdf;/home/unist/Zotero/storage/LFQA3V3F/1707.html}
}

@misc{schulmanTrustRegionPolicy2017a,
  title = {Trust {{Region Policy Optimization}}},
  author = {Schulman, John and Levine, Sergey and Moritz, Philipp and Jordan, Michael I. and Abbeel, Pieter},
  year = {2017},
  month = apr,
  number = {arXiv:1502.05477},
  eprint = {1502.05477},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1502.05477},
  url = {http://arxiv.org/abs/1502.05477},
  urldate = {2025-08-09},
  abstract = {We describe an iterative procedure for optimizing policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justified procedure, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO). This algorithm is similar to natural policy gradient methods and is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,trpo},
  file = {/home/unist/Zotero/storage/G7GKELJZ/Schulman 등 - 2017 - Trust Region Policy Optimization.pdf;/home/unist/Zotero/storage/YFWQBZF2/1502.html}
}

@misc{schultzMasteringBoardGames2025a,
  title = {Mastering {{Board Games}} by {{External}} and {{Internal Planning}} with {{Language Models}}},
  author = {Schultz, John and Adamek, Jakub and Jusup, Matej and Lanctot, Marc and Kaisers, Michael and Perrin, Sarah and Hennes, Daniel and Shar, Jeremy and Lewis, Cannada and Ruoss, Anian and Zahavy, Tom and Veli{\v c}kovi{\'c}, Petar and Prince, Laurel and Singh, Satinder and Malmi, Eric and Toma{\v s}ev, Nenad},
  year = {2025},
  month = may,
  number = {arXiv:2412.12119},
  eprint = {2412.12119},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2412.12119},
  url = {http://arxiv.org/abs/2412.12119},
  urldate = {2025-08-20},
  abstract = {Advancing planning and reasoning capabilities of Large Language Models (LLMs) is one of the key prerequisites towards unlocking their potential for performing reliably in complex and impactful domains. In this paper, we aim to demonstrate this across board games (Chess, Fischer Random / Chess960, Connect Four, and Hex), and we show that search-based planning can yield significant improvements in LLM game-playing strength. We introduce, compare and contrast two major approaches: In external search, the model guides Monte Carlo Tree Search (MCTS) rollouts and evaluations without calls to an external game engine, and in internal search, the model is trained to generate in-context a linearized tree of search and a resulting final choice. Both build on a language model pre-trained on relevant domain knowledge, reliably capturing the transition and value functions in the respective environments, with minimal hallucinations. We evaluate our LLM search implementations against game-specific state-of-the-art engines, showcasing substantial improvements in strength over the base model, and reaching Grandmaster-level performance in chess while operating closer to the human search budget. Our proposed approach, combining search with domain knowledge, is not specific to board games, hinting at more general future applications.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/unist/Zotero/storage/WW57KFYF/Schultz et al. - 2025 - Mastering Board Games by External and Internal Planning with Language Models.pdf;/home/unist/Zotero/storage/BNJ23KZI/2412.html}
}

@misc{sikchiDualRLUnification2024a,
  title = {Dual {{RL}}: {{Unification}} and {{New Methods}} for {{Reinforcement}} and {{Imitation Learning}}},
  shorttitle = {Dual {{RL}}},
  author = {Sikchi, Harshit and Zheng, Qinqing and Zhang, Amy and Niekum, Scott},
  year = {2024},
  month = jan,
  number = {arXiv:2302.08560},
  eprint = {2302.08560},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2302.08560},
  url = {http://arxiv.org/abs/2302.08560},
  urldate = {2025-08-21},
  abstract = {The goal of reinforcement learning (RL) is to find a policy that maximizes the expected cumulative return. It has been shown that this objective can be represented as an optimization problem of state-action visitation distribution under linear constraints. The dual problem of this formulation, which we refer to as dual RL, is unconstrained and easier to optimize. In this work, we first cast several state-of-the-art offline RL and offline imitation learning (IL) algorithms as instances of dual RL approaches with shared structures. Such unification allows us to identify the root cause of the shortcomings of prior methods. For offline IL, our analysis shows that prior methods are based on a restrictive coverage assumption that greatly limits their performance in practice. To fix this limitation, we propose a new discriminator-free method ReCOIL that learns to imitate from arbitrary off-policy data to obtain near-expert performance. For offline RL, our analysis frames a recent offline RL method XQL in the dual framework, and we further propose a new method f-DVL that provides alternative choices to the Gumbel regression loss that fixes the known training instability issue of XQL. The performance improvements by both of our proposed methods, ReCOIL and f-DVL, in IL and RL are validated on an extensive suite of simulated robot locomotion and manipulation tasks. Project code and details can be found at this https://hari-sikchi.github.io/dual-rl.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/home/unist/Zotero/storage/Q74D7HWW/Sikchi et al. - 2024 - Dual RL Unification and New Methods for Reinforcement and Imitation Learning.pdf;/home/unist/Zotero/storage/XNBTLXBU/2302.html}
}

@article{silverDeterministicPolicyGradienta,
  title = {Deterministic {{Policy Gradient Algorithms}}},
  author = {Silver, David and Lever, Guy and Heess, Nicolas and Degris, Thomas and Wierstra, Daan and Riedmiller, Martin},
  abstract = {In this paper we consider deterministic policy gradient algorithms for reinforcement learning with continuous actions. The deterministic policy gradient has a particularly appealing form: it is the expected gradient of the action-value function. This simple form means that the deterministic policy gradient can be estimated much more efficiently than the usual stochastic policy gradient. To ensure adequate exploration, we introduce an off-policy actor-critic algorithm that learns a deterministic target policy from an exploratory behaviour policy. We demonstrate that deterministic policy gradient algorithms can significantly outperform their stochastic counterparts in high-dimensional action spaces.},
  langid = {english},
  file = {/home/unist/Zotero/storage/X5SYZU8H/Silver et al. - Deterministic Policy Gradient Algorithms.pdf;/home/unist/Zotero/storage/X9I8JC4Q/Deterministic Policy Gradient Algorithms-Supplementary Material.pdf}
}

@misc{songCompositionalConservatismTransductive2024a,
  title = {Compositional {{Conservatism}}: {{A Transductive Approach}} in {{Offline Reinforcement Learning}}},
  shorttitle = {Compositional {{Conservatism}}},
  author = {Song, Yeda and Lee, Dongwook and Kim, Gunhee},
  year = {2024},
  month = apr,
  number = {arXiv:2404.04682},
  eprint = {2404.04682},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2404.04682},
  url = {http://arxiv.org/abs/2404.04682},
  urldate = {2025-03-30},
  abstract = {Offline reinforcement learning (RL) is a compelling framework for learning optimal policies from past experiences without additional interaction with the environment. Nevertheless, offline RL inevitably faces the problem of distributional shifts, where the states and actions encountered during policy execution may not be in the training dataset distribution. A common solution involves incorporating conservatism into the policy or the value function to safeguard against uncertainties and unknowns. In this work, we focus on achieving the same objectives of conservatism but from a different perspective. We propose COmpositional COnservatism with Anchor-seeking (COCOA) for offline RL, an approach that pursues conservatism in a compositional manner on top of the transductive reparameterization (Netanyahu et al., 2023), which decomposes the input variable (the state in our case) into an anchor and its difference from the original input. Our COCOA seeks both in-distribution anchors and differences by utilizing the learned reverse dynamics model, encouraging conservatism in the compositional input space for the policy or value function. Such compositional conservatism is independent of and agnostic to the prevalent behavioral conservatism in offline RL. We apply COCOA to four state-of-the-art offline RL algorithms and evaluate them on the D4RL benchmark, where COCOA generally improves the performance of each algorithm. The code is available at https://github.com/runamu/compositional-conservatism.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/home/unist/Zotero/storage/Z9PTDLHK/Song et al. - 2024 - Compositional Conservatism A Transductive Approach in Offline Reinforcement Learning.pdf;/home/unist/Zotero/storage/2PJM8XNM/2404.html}
}

@book{steinRealAnalysisMeasure2005a,
  title = {Real Analysis: Measure Theory, Integration, and {{Hilbert}} Spaces},
  shorttitle = {Real Analysis},
  author = {Stein, Elias M. and Shakarchi, Rami},
  year = {2005},
  series = {Princeton Lectures in Analysis},
  number = {3},
  publisher = {Princeton university press},
  address = {Princeton (N.J.)},
  isbn = {978-0-691-11386-9},
  langid = {english},
  lccn = {515.7},
  file = {/home/unist/Zotero/storage/X8IRKX8S/Stein and Shakarchi - 2005 - Real analysis measure theory, integration, and Hilbert spaces.pdf}
}

@book{steinRealAnalysisMeasurea,
  title = {Real {{Analysis}}: {{Measure Theory}}, {{Integration}} and {{Hilbert Spaces}}},
  author = {Stein, Elias M. and Shakarchi, Rami},
  file = {/home/unist/Zotero/storage/ITVHCYA6/_.pdf}
}

@book{stevenIntroductionMathematicalAnalysisa,
  title = {Introduction to {{Mathematical Analysis}}},
  author = {Steven, A.Douglass},
  file = {/home/unist/Zotero/storage/PKJ5CZT5/_.pdf;/home/unist/Zotero/storage/Q3RTRBKE/Douglass Solution.pdf}
}

@inproceedings{suttonFastGradientdescentMethods2009a,
  title = {Fast Gradient-Descent Methods for Temporal-Difference Learning with Linear Function Approximation},
  booktitle = {Proceedings of the 26th {{Annual International Conference}} on {{Machine Learning}}},
  author = {Sutton, Richard S. and Maei, Hamid Reza and Precup, Doina and Bhatnagar, Shalabh and Silver, David and Szepesv{\'a}ri, Csaba and Wiewiora, Eric},
  year = {2009},
  month = jun,
  pages = {993--1000},
  publisher = {ACM},
  address = {Montreal Quebec Canada},
  doi = {10.1145/1553374.1553501},
  url = {https://dl.acm.org/doi/10.1145/1553374.1553501},
  urldate = {2025-04-09},
  abstract = {Sutton, Szepesva{\textasciiacute}ri and Maei (2009) recently introduced the first temporal-difference learning algorithm compatible with both linear function approximation and off-policy training, and whose complexity scales only linearly in the size of the function approximator. Although their gradient temporal difference (GTD) algorithm converges reliably, it can be very slow compared to conventional linear TD (on on-policy problems where TD is convergent), calling into question its practical utility. In this paper we introduce two new related algorithms with better convergence rates. The first algorithm, GTD2, is derived and proved convergent just as GTD was, but uses a different objective function and converges significantly faster (but still not as fast as conventional TD). The second new algorithm, linear TD with gradient correction, or TDC, uses the same update rule as conventional TD except for an additional term which is initially zero. In our experiments on small test problems and in a Computer Go application with a million features, the learning rate of this algorithm was comparable to that of conventional TD. This algorithm appears to extend linear TD to off-policy learning with no penalty in performance while only doubling computational requirements.},
  isbn = {978-1-60558-516-1},
  langid = {english},
  file = {/home/unist/Zotero/storage/GW6BJ7TH/Sutton et al. - 2009 - Fast gradient-descent methods for temporal-difference learning with linear function approximation.pdf}
}

@inproceedings{suttonPolicyGradientMethods1999a,
  title = {Policy {{Gradient Methods}} for {{Reinforcement Learning}} with {{Function Approximation}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Sutton, Richard S and McAllester, David and Singh, Satinder and Mansour, Yishay},
  year = {1999},
  volume = {12},
  publisher = {MIT Press},
  url = {https://papers.nips.cc/paper\_files/paper/1999/hash/464d828b85b0bed98e80ade0a5c43b0f-Abstract.html},
  urldate = {2025-04-09},
  abstract = {Function  approximation  is  essential  to  reinforcement  learning,  but  the standard approach of approximating a  value function and deter(cid:173) mining  a  policy  from  it  has so  far  proven theoretically  intractable.  In this paper we explore an alternative approach in which the policy  is explicitly represented by its own function approximator,  indepen(cid:173) dent of the value function,  and is  updated according to the gradient  of expected reward with respect to the policy parameters.  Williams's  REINFORCE method and actor-critic methods are examples of this  approach.  Our  main  new  result  is  to  show  that  the  gradient  can  be  written  in  a  form  suitable  for  estimation  from  experience  aided  by  an  approximate  action-value  or  advantage  function.  Using  this  result,  we  prove for  the first  time that a  version  of policy  iteration  with arbitrary differentiable function approximation is convergent to  a  locally optimal policy.},
  keywords = {policy gradient},
  file = {/home/unist/Zotero/storage/F3S9XSME/Sutton et al. - 1999 - Policy Gradient Methods for Reinforcement Learning with Function Approximation.pdf}
}

@inproceedings{thomasBiasNaturalActorCritic2014a,
  title = {Bias in {{Natural Actor-Critic Algorithms}}},
  booktitle = {Proceedings of the 31st {{International Conference}} on {{Machine Learning}}},
  author = {Thomas, Philip},
  year = {2014},
  month = jan,
  pages = {441--448},
  publisher = {PMLR},
  issn = {1938-7228},
  url = {https://proceedings.mlr.press/v32/thomas14.html},
  urldate = {2025-04-07},
  abstract = {We show that several popular discounted reward natural actor-critics, including the popular NAC-LSTD and eNAC algorithms, do not generate unbiased estimates of the natural policy gradient as claimed. We derive the first unbiased discounted reward natural actor-critics using batch and iterative approaches to gradient estimation. We argue that the bias makes the existing algorithms more appropriate for the average reward setting. We also show that, when Sarsa(lambda) is guaranteed to converge to an optimal policy, the objective function used by natural actor-critics is concave, so policy gradient methods are guaranteed to converge to globally optimal policies as well.},
  langid = {english},
  file = {/home/unist/Zotero/storage/EESYFWFH/Thomas - 2014 - Bias in Natural Actor-Critic Algorithms.pdf}
}

@article{thrunIssuesUsingFunctiona,
  title = {Issues in {{Using Function Approximation}} for {{Reinforcement Learning}}},
  author = {Thrun, Sebastian and Schwartz, Anton},
  langid = {english},
  file = {/home/unist/Zotero/storage/QW6I4AFY/Thrun and Schwartz - Issues in Using Function Approximation for Reinforcement Learning.pdf}
}

@misc{tiboniDomainRandomizationEntropy2024,
  title = {Domain {{Randomization}} via {{Entropy Maximization}}},
  author = {Tiboni, Gabriele and Klink, Pascal and Peters, Jan and Tommasi, Tatiana and D'Eramo, Carlo and Chalvatzaki, Georgia},
  year = {2024},
  month = mar,
  number = {arXiv:2311.01885},
  eprint = {2311.01885},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2311.01885},
  url = {http://arxiv.org/abs/2311.01885},
  urldate = {2025-08-25},
  abstract = {Varying dynamics parameters in simulation is a popular Domain Randomization (DR) approach for overcoming the reality gap in Reinforcement Learning (RL). Nevertheless, DR heavily hinges on the choice of the sampling distribution of the dynamics parameters, since high variability is crucial to regularize the agent's behavior but notoriously leads to overly conservative policies when randomizing excessively. In this paper, we propose a novel approach to address sim-to-real transfer, which automatically shapes dynamics distributions during training in simulation without requiring real-world data. We introduce DOmain RAndomization via Entropy MaximizatiON (DORAEMON), a constrained optimization problem that directly maximizes the entropy of the training distribution while retaining generalization capabilities. In achieving this, DORAEMON gradually increases the diversity of sampled dynamics parameters as long as the probability of success of the current policy is sufficiently high. We empirically validate the consistent benefits of DORAEMON in obtaining highly adaptive and generalizable policies, i.e. solving the task at hand across the widest range of dynamics parameters, as opposed to representative baselines from the DR literature. Notably, we also demonstrate the Sim2Real applicability of DORAEMON through its successful zero-shot transfer in a robotic manipulation setup under unknown real-world parameters.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics,Domain Randomization,Sim-to-Real Transfer},
  file = {/home/unist/Zotero/storage/8ERGATGU/Tiboni et al. - 2024 - Domain Randomization via Entropy Maximization.pdf;/home/unist/Zotero/storage/34MELC5X/2311.html}
}

@misc{tiboniOnlineVsOffline2022,
  title = {Online vs. {{Offline Adaptive Domain Randomization Benchmark}}},
  author = {Tiboni, Gabriele and Arndt, Karol and Averta, Giuseppe and Kyrki, Ville and Tommasi, Tatiana},
  year = {2022},
  month = jun,
  number = {arXiv:2206.14661},
  eprint = {2206.14661},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2206.14661},
  url = {http://arxiv.org/abs/2206.14661},
  urldate = {2025-08-25},
  abstract = {Physics simulators have shown great promise for conveniently learning reinforcement learning policies in safe, unconstrained environments. However, transferring the acquired knowledge to the real world can be challenging due to the reality gap. To this end, several methods have been recently proposed to automatically tune simulator parameters with posterior distributions given real data, for use with domain randomization at training time. These approaches have been shown to work for various robotic tasks under different settings and assumptions. Nevertheless, existing literature lacks a thorough comparison of existing adaptive domain randomization methods with respect to transfer performance and real-data efficiency. In this work, we present an open benchmark for both offline and online methods (SimOpt, BayRn, DROID, DROPO), to shed light on which are most suitable for each setting and task at hand. We found that online methods are limited by the quality of the currently learned policy for the next iteration, while offline methods may sometimes fail when replaying trajectories in simulation with open-loop commands. The code used will be released at https://github.com/gabrieletiboni/adr-benchmark.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/home/unist/Zotero/storage/FZ8EAF4S/Tiboni et al. - 2022 - Online vs. Offline Adaptive Domain Randomization Benchmark.pdf;/home/unist/Zotero/storage/T8DYEJA7/2206.html}
}

@article{uhlenbeckTheoryBrownianMotion1930a,
  title = {On the {{Theory}} of the {{Brownian Motion}}},
  author = {Uhlenbeck, G. E. and Ornstein, L. S.},
  year = {1930},
  month = sep,
  journal = {Physical Review},
  volume = {36},
  number = {5},
  pages = {823--841},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRev.36.823},
  url = {https://link.aps.org/doi/10.1103/PhysRev.36.823},
  urldate = {2025-04-16},
  abstract = {With a method first indicated by Ornstein the mean values of all the powers of the velocity {$u$} and the displacement {$s$} of a free particle in Brownian motion are calculated. It is shown that {$u$} -{$u$}0⁢exp⁡(-{$B$}⁢{$t$}) and {$s$} -{$u$}0{$B$}⁢[1-exp⁡(-{$B$}⁢{$t$})] where {$u$}0 is the initial velocity and {$B$} the friction coefficient divided by the mass of the particle, follow the normal Gaussian distribution law. For {$s$} this gives the exact frequency distribution corresponding to the exact formula for {$s$}2 of Ornstein and F{\"u}rth. Discussion is given of the connection with the Fokker-Planck partial differential equation. By the same method exact expressions are obtained for the square of the deviation of a harmonically bound particle in Brownian motion as a function of the time and the initial deviation. Here the periodic, aperiodic and overdamped cases have to be treated separately. In the last case, when {$B$} is much larger than the frequency and for values of {$t\ggB-$}1, the formula takes the form of that previously given by Smoluchowski.},
  file = {/home/unist/Zotero/storage/6YGS84D6/Uhlenbeck and Ornstein - 1930 - On the Theory of the Brownian Motion.pdf;/home/unist/Zotero/storage/4AMX54TC/PhysRev.36.html}
}

@inproceedings{vanseijenTheoreticalEmpiricalAnalysis2009a,
  title = {A Theoretical and Empirical Analysis of {{Expected Sarsa}}},
  shorttitle = {Expected {{Sarsa}}},
  booktitle = {2009 {{IEEE Symposium}} on {{Adaptive Dynamic Programming}} and {{Reinforcement Learning}}},
  author = {{van Seijen}, Harm and {van Hasselt}, Hado and Whiteson, Shimon and Wiering, Marco},
  year = {2009},
  month = mar,
  pages = {177--184},
  issn = {2325-1867},
  doi = {10.1109/ADPRL.2009.4927542},
  url = {https://ieeexplore.ieee.org/document/4927542},
  urldate = {2025-05-07},
  abstract = {This paper presents a theoretical and empirical analysis of Expected Sarsa, a variation on Sarsa, the classic on-policy temporal-difference method for model-free reinforcement learning. Expected Sarsa exploits knowledge about stochasticity in the behavior policy to perform updates with lower variance. Doing so allows for higher learning rates and thus faster learning. In deterministic environments, Expected Sarsas updates have zero variance, enabling a learning rate of 1. We prove that Expected Sarsa converges under the same conditions as Sarsa and formulate specific hypotheses about when Expected Sarsa will outperform Sarsa and Q-learning. Experiments in multiple domains confirm these hypotheses and demonstrate that Expected Sarsa has significant advantages over these more commonly used methods.},
  keywords = {Artificial intelligence,Convergence,Dynamic programming,Intelligent systems,Optimal control,Probability distribution,Robot control,State estimation,State feedback,Supervised learning},
  file = {/home/unist/Zotero/storage/TTNCIZXE/van Seijen et al. - 2009 - A theoretical and empirical analysis of Expected Sarsa.pdf}
}

@misc{vaswaniAttentionAllYou2023a,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  year = {2023},
  month = aug,
  number = {arXiv:1706.03762},
  eprint = {1706.03762},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1706.03762},
  url = {http://arxiv.org/abs/1706.03762},
  urldate = {2025-05-26},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/unist/Zotero/storage/Y67EJXKG/Vaswani et al. - 2023 - Attention Is All You Need.pdf;/home/unist/Zotero/storage/XYENDFF2/1706.html}
}

@misc{wagenmakerSteeringYourDiffusion2025a,
  title = {Steering {{Your Diffusion Policy}} with {{Latent Space Reinforcement Learning}}},
  author = {Wagenmaker, Andrew and Nakamoto, Mitsuhiko and Zhang, Yunchu and Park, Seohong and Yagoub, Waleed and Nagabandi, Anusha and Gupta, Abhishek and Levine, Sergey},
  year = {2025},
  month = jun,
  number = {arXiv:2506.15799},
  eprint = {2506.15799},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2506.15799},
  url = {http://arxiv.org/abs/2506.15799},
  urldate = {2025-08-20},
  abstract = {Robotic control policies learned from human demonstrations have achieved impressive results in many real-world applications. However, in scenarios where initial performance is not satisfactory, as is often the case in novel open-world settings, such behavioral cloning (BC)-learned policies typically require collecting additional human demonstrations to further improve their behavior -- an expensive and time-consuming process. In contrast, reinforcement learning (RL) holds the promise of enabling autonomous online policy improvement, but often falls short of achieving this due to the large number of samples it typically requires. In this work we take steps towards enabling fast autonomous adaptation of BC-trained policies via efficient real-world RL. Focusing in particular on diffusion policies -- a state-of-the-art BC methodology -- we propose diffusion steering via reinforcement learning (DSRL): adapting the BC policy by running RL over its latent-noise space. We show that DSRL is highly sample efficient, requires only black-box access to the BC policy, and enables effective real-world autonomous policy improvement. Furthermore, DSRL avoids many of the challenges associated with finetuning diffusion policies, obviating the need to modify the weights of the base policy at all. We demonstrate DSRL on simulated benchmarks, real-world robotic tasks, and for adapting pretrained generalist policies, illustrating its sample efficiency and effective performance at real-world policy improvement.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/home/unist/Zotero/storage/W47ULMJM/Wagenmaker et al. - 2025 - Steering Your Diffusion Policy with Latent Space Reinforcement Learning.pdf;/home/unist/Zotero/storage/K5H4BK2J/2506.html}
}

@misc{wangDuelingNetworkArchitectures2016a,
  title = {Dueling {{Network Architectures}} for {{Deep Reinforcement Learning}}},
  author = {Wang, Ziyu and Schaul, Tom and Hessel, Matteo and van Hasselt, Hado and Lanctot, Marc and de Freitas, Nando},
  year = {2016},
  month = apr,
  number = {arXiv:1511.06581},
  eprint = {1511.06581},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1511.06581},
  url = {http://arxiv.org/abs/1511.06581},
  urldate = {2025-03-26},
  abstract = {In recent years there have been many successes of using deep representations in reinforcement learning. Still, many of these applications use conventional architectures, such as convolutional networks, LSTMs, or auto-encoders. In this paper, we present a new neural network architecture for model-free reinforcement learning. Our dueling network represents two separate estimators: one for the state value function and one for the state-dependent action advantage function. The main benefit of this factoring is to generalize learning across actions without imposing any change to the underlying reinforcement learning algorithm. Our results show that this architecture leads to better policy evaluation in the presence of many similar-valued actions. Moreover, the dueling architecture enables our RL agent to outperform the state-of-the-art on the Atari 2600 domain.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/unist/Zotero/storage/9SVVGM58/Wang et al. - 2016 - Dueling Network Architectures for Deep Reinforcement Learning.pdf;/home/unist/Zotero/storage/C5XZF8B3/1511.html}
}

@article{williamsSimpleStatisticalGradientfollowing1992a,
  title = {Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning},
  author = {Williams, Ronald J.},
  year = {1992},
  month = may,
  journal = {Machine Learning},
  volume = {8},
  number = {3},
  pages = {229--256},
  issn = {1573-0565},
  doi = {10.1007/BF00992696},
  url = {https://doi.org/10.1007/BF00992696},
  urldate = {2025-04-09},
  abstract = {This article presents a general class of associative reinforcement learning algorithms for connectionist networks containing stochastic units. These algorithms, called REINFORCE algorithms, are shown to make weight adjustments in a direction that lies along the gradient of expected reinforcement in both immediate-reinforcement tasks and certain limited forms of delayed-reinforcement tasks, and they do this without explicitly computing gradient estimates or even storing information from which such estimates could be computed. Specific examples of such algorithms are presented, some of which bear a close relationship to certain existing algorithms while others are novel but potentially interesting in their own right. Also given are results that show how such algorithms can be naturally integrated with backpropagation. We close with a brief discussion of a number of additional issues surrounding the use of such algorithms, including what is known about their limiting behaviors as well as further considerations that might be used to help develop similar but potentially more powerful reinforcement learning algorithms.},
  langid = {english},
  keywords = {Artificial Intelligence,connectionist networks,gradient descent,mathematical analysis,Reinforcement learning},
  file = {/home/unist/Zotero/storage/URPKSTJT/Williams - 1992 - Simple statistical gradient-following algorithms for connectionist reinforcement learning.pdf}
}

@misc{xuePolicyRegularizationGlobally2025a,
  title = {Policy {{Regularization}} on {{Globally Accessible States}} in {{Cross-Dynamics Reinforcement Learning}}},
  author = {Xue, Zhenghai and Feng, Lang and Xu, Jiacheng and Kang, Kang and Wen, Xiang and An, Bo and Yan, Shuicheng},
  year = {2025},
  month = mar,
  number = {arXiv:2503.06893},
  eprint = {2503.06893},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2503.06893},
  url = {http://arxiv.org/abs/2503.06893},
  urldate = {2025-08-20},
  abstract = {To learn from data collected in diverse dynamics, Imitation from Observation (IfO) methods leverage expert state trajectories based on the premise that recovering expert state distributions in other dynamics facilitates policy learning in the current one. However, Imitation Learning inherently imposes a performance upper bound of learned policies. Additionally, as the environment dynamics change, certain expert states may become inaccessible, rendering their distributions less valuable for imitation. To address this, we propose a novel framework that integrates reward maximization with IfO, employing F-distance regularized policy optimization. This framework enforces constraints on globally accessible states--those with nonzero visitation frequency across all considered dynamics--mitigating the challenge posed by inaccessible states. By instantiating F-distance in different ways, we derive two theoretical analysis and develop a practical algorithm called Accessible State Oriented Policy Regularization (ASOR). ASOR serves as a general add-on module that can be incorporated into various RL approaches, including offline RL and off-policy RL. Extensive experiments across multiple benchmarks demonstrate ASOR's effectiveness in enhancing state-of-the-art cross-domain policy transfer algorithms, significantly improving their performance.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/unist/Zotero/storage/4VKSRCNZ/Xue et al. - 2025 - Policy Regularization on Globally Accessible States in Cross-Dynamics Reinforcement Learning.pdf;/home/unist/Zotero/storage/ASRAR6NM/2503.html}
}

@misc{yeIterativeLabelRefinement2025a,
  title = {Iterative {{Label Refinement Matters More}} than {{Preference Optimization}} under {{Weak Supervision}}},
  author = {Ye, Yaowen and Laidlaw, Cassidy and Steinhardt, Jacob},
  year = {2025},
  month = jan,
  number = {arXiv:2501.07886},
  eprint = {2501.07886},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2501.07886},
  url = {http://arxiv.org/abs/2501.07886},
  urldate = {2025-08-18},
  abstract = {Language model (LM) post-training relies on two stages of human supervision: task demonstrations for supervised finetuning (SFT), followed by preference comparisons for reinforcement learning from human feedback (RLHF). As LMs become more capable, the tasks they are given become harder to supervise. Will post-training remain effective under unreliable supervision? To test this, we simulate unreliable demonstrations and comparison feedback using small LMs and time-constrained humans. We find that in the presence of unreliable supervision, SFT still retains some effectiveness, but DPO (a common RLHF algorithm) fails to improve the model beyond SFT. To address this, we propose iterative label refinement (ILR) as an alternative to RLHF. ILR improves the SFT data by using comparison feedback to decide whether human demonstrations should be replaced by model-generated alternatives, then retrains the model via SFT on the updated data. SFT+ILR outperforms SFT+DPO on several tasks with unreliable supervision (math, coding, and safe instruction-following). Our findings suggest that as LMs are used for complex tasks where human supervision is unreliable, RLHF may no longer be the best use of human comparison feedback; instead, it is better to direct feedback towards improving the training data rather than continually training the model. Our code and data are available at https://github.com/helloelwin/iterative-label-refinement.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/unist/Zotero/storage/QKBLXQCS/Ye et al. - 2025 - Iterative Label Refinement Matters More than Preference Optimization under Weak Supervision.pdf;/home/unist/Zotero/storage/X42WMUAI/2501.html}
}

@misc{yoonMonteCarloTree2025,
  title = {Monte {{Carlo Tree Diffusion}} for {{System}} 2 {{Planning}}},
  author = {Yoon, Jaesik and Cho, Hyeonseo and Baek, Doojin and Bengio, Yoshua and Ahn, Sungjin},
  year = {2025},
  month = jul,
  number = {arXiv:2502.07202},
  eprint = {2502.07202},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2502.07202},
  url = {http://arxiv.org/abs/2502.07202},
  urldate = {2025-08-25},
  abstract = {Diffusion models have recently emerged as a powerful tool for planning. However, unlike Monte Carlo Tree Search (MCTS)-whose performance naturally improves with inference-time computation scaling-standard diffusion-based planners offer only limited avenues for the scalability. In this paper, we introduce Monte Carlo Tree Diffusion (MCTD), a novel framework that integrates the generative strength of diffusion models with the adaptive search capabilities of MCTS. Our method reconceptualizes denoising as a tree-structured process, allowing partially denoised plans to be iteratively evaluated, pruned, and refined. By selectively expanding promising trajectories while retaining the flexibility to revisit and improve suboptimal branches, MCTD achieves the benefits of MCTS such as controlling exploration-exploitation trade-offs within the diffusion framework. Empirical results on challenging long-horizon tasks show that MCTD outperforms diffusion baselines, yielding higher-quality solutions as inference-time computation increases.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/unist/Zotero/storage/W3TKHC4C/Yoon et al. - 2025 - Monte Carlo Tree Diffusion for System 2 Planning.pdf;/home/unist/Zotero/storage/BU3YGHP5/2502.html}
}

@article{zengGraphDiffusionRobusta,
  title = {Graph {{Diffusion}} for {{Robust Multi-Agent Coordination}}},
  author = {Zeng, Xianghua and Su, Hang and Wang, Zhengyi and Lin, Zhiyuan},
  abstract = {Offline multi-agent reinforcement learning (MARL) struggles to estimate out-of-distribution states or actions due to the absence of real-time interactions with the environment. Although diffusion models have shown promising potential in addressing these challenges, they primarily apply independent diffusion to the historical trajectories of individual agents, which overlooks the crucial dynamics in multi-agent coordination and limits the policy robustness in dynamic environments. In this paper, we propose MCGD, a novel Multi-agent Coordination framework based on Graph Diffusion models to improve the effectiveness and robustness of collaborative policies. Specifically, we construct a sparse coordination graph with continuous node attributes and discrete edge attributes to identify the underlying multi-agent dynamics effectively. We then derive the transition probabilities between edge categories and present adaptive categorical diffusion to model the structure diversity of inter-agent coordination. According to the coordination structure, we define the neighbor-dependent forward noise and design anisotropic diffusion to increase the action diversity of each agent. Extensive experiments across various multi-agent environments demonstrate that MCGD significantly outperforms existing state-of-the-art baselines in coordination performance and exhibits superior robustness to dynamic environmental changes.},
  langid = {english},
  file = {/home/unist/Zotero/storage/N9DEI9PT/Zeng et al. - Graph Diffusion for Robust Multi-Agent Coordination.pdf}
}

@misc{zhengCanMISLFly2025,
  title = {Can a {{MISL Fly}}? {{Analysis}} and {{Ingredients}} for {{Mutual Information Skill Learning}}},
  shorttitle = {Can a {{MISL Fly}}?},
  author = {Zheng, Chongyi and Tuyls, Jens and Peng, Joanne and Eysenbach, Benjamin},
  year = {2025},
  month = mar,
  number = {arXiv:2412.08021},
  eprint = {2412.08021},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2412.08021},
  url = {http://arxiv.org/abs/2412.08021},
  urldate = {2025-08-21},
  abstract = {Self-supervised learning has the potential of lifting several of the key challenges in reinforcement learning today, such as exploration, representation learning, and reward design. Recent work (METRA) has effectively argued that moving away from mutual information and instead optimizing a certain Wasserstein distance is important for good performance. In this paper, we argue that the benefits seen in that paper can largely be explained within the existing framework of mutual information skill learning (MISL). Our analysis suggests a new MISL method (contrastive successor features) that retains the excellent performance of METRA with fewer moving parts, and highlights connections between skill learning, contrastive representation learning, and successor features. Finally, through careful ablation studies, we provide further insight into some of the key ingredients for both our method and METRA.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/unist/Zotero/storage/75A2Z3L3/Zheng et al. - 2025 - Can a MISL Fly Analysis and Ingredients for Mutual Information Skill Learning.pdf;/home/unist/Zotero/storage/ZPABCAAJ/2412.html}
}

@misc{zhengIntentionConditionedFlowOccupancy2025a,
  title = {Intention-{{Conditioned Flow Occupancy Models}}},
  author = {Zheng, Chongyi and Park, Seohong and Levine, Sergey and Eysenbach, Benjamin},
  year = {2025},
  month = jun,
  number = {arXiv:2506.08902},
  eprint = {2506.08902},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2506.08902},
  url = {http://arxiv.org/abs/2506.08902},
  urldate = {2025-08-21},
  abstract = {Large-scale pre-training has fundamentally changed how machine learning research is done today: large foundation models are trained once, and then can be used by anyone in the community (including those without data or compute resources to train a model from scratch) to adapt and fine-tune to specific tasks. Applying this same framework to reinforcement learning (RL) is appealing because it offers compelling avenues for addressing core challenges in RL, including sample efficiency and robustness. However, there remains a fundamental challenge to pre-train large models in the context of RL: actions have long-term dependencies, so training a foundation model that reasons across time is important. Recent advances in generative AI have provided new tools for modeling highly complex distributions. In this paper, we build a probabilistic model to predict which states an agent will visit in the temporally distant future (i.e., an occupancy measure) using flow matching. As large datasets are often constructed by many distinct users performing distinct tasks, we include in our model a latent variable capturing the user intention. This intention increases the expressivity of our model, and enables adaptation with generalized policy improvement. We call our proposed method intention-conditioned flow occupancy models (InFOM). Comparing with alternative methods for pre-training, our experiments on \$36\$ state-based and \$4\$ image-based benchmark tasks demonstrate that the proposed method achieves \$1.8 {\textbackslash}times\$ median improvement in returns and increases success rates by \$36{\textbackslash}\%\$. Website: https://chongyi-zheng.github.io/infom Code: https://github.com/chongyi-zheng/infom},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/unist/Zotero/storage/T2HGI2R6/Zheng et al. - 2025 - Intention-Conditioned Flow Occupancy Models.pdf;/home/unist/Zotero/storage/V8CQRGZS/2506.html}
}

@misc{zhouUnifiedTheoreticalAnalysis2025a,
  title = {A {{Unified Theoretical Analysis}} of {{Private}} and {{Robust Offline Alignment}}: From {{RLHF}} to {{DPO}}},
  shorttitle = {A {{Unified Theoretical Analysis}} of {{Private}} and {{Robust Offline Alignment}}},
  author = {Zhou, Xingyu and Wu, Yulian and Orabona, Francesco},
  year = {2025},
  month = may,
  number = {arXiv:2505.15694},
  eprint = {2505.15694},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2505.15694},
  url = {http://arxiv.org/abs/2505.15694},
  urldate = {2025-08-20},
  abstract = {In this paper, we theoretically investigate the effects of noisy labels in offline alignment, with a focus on the interplay between privacy and robustness against adversarial corruption. Specifically, under linear modeling assumptions, we present a unified analysis covering both reinforcement learning from human feedback (RLHF) and direct preference optimization (DPO) under different privacy-corruption scenarios, such as Local differential privacy-then-Corruption (LTC), where human preference labels are privatized before being corrupted by an adversary, and Corruption-then-Local differential privacy (CTL), where labels are corrupted before privacy protection. Our analysis leverages a reduction framework that reduces the offline alignment problem under linear modeling assumptions to parameter estimation in logistic regression. This framework allows us to establish an interesting separation result between LTC and CTL, demonstrating that LTC presents a greater challenge than CTL in offline alignment, even under linear models. As important by-products, our findings also advance the state-of-the-art theoretical results in offline alignment under privacy-only or corruption-only scenarios.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/unist/Zotero/storage/AUAYNSUK/Zhou et al. - 2025 - A Unified Theoretical Analysis of Private and Robust Offline Alignment from RLHF to DPO.pdf;/home/unist/Zotero/storage/FTGC77U6/2505.html}
}
