\documentclass[8pt]{beamer}
\usefonttheme[onlymath]{serif}


\setbeamertemplate{frametitle}{%
  \vskip1ex
  \usebeamerfont{frametitle}%
  \insertsubsectionhead\par        %  ← 원하는 대로 변경 가능
  \vskip1ex
  \hrule                             % 밑줄(선택)
}

% 테마 선택 (선택 사항)
% \usetheme{Madrid} % 기본 테마, 다른 테마 사용 가능
% \font{serif}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage[T1]{fontenc} % To use combination of textbf, textit
\usepackage[dvipsnames]{xcolor}   % can use more variant colors

% \setcounter{MaxMatrixCols}{20}

% (필요한 패키지들)
% \usepackage{amsthm}
\setbeamertemplate{theorems}[numbered]  % 정리, 정의 등에 번호를 달아줌

% \theoremstyle{plain} % insert bellow all blocks you want in italic
% \newtheorem{theorem}{Theorem}[section] % to number according to section
% 
% \theoremstyle{definition} % insert bellow all blocks you want in normal text
% \newtheorem{definition}{Definition}[section] % to number according to section
% \newtheorem*{idea}{Proof idea} % no numbered block

\newtheorem{proposition}[theorem]{Proposition}

\usepackage{tcolorbox}

% 필요할 경우 패키지 추가
\usepackage{graphicx} % 이미지 삽입을 위한 패키지
\usepackage{amsmath}   % 수식 사용
\usepackage{hyperref}  % 하이퍼링크 추가
\usepackage{cleveref}
\usepackage{multicol}  % 여러 열 나누기
\usepackage{ulem} % 취소선 및줄 나누기
\usepackage{mathtools} % dcases
%\usepackage{xparse} % NewDocumentCommand



% \NewDocumentCommand{\DefThreeOp}{m}{%
%   % \csname #1\endcsname 라는 이름으로, 3개 인자를 받는 새 매크로를 정의
%   \expandafter\NewDocumentCommand\csname #1\endcsname{mmm}{%
%     \operatorname{#1}\!\bigl(##1,\,##2,\,##3\bigr)%
%   }%
% }

\newcommand{\mrm}[1]{\mathrm{#1}}
\newcommand{\mbb}[1]{\mathbb{#1}}
\newcommand{\mb}[1]{\mathbf{#1}}
\newcommand{\mc}[1]{\mathcal{#1}}
\newcommand{\tb}[1]{\textbf{#1}}
\newcommand{\ti}[1]{\textit{#1}}
\newcommand{\Pois}[1]{\operatorname{Pois}(#1)}

\newcommand{\myber}[1]{\operatorname{Bern}\!\left(#1\right)}
\newcommand{\Bin}[2]{\operatorname{Bin}\!\left(#1,#2\right)}
\newcommand{\NBin}[2]{\operatorname{NBin}\!\left(#1,#2\right)}
\newcommand{\mytoinf}[1]{#1 \rightarrow \infty}
\newcommand{\myexp}[1]{\exp{\left(#1\right)}}
\newcommand{\Unif}[2]{\operatorname{Unif}\!\left(#1, #2\right)}
\newcommand{\mygeom}[1]{\operatorname{Geom}\!\left(#1\right)}
\newcommand{\Expo}[1]{\operatorname{Expo}\!\left(#1\right)}
\newcommand{\abs}[1]{\left\lvert #1 \right\rvert}
\newcommand{\expec}[1]{\operatorname{E}\left[ #1 \right]}
\newcommand{\Var}[1]{\operatorname{Var}\left[#1\right]}
\newcommand{\myskew}[1]{\operatorname{Skew}\!\left[#1\right]}
\newcommand{\mykurt}[1]{\operatorname{Kurt}\!\left[#1\right]}
\newcommand{\mywei}[2]{\operatorname{Wei}\!\left(#1, #2\right)}
\newcommand{\Span}{\operatorname{Span}}
\newcommand{\Cov}[2]{\operatorname{Cov}\!\left(#1, #2\right)}
\newcommand{\intinfty}{\int_{-\infty}^\infty}
\newcommand{\Corr}[2]{\operatorname{Corr}\!\left(#1, #2\right)}
\newcommand{\Mult}[3]{\operatorname{Mult}_{#1}\!\left(#2, #3\right)}
\newcommand{\Beta}[2]{\operatorname{Beta}\!\left(#1, #2\right)}
\newcommand{\HGeom}[3]{\operatorname{HGeom}\!\left(#1, #2, #3\right)}
\newcommand{\NHGeom}[3]{\operatorname{NHGeom}\!\left(#1,#2, #3\right)}
\newcommand{\GammaDist}[2]{\operatorname{Gamma}\!\left(#1, #2\right)}
%\DefThreeOp{PHGeom}

\newcommand{\im}{\operatorname{im}}
\newcommand{\tr}{\operatorname{tr}}


% 발표 제목, 저자, 날짜 설정
\title{Linear algebra: Linear Transformations}
\author{Gwanwoo Choi}
% \date{}

\begin{document}
% 표지 슬라이드

\begin{frame}
    \titlepage
\end{frame}

\subsection{Linear Transformations}
\begingroup
    \setbeamertemplate{frametitle}{%
    \vskip1ex
    \usebeamerfont{frametitle}%
    \insertframetitle\par        %  ← 원하는 대로 변경 가능
    \vskip1ex
    \hrule                             % 밑줄(선택)
    }
    \begin{frame}
        \frametitle{Table of Contents}
        \tableofcontents[currentsubsection]
    \end{frame}
\endgroup

% % 목차 슬라이드

\begin{frame}{.}
    \begin{definition}[Linear Transformation]
        Let $V$ and $W$ be vector spaces over the field $F$. A \tb{linear transformation from $V$ to $W$} is a funtion $T$ from $V$ into $W$ such that
        \[
            T(c\alpha + \beta) = c (T\alpha) + T\beta
        \]
        for all $\alpha$ and $\beta$ in $V$ and all scalars $c$ in $F$.
    \end{definition}

    \begin{example}
        If $V$ is any vector space, the \tb{identity transformation} $I$, defined by $I\alpha = \alpha$, is a linear transformation from $V$ into $V$.
        The \tb{zero transformation} 0, defined by $0 \alpha = 0$, is a linear transformation from $V$ into $V$.
    \end{example}

    \begin{example}
        Let $F$ be a field and let $V$ be the space of polynomial functions $f$ from $F$ into $F$, given by
        \[
            f(x) = c_0 + c_1 x + \cdots + c_k x^k
        \]
        Let 
        \[
            (Df) (x) = c_1 + 2 c_2 x + \cdots + k c_k x^{k-1}
        \]
        Then $D$ is a linear transformation from $V$ into $V$ the differentiation transformation.
    \end{example}
\end{frame}

\begin{frame}{.}
    \begin{example}
        Let $A$ be a fixed $m \times n$ matrix with entries in the field $F$. The function $T$ defined by $T(X) = AX$ is a linear transformation from $F^
        {n\times 1}$ into $F^{m \times 1}$. The function $U$ defined by $U(\alpha) = \alpha A$ is a linear transformation from $F^m$ into $F^n$
    \end{example}

    \begin{example}
        Let $P$ be a fixed $m \times m$ matrix with entries in the field $F$ and let $Q$ be a fixed $n \times n$ matrix over $F$. Define a function $T$ from the space $F^{m \times n}$ into itself by $T(A) = PAQ$. Then $T$ is a linear transformation from $F^{m \times n}$ into $F^{m \times n}$, because
        \[
            T(cA + B) = P(cA + B)Q = cPAQ + PBQ = cT(A) + T(B)
        \]
    \end{example}

    \begin{example}
        Let $V$ be the space of all functions from $\mbb{R}$ to $\mbb{R}$ which are continuous. Define $T$ by 
        \[
            (Tf) (x) = \int_0^x f(t) dt
        \]
        Then $T$ is a linear transformation from $V$ to $V$.
        The function $Tf$ is not only continuous but has a continuous first derivative.
        The linearity of integration is one of its fundamental properties.
    \end{example}
\end{frame}

\begin{frame}{.}
    Let $V$ and $W$ be the vector space and $T$ is a linear transformation from $V$ into $W$.
    Then $T(0) = 0$.
    \[
        T(0) = T(0 + 0) = T(0) + T(0) \implies T(0) = 0
    \]
    \smallskip

    Also, linear transformation $T$ preserves linear combination,
    \[
        T(c_1\alpha_1 + \cdots + c_n \alpha_n) = c_1 T(\alpha_1) + \cdots + c_n T(\alpha_n)
    \]
\end{frame}

\begin{frame}{.}
    \begin{theorem}\label{th:1}
        Let $V$ be a finite-dimensional vector space over the field $F$ and let $\{\alpha_1, \dots, \alpha_n\}$ be an ordered basis for $V$.
        Let $W$ be a vector space over the same field $F$ and let $\beta_1, \dots, \beta_n$ be any vectors in $W$. Then there is precisely one linear transformation $T$ from $V$ into $W$ such that
        \[
            T(\alpha_j) = \beta_j, j=1, \dots, n
        \]
    \end{theorem}
    \begin{proof}
        For any vector $a \in V$, We can express $a = x_1 \alpha_1 + \cdots x_n \alpha_n$ for some scalars $x_i$.
        Let function $T$ from $V$ into $W$ s.t. $T(a) = T(x_1 \alpha_1 + \cdots + x_n \alpha_n) = x_1 \beta_1 + \cdots + x_n \beta_n$.
        Note that by assumption, $T(\alpha_j) = \beta_j$.
        Clames are 1. $T$ is linear transformation, 2. $T$ is unique.

        \begin{enumerate}
            \item For any $a_1, a_2 \in V$, let $a_1 = x_1 \alpha_1 + \cdots + x_n \alpha_n, a_2 = y_1 \alpha_1 + \cdots + x_n \alpha_n$. 
            Then for any scalar $c \in F, ca_1 + a_2 = (cx_1 + y_1)\alpha_1 + \cdots (cx_n + y_n) \alpha_n$ and $T(ca_1 + a_2) = (cx_1 + y_1)\beta_1 + \cdots + (cx_n + y_n)\beta_n$ by assumption.
            But also, $cT(a_1) = cx_1 \alpha_1 + \cdots + cx_n \alpha_n$ and $T(a_2) = y_1 \alpha_1 + \cdots + y_n \alpha_n$ thus $cT(a_1) + T(a_2) = (cx_1 + y_1) \alpha_1 + (cx_n + y_n) \alpha_n$.
            $\therefore T(ca_1 + a_2) = cT(a_1) + T(a_2)$ and this implies $T$ is linear transformation.
            \item Suppose there exists another linear transformation $U$ ($U(\alpha_j) = \beta_j$) , for which $\exists a \in V$ s.t. $U(a) \neq T(a)$. $a$ can be expressed by $a = x_1 \alpha_1 + \cdots x_n \alpha_n$ for some scalars $x_1, \dots, x_n$. Then $U(a) = x_1 \beta_1 + \cdots x_n \beta_n$ and $T(a) = x_1 \beta_1 + \cdots x_n \beta_n$. So $T(a) = U(a)$ and the assumption is wrong. So, $T$ is unique.
        \end{enumerate}
    \end{proof}
\end{frame}

\begin{frame}{.}
    \begin{example}
        The vectors $\alpha_1 = (1,2), \alpha_2=(3,4)$ are linearly independent and therefore form a basis for $\mbb{R}^2$. According to Theorem \ref{th:1}, there is a unique linear transformation from $\mbb{R}^2$ into $\mbb{R}^3$ such that $T \alpha_1 = (3,2,1), T \alpha_2 = (6,5,4)$.

        For standard basis $\epsilon_1 = (1,0)$, there exists some scalars $c_1 (1,2) + c_2 (3,4) = (1,0) \implies c_1=-2, c_2=1$.
        Thus $T(1,0) = -2 (3,2,1) + (6,5,4) = (0,1,2)$
    \end{example}

    \begin{example}
        Let $T$ be a linear transformation from the $F^m$ into the $F^n$. Theorem \ref{th:1} tells us that $T$ is uniquely determined by the sequence of vectors $\beta_1, \dots, \beta_m$ where $\beta_i = T \epsilon_i, i=1, \dots, m$. For any $\alpha =(x_1, \dots, x_m)\in F^m$, $T \alpha = x_1 \beta_1 + \cdots + x_m \beta_m$.

        If $B$ is the $m \times n$ matrix which has row vectors $\beta_1, \dots, \beta_m$, this says that 
        \[
            \begin{gathered}
                T \alpha = \alpha B \\
                T (x_1, \dots, x_m) = \left[\begin{matrix} x_1 & \cdots & x_m \end{matrix}\right] \left[\begin{matrix}
                    B_{11} & \cdots & B_{1n} \\
                    \vdots & \empty & \vdots \\
                    B_{m1} & \cdots & B_{mn}
                \end{matrix}\right]
            \end{gathered}
        \]
    \end{example}
\end{frame}

\begin{frame}{.}
    For any linear transformation $T$, we can consider about $\ker{(T)} := \{\alpha | T(\alpha) = 0\}$.

    This $\ker (T)$ is subspace of $V$. Let $T$ be the linear transformation from $V$ into $W$.
    \begin{itemize}
        \item For $T(\alpha_1) = 0, T(\alpha_2) =0, \alpha_1, \alpha_2 \in V$, $T(\alpha_1 + \alpha_2) = T(\alpha_1) + T(\alpha_2) = 0 \in \ker (T)$
        \item $T(c\alpha_1) = cT(\alpha_1)  = 0 \in \ker (T)$
        \item $T(0) = 0$. So $ 0 \in \ker (T)$. Note that $\{0\} \subset \ker (T)$.
    \end{itemize}

    This \tb{Kernel space} is also called by \tb{Null space}.
    \tb{Nullity} means the dimension of kernel space $\dim \ker (T)$.

    \bigskip

    If $V$ is finite-dimensional, the \tb{image}  $\im (T)$ (or also called \tb{range}) is defined by $\im (T) = \{ f(\alpha) | \alpha \in V\}$.
    This image $\im(T)$ is subspace of $W$.
    \begin{itemize}
        \item $T(0) =0$, so $0 \in \im (T)$
        \item Let $T(\alpha_1) = \beta_1$ and $T(\alpha_2) = \beta_2$. By the property of linear transformation, $T(c\alpha_1 + \alpha_2) = cT(\alpha_1) + T(\alpha_2) = c\beta_1 + \beta_2$. This $c\beta_1 +\beta_2 \in W$ because $W$ is vector space.
    \end{itemize}

    \tb{Rank} means the dimension of image space $\dim \im(T)$. 
    
\end{frame}

\begin{frame}{.}
    \begin{theorem}\label{th:2}
        Let $V$ and $W$ be vector space over the field $F$ and let $T$ be a linear transformation from $V$ into $W$. 
        Suppose that $V$ is finite-dimensional. 
        Then
        \[
            \dim \im(T) + \dim \ker(T) = \dim V 
        \]
    \end{theorem}

    \begin{proof}
    Let $\{\alpha_1 , \dots, \alpha_k\}$ be the basis of $\ker (T)$, i.e. $T(\alpha_i) = 0, \forall i=1, \dots, k$. We can extend this to $\{\alpha_1, \dots, \alpha_n\}$, the basis of $V$. Then $\dim \ker (T) =k$ and $\dim V =n$.

    \smallskip
    Now for any vectory $\alpha \in V$, there exists some $x_1, \dots, x_n \in F$ s.t. $\alpha = x_1 \alpha_1 + \cdots +  x_n \alpha_n$.
    Let $T(\alpha) = 0 $ then $T(\alpha) = x_{k+1} T(\alpha_{k+1}) + \cdots x_{n} T(\alpha_n) = 0 \implies T(x_{k+1} \alpha_{k+1} + \cdots + x_n \alpha_n) = 0$. But $\alpha_i \notin \ker (T), \forall i=k+1, \dots n$ so $x_{k+1} = \cdots = x_n = 0$. This implies $\{T(\alpha_{k+1}), \dots, T(\alpha_{n})\}$ is lin. ind.

    \smallskip
    Also, for any $\beta \in \Span \{ T(\alpha_{k+1}), \dots, T(\alpha_n)\}$, $\beta \in \im(T)$. Thus, $\{T(\alpha_{k+1}), \dots, T(\alpha_n)\}$ consists basis for $\im(T)$. So $\dim \im(T) = n -k$.

    $\therefore \dim \im(T) + \dim \ker (T) = n-k + k = n = \dim V$.
    \end{proof}
    
\end{frame}

\begin{frame}{.}
    \begin{theorem}
        If $A$ is an $m \times n$ matrix with entries in the field $F$, then
        \[
            row\ rank(A) = column\ rank(A)
        \]
    \end{theorem}
    \begin{proof}
        Let define linear transform $T(X):= AX$, where $T$ is the transformation from $F^{n \times 1}$ to $F^{m \times 1}$.
        By the Theorem \ref{th:2}, $\dim \ker(T) + \dim \im(T) = \dim F^{n \times 1}$ holds.

        \smallskip
        $\im(T) = \{T(X)| X \in F^{n\times 1}\}$. $T(X) = AX = x_1 A_{:, 1} + \cdots + x_n A_{:, n}$, and this implies $\im(T) = column\ rank(A)$. So $\dim \ker(T) + column\ rank (T) = n $ holds.

        \smallskip
        $\ker(T) = \{X|AX = 0 \}$ and dimension of solution space $AX =0$ is known as $n- row\ rank(A)$. Thus, $\ker(T) = n - row\ rank(A)$.

        \smallskip
        So, $n - row\ rank(A) + column\ rank(A) =n$ holds and this implies $row\ rank(A) = column\ rank(A)$.

    \end{proof}
\end{frame}

\begin{frame}{.}
    \begin{block}{Exercise 3.1.13}
        Let $V$ be a vector space and $T$ a linear transformation from $V$ into $V$. Prove that the following two statements about $T$ are equivalent.
        \begin{enumerate}
            \item The intersection of the $\ker(T)$ and $\im(T)$ is zero subspace of $V$
            \item if $T(T(\alpha)) = 0$, then $T(\alpha) = 0$
        \end{enumerate}
    \end{block}
    \begin{itemize}
        \item ($\implies$) Assume 1. holds. if $T(T(\alpha)) = 0$, then $T(\alpha) \in \ker (T)$. Also, $T(\alpha) \in \im(T)$. Thus $T(\alpha) \in (\ker(T) \cap \im(T))$ and by the assumption, $T(\alpha) = 0$.
        \item ($\impliedby$) Assume 2. holds. Let $\alpha \in (\ker(T) \cap \im(T))$. Then $\exists \alpha^\prime \in \im(T)$ s.t. $T(\alpha^\prime) = \alpha$. Also, $T(\alpha) = 0$ holds. Thus, $T(T(\alpha^\prime)) =0$ holds. By the assumption, $T(\alpha^\prime) = \alpha = 0$.
        This implies $\{0\} = (\ker(T) \cap \im(T))$.
    \end{itemize}
\end{frame}

\subsection{The Algebra of Linear Transformation}

\begingroup
    \setbeamertemplate{frametitle}{%
    \vskip1ex
    \usebeamerfont{frametitle}%
    \insertframetitle\par        %  ← 원하는 대로 변경 가능
    \vskip1ex
    \hrule                             % 밑줄(선택)
    }
    \begin{frame}
        \frametitle{Table of Contents}
        \tableofcontents[currentsubsection]
    \end{frame}
\endgroup

\begin{frame}{.}
    Let $V$ and $W$ be vector spaces over the field $F$. Let $T$ and $U$ be linear transformations from $V$ into $W$. Let $c$ is any element of $F$.

    \begin{itemize}
        \item The function $(T+U)$ is defined by $(T+U)(\alpha):= T(\alpha) + U(\alpha)$
        \item The function $(cT)$ is defined by $(cT)(\alpha):= cT(\alpha)$
    \end{itemize}

    \begin{theorem}
        The set of all linear transformations from $V$ into $W$, together with the addition and scalar multiplication defined above, is a vector space over the field $F$.
    \end{theorem}

    We shall denote the space of linear transformations from $V$ into $W$ by $L(V,W)$. Note that $L(V,W)$ only can be defined when $V$ and $W$ are vector sapces over the same field.
\end{frame}

\begin{frame}{.}
    \begin{theorem} \label{th:5}
        Let $V$ be an $n$-dimensional vector space over the field $F$, and $W$ be an $m$-dimensional vector space over $F$. Then the space $L(V,W)$ is finite-dimensional and has dimension $mn$.
    \end{theorem}

    \ti{Proof.} Let $\mc{B} = \{\alpha_1, \cdots, \alpha_n\}$ and $\mc{B}^\prime = \{\beta_1, \cdots, \beta_n\}$ be ordered basis for $V$ and $W$, respectively.

    For each pair of integers $(p,q), \forall 1 \leq p \leq m, 1 \leq q \leq n$, we can define linear transformation $T_{p,q}$ s.t. maps $q$-th element of $\mc{B}$, $\alpha_q$, to  $p$-th element of $\mc{B}^\prime$, $\beta_p$. i.e., $T_{p,q}(\alpha_i) = \delta_{iq} \beta_p$ (where $\delta$ is kroneker-delta). By Theorem \ref{th:1}, each $T_{p,q}$ is unique transformation.

    Clam: This $\mc{T} = \{T_{p,q}| 1\leq p \leq m, q \leq q \leq n\}$ forms the basis of vector space $L(V,W)$.
    \begin{itemize}
        \item Span $L(V,W)$: For any transformation $K \in L(V,W)$, $\forall 1\leq p \leq m, 1 \leq q \leq n, \exists A_{p,q}$ s.t. $K(\alpha_j) = \sum_{p=1}^m A_{pj} \beta_p$. $\implies K(\alpha_j) = \sum_{p=1}^m A_{pj} T_{p,j}(\alpha_j) \implies K(\alpha_j) = \sum_{p=1}^m A_{pj} \sum_{q=1}^n T_{p,q}(\alpha_q) = \sum_{p=1}^m \sum_{q=1}^n A_{p,q} T_{p,q} (\alpha_q) = \sum_{p=1}^m \sum_{q=1}^n A_{p,q} \delta_{p,q}\alpha_q$. This implies $K = \sum_{p=1}^m \sum_{q=1}^n A_{p,q} T_{p,q}$, $K \in \Span \mc{T}$.
        \item $\mc{T}$ is lin. ind.: For scalars $A_{p,q}, 1\leq p \leq m, 1 \leq q \leq n $, let assume transformation $K \in L(V,W), K = \sum_p \sum_q A_{p,q} T_{p,q}$ satisfies $K = 0$, zero transformation. Then for any $1\leq j \leq n$, $K(\alpha_j) = \sum_{p}\sum_q A_{p,q} T_{p,q} (\alpha_j) = \sum_p A_{p,j} T_{p,j} (\alpha_j) = \sum_p A_{p,j} \beta_p= 0$. But $\sum_p A_{p,j} \beta_p \in \Span \mc{B}^\prime$ and $\mc{B}^\prime$ is basis, so $A_{p,j}=0$ and this implies lin. ind. $\qedsymbol$
    \end{itemize}

\end{frame}

\begin{frame}{.}
    \begin{theorem}
        Let $V$, $W$, and $Z$ be vector spaces over the field $F$. Let $T$ be a linear transformation from $V$ into $W$ and $U$ a linear transformation from $W$ into $Z$. Then the composed function $UT$ defined by $(UT)(\alpha) := U(T(\alpha))$ is a linear transformation from $V$ into $Z$.

        \ti{Proof.}
        \[
            (UT) (c\alpha + \beta) = U [T (c \alpha + \beta)] = U[cT(\alpha) + T(\beta)] = c(UT)(\alpha) + (UT)(\beta)
        \] 
    \end{theorem}

    \begin{definition}
        If $V$ is a vector space over the $F$, a \tb{linear operator on} $V$ is a linear transformation from $V$ into $V$.
    \end{definition}

    Let $V$ be a vector space over the field $F$; let $U$, $T_1$ and $T_2$ be linear operators on $V$; let $c$ be an element of $F$
    \begin{enumerate}
        \item $U = UI = U$
        \item $U(T_1+T_2) = UT_1 +UT_2$
        \item $(T_1 + T_2)U = T_1U + T_2U$
        \item $c(UT_1) = (cU)T_1 = U(cT_1)$
    \end{enumerate}
\end{frame}

\begin{frame}{.}

    \begin{definition}
        The function $T$ from $V$ into $W$ is called \tb{invertible} if there exists a function $U$ from $W$ into $V$ s.t. $UT = I$, identity functionon $V$ and $TU = I$, identity function on $W$. If $T$ is invertible, the function $U$ is unique and is denoted by $T^{-1}$.
    \end{definition}

    The concepts of injective(one-to-one), surjective(onto), and bijective(one-to-one correspondence) are
    \begin{itemize}
        \item $T$ is one-to-one (or called injective), that is, $T\alpha = T\beta \implies \alpha =\beta$.
        \item $T$ is onto (or called surjective), that is, the range of $T$ is (all of $W$) (surjective). More concretely, $\forall w \in W, \exists v \in V$ s.t. $T(v) = w$.
        \item If $T$ is both injective and surjective, then $T$ is bijective.
    \end{itemize}

    $T$ is invertible if and only if $T$ is bijective.

    \begin{theorem}{.}
        Let $V$ and $W$ be vector spaces over the field $F$ and let $T$ be a linear transformation from $V$ into $W$. If $T$ is invertible, then the inverse function $T^{-1}$ is a linear transformation from $W$ onto $V$.
    \end{theorem}
    \ti{Proof.} Let $T(\alpha_1) = \beta_1, T(\alpha_2) = \beta_2$. Then $\alpha_1 = T^{-1}(\beta_1) \implies c\alpha_1 = c T^{-1}(\beta_1)$, $\alpha_2 = T^{-1}(\beta_2)$. And from $T(c\alpha_1 + \alpha_2) = c\beta_1 + \beta_2$, $c\alpha_1 + \alpha_2 = c T^{-1}(\beta_1) + T^{-1}(\beta_2)$ holds. $\therefore T^{-1}(c\beta_1 + \beta_2) = cT^{-1}(\beta_1) + T^{-1}(\beta_2)$.

\end{frame}

\begin{frame}{.}
    \begin{definition}
        Linear transformation $T$ is \tb{non-singular} if $T\gamma = 0 \implies \gamma = 0$. i.e., $\ker(T) = \{0\}$.

        \begin{theorem} \label{th:3}
            Let $T$ be a linear transformation from $V$ into $W$. Then $T$ is non-singular if and only if $T$ carries each linearly independent subset of $V$ onto a lienarly independent subset of $W$.
        \end{theorem}
        \ti{Proof.}
        \begin{itemize}
            \item $(\implies)$: Let $\{\alpha_1, \dots, \alpha_s\}$ be lin. ind. set.
            For any $\alpha \in V$, $\exists c_1, \dots, c_s \in F$ s.t. $\alpha = c_1 \alpha_1 + \cdots + c_n \alpha_s$.
            Claim is $\{T(\alpha_1), \dots, T(\alpha_s)\}$ is lin. ind.
            Let $T(\alpha) = T(c_1\alpha_1 + \cdots + c_s \alpha_s) = c_1 T(\alpha_1) + \cdots + c_s T(\alpha_s)= 0$.
            Then by the assumption, $T(c_1\alpha_1 + \cdots + c_s \alpha_s) = 0$ implies $c_1\alpha_1 + \cdots + c_s \alpha_s = 0$.
            By lin. lid. of $\{\alpha_1, \dots, \alpha_s\}$, $c_1 = \cdots = c_s = 0$.
            $\therefore \{T(\alpha_1),\dots, T(\alpha_s)\}$ is lin. ind.
            \item $(\impliedby)$: Before proof, note that $\{0\}$ is not lin. ind. since $c \cdot 0 = 0$ does not implies $c = 0$.
            There exists $c \neq 0$ s.t. $c \cdot 0 = 0$. For example, $1 \cdot 0 = 0$.
            For non-zero $\alpha \in V$, $\{\alpha\}$ is lin. ind.
            By the assumption, $\{T(\alpha)\}$ is lin. ind. Because $\{T(\alpha)\}$ is lin. ind. $T(\alpha) \neq 0$. This implies $\alpha \neq 0 \implies T(\alpha) \neq 0$ and $T(\alpha) = 0 \implies \alpha = 0$. $\therefore T$ is non-singluar (i.e. $\ker(T) = \{0\}$).
        \end{itemize}
    \end{definition}
\end{frame}

\begin{frame}{.}
    \begin{theorem}\label{th:4}
        Let $V$ and $W$ be finite-dimensional vector space over the field $F$ such that $\dim V = \dim W$. If $T$ is a linear transformation from $V$ into $W$, the following are equivalent
        \begin{enumerate}
            \item $T$ is invertible
            \item $T$ is non-singular ($\ker(T) = \{0\}$)
            \item $T$ is surjective (onto) ($\im(T) = W$)
        \end{enumerate}

        \ti{Proof.}
        \begin{itemize}
            \item (2. $\iff$ 3): Note that $\dim \ker (T) + \dim \im (T) = \dim V = \dim W$. $\im(T) \subseteq W$ and $\dim \im(T) = \dim W \implies \im(T) = W$ holds.

            So $\im(T) = W \iff \dim \im(T) = \dim W \iff \dim \ker (T) = 0 \iff \ker (T) = \{0\}$. So, for any $T \in L(V,W)$, $T$ is non-singular $\iff$ $T$ is surjective.
            \item (1. $\implies$ 2.): Assume 1. holds. Then $\exists! \alpha \in V$ s.t. $T(\alpha) = 0$. Because $T$ is linear transformation, $T(0) = 0$. Thus $\ker(T) =\{0\}$
            \item (1. $\impliedby$ 2.): Assume 2. holds. Let assume $\alpha_1, \alpha_2 \in V$ satisfy $T(\alpha_1) = T(\alpha_2)$. Then $T(\alpha_1 - \alpha_2) =0$ and since $\ker(T) = \{0\}$, $\alpha_1 = \alpha_2$. Thus $T$ is injective $\impliedby$ 2. From the fact $T$ is surjective $\impliedby$ 2., 1. $\impliedby$ 2.
        \end{itemize}
        Therefore, 1. $\iff$ 2. $\iff$ 3  $\qed$

        Notice the meaning of non-singuarity for $T$. If $T$ is non-singular, then for the basis of $V$, $\mc{B} = \{\alpha_1, \dots, \alpha_n\}$, $T(\mc{B}) = \{T(\alpha_1), \dots, T(\alpha_n)\}$ consists basis for $W$.
    \end{theorem}
\end{frame}

\begin{frame}{.}
    The set of invertible linear operator on a space $V$, with the operation of composition, provides a nice example of what is known in algebra as a \tb{group}.

    \begin{definition}
        A \tb{group} consists of the following
        \begin{itemize}
            \item A set $G$
            \item A rule (or operation) which associates with each pair of elements $x, y$ in $G$ an element $xy$ in $G$ in such a way that
            \begin{itemize}
                \item $x(yz) = (xy)z, \forall x,y,z \in G$
                \item There is an element $e$ in $G$ s.t. $ex=xe=x$, for every $x$ in $G$
                \item to each element $x$ in $G$ there corresponds an element $x^{-1}$ in $G$ s.t. $x x^{-1} = x^{-1}x = e$
            \end{itemize}
        \end{itemize}

        A group is called \tb{commutative} if it satisfies the condition $xy = yx$ for each $x$ and $y$.
    \end{definition}

    A field can be described as a set with two operations, called addition and multiplication, which is a commutative group under addition and in which the non-zero elements form a commutative group under multiplication, with the distributive law $x(y+z) = xy + xz$ holding.
\end{frame}

\begin{frame}{.}
    \begin{block}{Exercise 3.2.6}
        Let $T$ be a linear transformation from $\mbb{R}^3$ into $\mbb{R}^2$, and let $U$ be a linear transformation from $\mbb{R}^2$ into $\mbb{R}^3$. Prove that the transformation $UT$ is not invertible. Generalize the theorem.
    \end{block}

    Let assume $\{\epsilon_1, \epsilon_2, \epsilon_3\}$ be the basis of $\mbb{R}^3$. $\{T(\epsilon_1), T(\epsilon_2), T(\epsilon_3)\}$ is not lin. ind. because $\dim \mbb{R}^2 = 2$. 
    Let assume $T(\epsilon_3) = x_1 T(\epsilon_1) + x_2 T(\epsilon_2)$. Then $(UT)(\epsilon_3) = x_1 (UT)(\epsilon_1) + x_2 (UT)(\epsilon_2) = (UT)(x_1 \epsilon_1 + x_2 \epsilon_2)$.

    $\epsilon_3 \neq x_1 \epsilon_1 + x_2 \epsilon_2, (UT)(\epsilon_3) = (UT)(x_1  \epsilon_1 + x_2 \epsilon_2) \implies UT$ is not invertible. 

    \begin{block}{Exercise 3.2.7}
        Find two linear operators $T$ and $U$ on $\mbb{R}^2$ such that $TU =0$ but $UT \neq 0$.
    \end{block}
    Let $U(1,0) := (0, 0), U(0,1) := (1, 0)$ and $T(1,0) := (0, 0), T(0, 1) := (0, 1)$. Then $(TU)(1, 0) = (0,0),(TU)(0, 1) = (0, 0) \implies TU = 0$ and $(UT)(1,0) = (0, 0), (UT) (0,1) = (1,0) \implies UT \neq 0$.
\end{frame}

\begin{frame}{.}
    \begin{block}{Exercise 3.2.10}
        Let $A$ be an $m \times n$ matrix with entries in $F$ and let $T$ be the linear transformation from $F^{n \times 1}$ into $F^{m \times 1}$ defined by $T(X) = AX$.
        Show that if $m < n$ it may happen that $T$ is onto without being non-singular.
        Similarly, show that if $m > n$ we may have $T$ non-singular but not onto.
    \end{block}
    Let $\mc{B} = \{\epsilon_1, \dots, \epsilon_n\}$.
    Then $T(\mc{B}) = \{T(\epsilon_1), \dots, T(\epsilon_n)\}$.
    If $m <n$, then $T(\mc{B})$ is not lin. ind. Let assume $\{T(\epsilon_1), \dots, T(\epsilon_m)\}$ is lin. ind. 
    This implies $\Span\{T(\epsilon_1), \dots, T(\epsilon_m)\} = F^{m \times 1}$ so $T$ is onto.
    But $T(\mc{B})$ is not lin. ind. and this implies there exists some non-zero components $x_i$ s.t. $x_1T(\epsilon_1) + \cdots + x_n T(\epsilon_n) =T(x_1 \epsilon_1 + \cdots + x_n\epsilon_n)= 0$ and this implies some element in $F^{n \times 1}$, $x_1 \epsilon_1 + \cdots + x_n \epsilon_n \in \ker T$ and $T$ becomes singular.

    If $m >n$, let assume each $\mc{B} = \{\epsilon_1, \dots, \epsilon_n\}$ and $\mc{B}^\prime = \{\epsilon_1^\prime, \dots, \epsilon_m^\prime\}$ becomes basis of $F^{n \times 1}$ and $F^{m \times 1}$ and $T$ is defined by $T(\epsilon_1) = \epsilon_1^\prime, \dots, T(\epsilon_n) = \epsilon_n^\prime$.
    Then by Theorem \ref{th:3}, $T$ becomes non-singular but there exists $\epsilon_{n+1}^\prime, \dots, \epsilon_m \notin \im (T)$, which implies $T$ is not onto.
\end{frame}

\begin{frame}{.}
    \begin{block}{$\star$ Exerciese 3.2.11}
        Let $V$ be a finite-dimensional vector space and let $T$ be a linear operator on $V$.
        Suppose that $\dim \im (T^2) = \dim \im (T)$.
        Prove that the range and null space of $T$ are disjoint, i.e., have only the zero vector in common.
    \end{block}

    By the rank-nullity theorem, $\dim V = \dim \ker (T^2) + \dim \im (T^2)$ and $\dim V = \dim \ker (T) + \im (T)$ holds.
    $\implies \dim V  -  \dim \ker (T^2) = \dim V - \dim \ker (T) \implies \dim \ker(T^2) = \dim \ker (T)$.

    % Let $\{\alpha_1, \dots, \alpha_n\}$ be basis for $V$. 
    % For $k < n$, Let $\Span{\alpha_1, \dots, \alpha_k} = \ker (T)$.
    % Then $\Span{T(\alpha_{k+1}), \dots, T(\alpha_n)} = \im(T)$.
    % Because $\dim \ker(T^2) = \dim \ker(T)$, $T(\alpha_{k+1}), \dots T(\alpha_n) \in \Span{\alpha_{k+1}, \dots, \alpha_n} \implies \Span{T(\alpha_{k+1}), \dots, T(\alpha_n)} = \im(T) = \Span{\alpha_{k+1}, \dots, \alpha_n}$.
    % Then $\Span{T(T(\alpha_{k+1})), \dots, T(T(\alpha_n))} = \Span{\alpha_{k+1}, \dots, \alpha_{n}}$ also holds.

    % $\therefore \ker(T) = \Span{\alpha_1, \dots, \alpha_k} \cap \Span{\alpha_{k+1}, \dots, \alpha_n}= \im(T) = \{0\}$.

    Let $\dim \ker (T) = k$ and $\ker (T) = \Span \{\alpha_1, \dots, \alpha_k\}$ and $\{\alpha_1, \dots, \alpha_k\}$ is lin. ind.
    For lin. ind. set $\{\alpha_{k+1}, \dots, \alpha_n\}$, $T(\alpha_{k+1}), \dots, T(\alpha_n) \in \im(T)$.

    Let assume $\exists \alpha \in \Span \{\alpha_{k+1}, \dots, \alpha_n\}$ s.t. $T(T(\alpha)) = 0$ holds.
    Then $T(\alpha) \in \ker (T)$ and $\alpha \in \ker (T^2)$. 
    But $\alpha \notin \ker(T)$. 
    So $\dim \ker(T^2) > \dim \ker(T)$.

    But by the condition $\dim \im (T^2) = \dim \im (T)$, $\dim \ker (T^2) = \dim \ker (T)$ should be hold.
    So our assumption $\exists \alpha \in \Span \{\alpha_{k+1}, \dots \alpha_n\}$ s.t. $T(T(\alpha)) = 0$ is wrong.

    This implies $\forall \alpha \in \Span \{\alpha_{k+1}, \dots, \alpha_n\}, T(\alpha) \notin \ker(T)$.

    In summarize, for any $\alpha \in \Span \{\alpha_1, \dots, \alpha_k\}, T(T(\alpha)) = 0$ and for any $\beta \in \Span \{\alpha_{k+1}, \dots, \alpha_n\}$, $T(T(\beta)) \neq 0$. 
    $\therefore \ker(T) \cap \im(T) = \{0\}$.

\end{frame}

\subsection{Isomorphism}
\begingroup
    \setbeamertemplate{frametitle}{%
    \vskip1ex
    \usebeamerfont{frametitle}%
    \insertframetitle\par        %  ← 원하는 대로 변경 가능
    \vskip1ex
    \hrule                             % 밑줄(선택)
    }
    \begin{frame}
        \frametitle{Table of Contents}
        \tableofcontents[currentsubsection]
    \end{frame}
\endgroup

\begin{frame}{.}
    \begin{definition}
        If $V$ and $W$ are vector spaces over the field $F$, any one-one linear transformation $T$ of $V$ onto $W$ is called \tb{isomorphism of $V$ onto $W$}.
        If there exists an isomorphism of $V$ onto $W$, we say that $V$ is \tb{isomorphic} to $W$.
    \end{definition}
    \begin{itemize}
        \item $V$ is trivially isomorphic to itself, $V$, with the identity operator begin an isomorhpism of $V$ onto $V$.
        \item If $V$ is isomorphic to $W$ via an isomorphism $T$, then $W$ is isomorphic to $V$, because $T^{-1}$ is an isomorphism of $W$ onto $V$.
        \item If $V$ is isomorphic to $W$ and $W$ is isomorphic to $Z$, then $V$ is isomorphic to $Z$.
    \end{itemize}
    \begin{theorem}
        Every $n$-dimensional vector space over the field $F$ is isomorphic to the space $F^n$.
    \end{theorem}

    Let $\mc{B} = \{\alpha_1, \dots, \alpha_n\}$ the basis of $V$.
    We can consider linear map $T$ that maps $\alpha =x_1 \alpha_1 + \cdots + x_n \alpha_n $ to $(x_1, \dots, x_n)$.
    Then this $T$ is one-to-one (invertible) map and $V$ and $F^n$ are isomorphic.
    $\qedsymbol$
\end{frame}

\begin{frame}{.}
    \begin{block}{Exercise 3.3.6}
        Let $V$ and $W$ be finite-dimensional vector spaces over the field $F$. Prove that $V$ and $W$ are isomorphic if and only if $\dim V = \dim W$
    \end{block}
    \begin{itemize}
        \item ($\implies$): Let $T$ is isomorphism from $V$ onto $W$.
        Then $T$ is invertible.
        Let $\mc{B} = \{\alpha_1, \dots, \alpha_n\}$ be the basis of $V$.
        Claim is $\{T(\alpha_1), \dots, T(\alpha_n)\}$ basis of $W$.
        For any $\beta \in W$, $\exists! \alpha \in V$ s.t. $T(\alpha) = \beta$.
        There exists $x_1, \dots, x_n$ s.t. $\alpha = x_1 \alpha_1 + \cdots + x_n \alpha_n \implies x_1 T(\alpha_1) + \cdots + x_n T(\alpha_n) = \beta \implies \beta \in \Span \{T(\alpha_1) ,\dots, T(\alpha_n)\}$.
        Also, $\{T(\alpha_1), \dots, T(\alpha_n)\}$ is lin. ind. Let $x_1 T(\alpha_1) + \cdots + x_n T(\alpha_n) = 0$. Then $T(x_1 \alpha_1 + \cdots + x_n \alpha_n) = 0 $ and since $\ker (T) = \{0\}$, $x_1 \alpha_1 + \cdots + x_n \alpha_n$.
        Since $\{\alpha_1, \dots, \alpha_n\}$ is lin. ind., $x_1 = \cdots = x_n = 0$.
        So, $\{T(\alpha_1), \dots, T(\alpha_n)\}$ is lin. ind.
        $\therefore \{T(\alpha_1), \dots, T(\alpha_n)\}$ is basis of $W$ and $\dim W = \dim V = n$.
        \item ($\impliedby$): Let $\dim W = \dim V = n$ and each $\{\alpha_1, \dots, \alpha_n\}$ and $\{\beta_1, \dots, \beta_n\}$ becomes basis of $V$ and $W$. Let $T$ be the linear map from $V$ onto $W$ s.t. $T(\alpha_i) = \beta_i$. Let $\alpha = x_1 \alpha_1 + \cdots + x_n \alpha_n \in V$ satisfies $T(\alpha) = 0$.
        Then $x_1 \beta_1 + \cdots + x_n \beta_n = 0$. Then $x_1 = \cdots = x_n =0 $. So $T(\alpha) = 0 \implies \alpha = 0$ and $\ker (T) = 0 \implies T$ is invertible.
        $\therefore T$ is isomorpism from $V$ onto $W$.
    \end{itemize}
\end{frame}

\begin{frame}{.}
    \begin{block}{Exercise 3,3.7}
        Let $V$ and $W$ be vector spaces over the field $F$ and let $U$ be an isomorphism of $V$ onto $W$.
        Prove that $T \to UT U^{-1}$ is an isomorphism of $L(V,V)$ onto $L(W,W)$.
    \end{block}

    Note that $L(V,V)$ and $L(W,W)$ are vector spaces and $T \in L(V,V), U T U^{-1} \in L(W,W)$.
    Let $\{\alpha_1, \dots, \alpha_n\}$ be the basis of $V$.
    For any $T \in L(V,V)$, we can consider basis of $L(V,V)$ as $\{T_{11}, \dots, T_{1n}, \dots, T_{mn}\}$ where $T_{pq}(\alpha_j) = \delta_{qj} \alpha_p$.

    For any $\alpha \in V$, there exists scalars $x_i$ such that $\alpha = x_1 \alpha_1 + \cdots + x_n \alpha_n$. Then $T(\alpha) = $

    \textcolor{red}{To be done..}
\end{frame}

\subsection{Representation of Transformations by Matrices}

\begingroup
    \setbeamertemplate{frametitle}{%
    \vskip1ex
    \usebeamerfont{frametitle}%
    \insertframetitle\par        %  ← 원하는 대로 변경 가능
    \vskip1ex
    \hrule                             % 밑줄(선택)
    }
    \begin{frame}
        \frametitle{Table of Contents}
        \tableofcontents[currentsubsection]
    \end{frame}
\endgroup

\begin{frame}{.}
    Let $V$ be an $n$-dimensional vector space over the field $F$ and let $W$ be an $m$-dimensional vector space over $F$.
    Let $\mc{B} = \{\alpha_1, \dots, \alpha_n\}$ be an ordered basis for $V$ and $\mc{B}^\prime = \{\beta_1, \dots, \beta_m\}$ an ordered basis for $W$.
    If $T$ is any linear transformation from $V$ into $W$, then $T$ is determined by its action on the vectors $\alpha_j$.
    Each of the $n$ vectors $T(\alpha_j)$ is uniquely expressible as a linear combination
    \[
        T(\alpha_j) = \sum_{i=1}^m A_{i,j} \beta_i
    \]
    of the $\beta_i$, the scalars $A_{1,j}, \dots, A_{m,j}$, being the coordinates of $T(\alpha_j)$ in the ordered basis $\mc{B}^\prime$.

    Accordingly, the transformation $T$ is determined by the $mn$ scalars $A_{i,j}$ via the upper equation.
    The $m \times n$ matrix $A$ defined by $A(i,j) = A_{i,j}$ is called 
    \begin{itemize}
        \item \tb{the matrix of $T$ relative to the pair of ordered bases $\mc{B}$ and $\mc{B}^\prime$}.
        \item \tb{A matrix associated with linear map $T$}
    \end{itemize}
\end{frame}

\begin{frame}{.}
    For convenience of denoting, let introduce some symbols.
    \begin{definition}
        For linear map $T \in L(V,W)$ and basis $\mc{B}, \mc{B}^\prime$, each of which is basis of $V$ and $W$, let define $[T]^{\mc{B}}_{\mc{B}^\prime}$ as
        \[
            [T]^{\mc{B}}_{\mc{B}^\prime} := \left[\begin{matrix}
                A_{1,1} & \cdots & A_{1,n} \\ \vdots & \empty & \vdots \\ A_{m,1} & \cdots & A_{m,n}
            \end{matrix}\right]
        \]
        s.t. $ T(\alpha_j) = \left[\begin{matrix} \beta_1 & \cdots & \beta_m \end{matrix}\right] \left[\begin{matrix}
            A_{1,j} \\ \vdots \\ A_{m,j}
        \end{matrix}\right]$.

        Let $\Psi : F^{m \times n} \to L(V,W)$ the mapping from matrix space $F^{m \times n}$ to linear mapping space. 
        Then $\Psi(A) = T$.
        Let $\Phi : L(V,W) \to F^{m \times n}$ the mapping from linear mapping space to matrix space.
        Then $\Phi(T) = A$.

    \end{definition}
    \begin{example}
        Let $A \in F^{m \times n}$.
        Let $T \in L(F^{n \times 1}, F^{m \times 1})$ defined by $T(X) = AX$.
        If we set $\mc{B} = \{\epsilon_1, \dots, \epsilon_n\}, \mc{B}^\prime = \{\epsilon^\prime_1, \dots, \epsilon^\prime_m\}$ where $\epsilon_j$ is the $j$-th standard basis of $F^{n \times 1}$ and $\epsilon^\prime_i$ is the $i$-th standard basis of $F^{m \times 1}$.
        Find $[T]^{\mc{B}}_{\mc{B}^\prime}$.
    \end{example}
    
    $T(\epsilon_j) = A_{:,j} = \sum_{i=1}^m A_{i,j} \epsilon^\prime_{i} \implies [T]^{\mc{B}}_{\mc{B}^\prime} = A$.
\end{frame}

\begin{frame}{.}
    \begin{theorem}
        Let $V$ be an $n$-dimensional vector space over the field $F$ and $W$ an $m$-dimensional vector space over $F$.
        Let $\mc{B}$ be an ordered basis for $V$ and $\mc{B}^\prime$ be an ordered basis for $W$. For each linear transformation $T$ from $V$ into $W$, there is an $m\times n$ matrix $A$ with entries in $F$ such that
        \[
            [T(\alpha)]_{\mc{B}^\prime} =[T]^{\mc{B}}_{\mc{B}^\prime}[\alpha]_{\mc{B}} = A [\alpha]_{\mc{B}}
        \]
        for every vector $\alpha$ in $V$.
        Furthermore, $T\to A$ is a one-one correspondence between the set of all linear transformations from $V$ into $W$ and the set of all $m \times n$ matrices over the field $F$.
    \end{theorem}

    \ti{Proof.} Let $\mc{B} = \{\alpha_1, \dots, \alpha_n\}$ and $\mc{B}^\prime = \{\beta_1, \dots, \beta_m\}$ be the basis of $V$ and $W$.
    \[
        T(\alpha) = T\left(\sum_{j=1}^n x_j \alpha_j \right) = \sum_{j=1}^n x_j T(\alpha_j) = \sum_{j=1}^n x_j \sum_{i=1}^m A_{ij} \beta_i = \sum_{i=1}^m \left(\sum_{j=1}^n A_{ij}x_j\right)\beta_i
    \]
    \qed
\end{frame}

\begin{frame}{.}
    Proof in matrix viewpoint.
    \[
    \begin{aligned}
        T\left( \left[\begin{matrix} \alpha_1 & \cdots & \alpha_n \end{matrix}\right] \left[\begin{matrix} x_1 \\ \vdots \\ x_n \end{matrix} \right]\right) &= \left[\begin{matrix} T(\alpha_1) & \cdots & T(\alpha_n) \end{matrix}\right] \left[\begin{matrix} x_1 \\ \vdots \\ x_n \end{matrix}\right] \\
        &= \left[\begin{matrix} \beta_1 & \cdots & \beta_m \end{matrix}\right] \left[\begin{matrix} A_{11} & \cdots & A_{1n} \\ \vdots & \empty & \vdots \\ A_{m1} & \cdots & A_{mn}\end{matrix}\right] \left[\begin{matrix} x_1 \\ \vdots \\ x_n\end{matrix}\right]
    \end{aligned}
    \]
    The matrix $A$ which is associated with $T$ is called the \tb{matrix of $T$ relative to the ordered bases $\mc{B}, \mc{B}^\prime$}.
    Note that it says that $A$ is the matrix whose columns $A_1, \dots, A_n$ are given by
    \[
        A_j = [T(\alpha_j)]_{\mc{B}^\prime}
    \]
\end{frame}


\begin{frame}{.}
    Note that this matrix that associated with linear map is linear.
    Let $T, U \in L(V,W)$.
    For $j =1,\dots, n$, there uniquely exists column matrices $A, B \in F^{m \times n}$ such that $T(\alpha_j) = \left[\begin{matrix} \beta_1 & \cdots & \beta_m \end{matrix}\right] \left[\begin{matrix} A_{1,j} \\ \vdots \\ A_{m,j} \end{matrix}\right]$, $U(\alpha_j) = \left[\begin{matrix} \beta_1 & \cdots & \beta_m \end{matrix}\right] \left[\begin{matrix} B_{1,j} \\ \vdots \\ B_{m,j} \end{matrix}\right]$. Note that $cA + B$ is the matrix of $cT + U$ relative to $\mc{B}, \mc{B}^\prime$ because $\left[\begin{matrix}
    \beta_1 & \cdots & \beta_m
    \end{matrix}\right] \left(c \left[\begin{matrix}
        A_{1,j} \\ \vdots \\ A_{m,j}
    \end{matrix}\right] + \left[\begin{matrix}
        B_{1,j} \\ \vdots \\ B_{m,j}
    \end{matrix}\right]\right) = (cT + U)(\alpha_j)$.
\end{frame}

\begin{frame}{.}
    \begin{theorem}
        Let $V$ be an $n$-dimensional vector space over the field $F$ and let $W$ be an $m$-dimensional vector space over $F$.
        For each pair of ordered bases $\mc{B}$, $\mc{B}^\prime$ for $V$ and $W$ respectively,
        Let define $\Phi$ as $\Phi : L(V,W) \to F^{m \times n}$.
        Then the map $\Phi$ is isomorphism between $L(V,W)$ and $F^{m \times n}$.
    \end{theorem}

    \ti{Proof.}
    \begin{itemize}
        \item (Linearity): Let $T, U \in L(V,W)$ and $A = [T]^{\mc{B}}_{\mc{B}^\prime}, B = [T]^{\mc{B}}_{\mc{B}^\prime}$.
        For $j$th element of basis $\mc{B}, \alpha_j$, $T(\alpha_j) = \left[\begin{matrix}
            \beta_1 & \cdots & \beta_m
        \end{matrix}\right] \left[\begin{matrix}
            A_{1,j} \\ \vdots \\ A_{m,j}
        \end{matrix}\right] = \mc{B}^\prime A_{:,j}$, $U(\alpha_j) = \mc{B}^\prime B_{:,j}$.
        $(cT + U)(\alpha_j) = cT(\alpha_j) + U(\alpha_j) = \mc{B}^\prime (cA_{:,j} + B_{:,j})$.
        This implies $\Phi(cT + U) = (cA + B) = cA + B = c\Phi(T) + \Phi(U)$ and $\Phi$ is linear map.
        \item (Invertible [one-to-one correspondence]): By Theorem \ref{th:1}, zero mapping $0$ has unique associated zero matrix $0$. i.e., $\exists! 0 \in L(V,W)$ s.t. $\Phi(0) = 0$. So $\ker \Phi = \{0\}$.
        By Theorem \ref{th:5}, $\dim L(V,W) = \dim F^{m \times n} = mn$.
        By theorem \ref{th:4}, $\Phi$ is invertible.
    \end{itemize}
    \qed
\end{frame}

\begin{frame}{.}
    Let focus on matrix of linear operator.

    \begin{definition}
        Let $\mc{B}$ be the basis of $V$. And $T \in L(V,V)$.
        Then it is most convenient to use the same ordered basis in each case, that is, to take $\mc{B} = \mc{B}^\prime$.
        We shall then call the representing matrix simply the \tb{matrix of $T$ relative to the ordered basis $\mc{B} = \{\alpha_1, \dots, \alpha_n\}$}.
    
        \smallskip
        We can simply denote this by
        \[
            [T(\alpha)]_{\mc{B}} = [T]^{\mc{B}}_{\mc{B}}[\alpha]_{\mc{B}} = [T]_{\mc{B}}[\alpha]_{\mc{B}} = A[\alpha]_{\mc{B}}
        \]
        where $A = \Phi(T) \in F^{n \times n}$.
    \end{definition}

    \begin{example}
        Let $V$ be the vector space and $I \in L(V,V)$, the identity operator. For the basis of $V$, $\mc{B} = \{\alpha_1, \dots, \alpha_n\}$, Find $[I]^{\mc{B}}_{\mc{B}}$.
    \end{example}
    Let $A = [T]^{\mc{B}}_{\mc{B}}$.
    $I(\alpha_j) = \alpha_j \implies A_{:, j} = \left[\begin{matrix}
    0 \\ \vdots \\ 1 \\ \vdots \\ 0
    \end{matrix}\right] \implies A = I$.

\end{frame}

\begin{frame}{.}
    \begin{theorem}
        Let $V, W$, and $Z$ be finite-dimensional vector spaces over the field $F$.
        Let $T \in L(V,W), U \in L(W,Z)$.
        Let $\mc{B} = \{\alpha_1, \dots, \alpha_n\}, \mc{C} = \{\beta_1, \dots, \beta_m\}, \mc{D} = \{\gamma_1, \dots, \gamma_p\}$ are ordered bases for the spaces $V, W$ and $Z$, respectively.
        Let $[T]^{\mc{B}}_{\mc{C}} = A, [U]^{\mc{C}}_{\mc{D}} = B$.
        Then the matrix of the composition $UT$ relative to the pair $\mc{B}, \mc{D}$ is the product matrix $BA$. i.e., $C = [UT]^{\mc{B}}_{\mc{D}} = [U]^{\mc{C}}_{\mc{D}} [T]^{\mc{B}}_{\mc{C}} = BA$.
    \end{theorem}

    \ti{Proof.}
    \[  
        \begin{aligned}
            (UT) (\alpha_j) &= U (T(\alpha_j)) = U \left( \sum_{k=1}^m A_{kj} \beta_k \right) = \sum_{k=1}^m A_{kj} U(\beta_k) \\
            &=\sum_{k=1}^m A_{kj} \sum_{i=1}^p B_{ik} \gamma_i = \sum_{i=1}^p \sum_{k=1}^m B_{ik} A_{kj} \gamma_i
        \end{aligned}
    \]
    Thus, $C_{ij} = \sum_{k=1}^m B_{ik} A_{kj}$.
\end{frame}

\begin{frame}{.}
    Let assume that vector space $V$ has ordered basis $\mc{B} =\{\alpha_1, \dots, \alpha_n\}$.
    For linear operator $T, U \in L(V,V)$, and idnetity operator $I$, $UT =TU = I$ holds.
    Then $I = [I]^{\mc{B}}_{\mc{B}} = [UT]^{\mc{B}}_{\mc{B}} = [U]^{\mc{B}}_{\mc{B}} [T]^{\mc{B}}_{\mc{B}} = [T]^{\mc{B}}_{\mc{B}} [U]^{\mc{B}}_{\mc{B}}$ holds.
    This implies ${[T]^{\mc{B}}_{\mc{B}}}^{-1} = [U]^{\mc{B}}_{\mc{B}}$.

    Also for the operator $T,U$, if $U T = T U = I$ holds, then $U$ is called \tb{invert operator of $T$} and denoted by $T^{-1}$.
    We can re-write as follows, 


\end{frame}

\begin{frame}{.}
    Let $\mc{B} = \{\alpha_1, \dots, \alpha_n\}, \mc{B}^\prime = \{\alpha_1^\prime, \dots, \alpha_n^\prime\}$ both be the ordered basis for vector space $V$.
    There exists \ti{unique} operator $U$ s.t. $U(\alpha_j) = \alpha_j^\prime, j=1, \dots, n$.
    Theorem $\ref{th:3}$ says that linear transformation $U$ is invertible since it carris $\mc{B}$ onto $\mc{B}^\prime$.

    \smallskip
    $[U]^{\mc{B}}_{\mc{B}^\prime} = I$ by definition.
    Then what is the value of $[U]_{\mc{B}}$?
    % For the two bases $\mc{B}$ and $\mc{B}^\prime$, there exists matrix $P$ such that $[\alpha]_{\mc{B}} = P [\alpha]_{\mc{B}^\prime}$ where $\alpha \in V$.
    % This implies $[\alpha_j]_{\mc{B}} = \left[\begin{matrix}
    %     \delta_{1j} \\ \vdots \\ \delta_{nj}
    % \end{matrix}\right] = P $
    % Also, Let $A = [U]_{\mc{B}}$ s.t. $U(\alpha_j) = \left[ \begin{matrix}
    %     \alpha_1 & \cdots & \alpha_n
    % \end{matrix}\right] A_{:, j}$.
    Let $P = [U]_{\mc{B}}$.
    For the two bases $\mc{B}$ and $\mc{B}^\prime$, $U(\alpha_j) = \left[ \begin{matrix}
        \alpha_1 & \cdots & \alpha_n
    \end{matrix}\right] P_{:, j} = \alpha^\prime_j$ holds.
    $\left[ \begin{matrix}
        \alpha_1 & \cdots & \alpha_n
    \end{matrix}\right] P_{:, j} = \alpha^\prime_j
    \implies P_{:, j} = [\alpha_j^\prime]_{\mc{B}}$.
    % Let denote equation, $U(\alpha_j) = \left[ \begin{matrix}
    %     \alpha_1 & \cdots & \alpha_n
    % \end{matrix}\right] P_{:, j} = \alpha^\prime_j$, as $U(\alpha_j) = [\mc{B}] P_{:, j}$ for simplicity.

    Then for any $\alpha \in V$,$\alpha = \left[\begin{matrix}
        \alpha_1^\prime & \cdots & \alpha_n^\prime 
    \end{matrix}\right] [\alpha]_{\mc{B}^\prime}$.
    Let $[\alpha]_{\mc{B}^\prime} = \left[\begin{matrix}
        x_1 \\ \vdots \\ x_n
    \end{matrix}\right]$.
    Then $\alpha = \left[\begin{matrix}
    \alpha_1^\prime & \cdots & \alpha_n^\prime
    \end{matrix}\right]\left(x_1 \left[\begin{matrix}
        1 \\ \vdots \\ 0
    \end{matrix}\right] + \cdots + x_n \left[\begin{matrix}
        0 \\ \vdots \\ 1
    \end{matrix}\right]\right) = \left[\begin{matrix}
    \alpha_1 & \cdots & \alpha_n
    \end{matrix}\right] \left( x_1 P_{:, 1} + \cdots + x_n P_{:, n}\right) = \left[\begin{matrix}
    \alpha_1 & \cdots & \alpha_n
    \end{matrix}\right] P \left[\begin{matrix}
    x_1 \\ \vdots \\ x_n
    \end{matrix}\right]$.
    This implies $[\alpha]_{\mc{B}} = P [\alpha]_{\mc{B}^\prime} = [U]_{\mc{B}}[\alpha]_{\mc{B}^\prime}$.

    \smallskip
    So this linear operator $U$ is called \tb{the change of basis from $\mc{B}$ to $\mc{B}^\prime$}.

\end{frame}

% \begin{frame}{.}
%     Let $\mc{B} = \{\alpha_1, \dots, \alpha_n\}, \mc{B}^\prime = \{\alpha_1^\prime, \dots, \alpha_n^\prime\}$ both be the ordered basis for vector space $V$.
%     There exists \ti{unique} matrix $P$ that satisfies $[\alpha_j^\prime]_{\mc{B}} = P_{:, j}$ (i.e. $[\alpha^\prime_j]_{\mc{B}} = P [\alpha^\prime_j]_{\mc{B}^\prime}$).
%     We call this matrix $P$ as the \tb{change of basis from $\mc{B}$ to $\mc{B}^\prime$}.
%     This matrix transforms from coordinate of $\mc{B}^\prime$ to coordinate of $\mc{B}$.
%     For example,
%     $ [\alpha^\prime_1]_{\mc{B}} = P [\alpha_1^\prime]_{\mc{B}^\prime} \implies \alpha_1^\prime 
%     = \left[\begin{matrix}
%         \alpha_1^\prime & \cdots & \alpha_n^\prime
%     \end{matrix}\right] \left[\begin{matrix}
%     1 \\ 0 \\ \vdots \\ 0
%     \end{matrix}\right] 
%     = \left[\begin{matrix}
%         \alpha_1 & \cdots & \alpha_n
%     \end{matrix}\right] P \left[\begin{matrix}
%         1 \\ 0 \\ \vdots \\ 0
%     \end{matrix}\right] = \left[\begin{matrix}
%         \alpha_1 & \cdots & \alpha_n
%     \end{matrix}\right] \left[\begin{matrix}
%         P_{11} \\ P_{21} \\ \vdots \\ P_{n1}
%     \end{matrix}\right]$.
%     Let denote this equation as $\alpha^\prime_1 = [\mc{B}^\prime][\alpha_1^\prime]_{\mc{B}^\prime} = [\mc{B}]P[\alpha^\prime_1]_{\mc{B}^\prime} = [\mc{B}]P_{:, 1}$ simply.

%     \bigskip
%     For any $\alpha \in V$, there exists some coordinates $[\alpha]_{\mc{B}^\prime} = \left[ \begin{matrix}
%         y_1 \\ \vdots \\ y_n
%     \end{matrix}\right]$ that satisfies $\alpha = [\mc{B}^\prime][\alpha]_{\mc{B}^\prime}$.
%     Then $\alpha = [\mc{B}^\prime]\left( y_1 [\alpha^\prime_1]_{\mc{B}^\prime} + \cdots + y_n  [\alpha^\prime_n]_{\mc{B}^\prime} \right) = y_1 [\mc{B}]P_{:, 1} + \cdots + y_n [\mc{B}]P_{:, n} = [\mc{B}]\left( y_1 P_{:, 1} + \cdots + y_n P_{:, n}\right) = [\mc{B}]P [\alpha]_{\mc{B}^\prime}$.

%     So for the same vector $\alpha \in V$, we obtain both coordinates $P[\alpha]_{\mc{B}^\prime}$ and $[\alpha]_{\mc{B}}$ for different bases $\mc{B}^\prime$ and $\mc{B}$ and there exists relationship $[\alpha]_{\mc{B}} = P [\alpha]_{\mc{B}^\prime}$.
%     This is why the matrix $P$ is called change of basis.
% \end{frame}

\begin{frame}{.}
    \begin{theorem}
        Let $V$ be a finite-dimensional vector space over the field $F$, and let $\mc{B} = \{\alpha_1, \dots, \alpha_n\}$ and $\mc{B}^\prime = \{\alpha_1^\prime, \dots, \alpha_n^\prime\}$ be ordered basis of $V$.
        Suppose $T$ is a linear operator on $V$.
        If $P = \left[\begin{matrix}
         | & \cdots & | \\ P_{:,1} & \cdots & P_{:, n} \\ | & \empty & | 
        \end{matrix}\right]$ is the $n \times n$ matrix where $P_{:, j} = [\alpha_j^\prime]_\mc{B}$, then $[T]_{\mc{B}^\prime} = P^{-1}[T]_{\mc{B}}P$.
        Alternatively, if $U$ is the invertible operator on $V$ defined by $U(\alpha_j) = \alpha_j^\prime, j=1,\dots, n$, then $[T]_{\mc{B}^\prime} = [U]_{\mc{B}}^{-1}[T]_{\mc{B}}[U]_{\mc{B}}$.
    \end{theorem}
    \ti{Proof.}

    Because $U$ is change of basis from $\mc{B}$ to $\mc{B}^\prime$, $[U]_{\mc{B}} = P$ and $[\alpha]_{\mc{B}} = [U]_{\mc{B}} [\alpha]_{\mc{B}^\prime}$.

    For any $\alpha \in V$, $[T(\alpha)]_{\mc{B}} = [T]_{\mc{B}} [\alpha]_{\mc{B}} = [T]_{\mc{B}} [U]_{\mc{B}} [\alpha]_{\mc{B}^\prime}$.
    Also $[T(\alpha)]_{\mc{B}} = [U]_{\mc{B}}[T(\alpha)]_{\mc{B}^\prime}$.
    Thus, $[U]_{\mc{B}} [T(\alpha)]_{\mc{B}^\prime} = [T]_{\mc{B}}[U]_{\mc{B}}[\alpha]_{\mc{B}^\prime} \implies [T(\alpha)]_{\mc{B}^\prime} = [U]_{\mc{B}}^{-1}[T]_{\mc{B}} [U]_{\mc{B}} [\alpha]_{\mc{B}^\prime}$ holds.

    By the definition, $[T(\alpha)]_{\mc{B}^\prime} = [T]_{\mc{B}^\prime} [\alpha]_{\mc{B}^\prime}$. This implies $[T]_{\mc{B}^\prime} = [U]^{-1}_{\mc{B}} [T]_{\mc{B}} [U]_{\mc{B}}$. $\qed$
\end{frame}

\begin{frame}{.}
    \begin{example}
        Let $T$ be the linear operator on $\mbb{R}^2$ defined by $T(x_1, x_2) = (x_1, 0)$. Let $\mc{B} = \{\epsilon_1, \epsilon_2\}$ be the basis of $\mbb{R}^2$ where $\epsilon$ is standard basis of $\mbb{R}^2$, and $\mc{B}^\prime = \{(1,1), (2,1)\}$ be the another basis of $V$. Find $[T]_{\mc{B}^\prime}$.
    \end{example}

    First, $[T]_{\mc{B}} = \left[\begin{matrix}
        1 & 0 \\ 0 & 0
    \end{matrix}\right]$.

    Let $U$ be the linear operator that is change of basis from $\mc{B}$ to $\mc{B}^\prime$. Then $U(\epsilon_1) = (1,1) = \epsilon_1 + \epsilon_2, U(\epsilon_2) = (2,1) = 2 \epsilon_1 + \epsilon_2$. This implies $[U]_{\mc{B}} = \left[\begin{matrix}
    1 & 2 \\ 1 & 1
    \end{matrix}\right]$.

    Then by $[T]_{\mc{B}^\prime} = [U]_{\mc{B}}^{-1}[T]_{\mc{B}}[U]_{\mc{B}}$, $[T]_{\mc{B}^\prime} = \left[\begin{matrix}
    -1 & 2 \\ 1 & 1
    \end{matrix}\right]\left[ \begin{matrix}1 & 0 \\ 0 & 0\end{matrix}\right]\left[\begin{matrix}
    1 & 2\\ 1 & 1
    \end{matrix}\right] = \left[\begin{matrix}
        -1 & -2 \\ 1 & 2
    \end{matrix}\right]$ 

    We can easily check this by

    $T(1, 1) = (1, 0) = - \epsilon_1^\prime + \epsilon_2^\prime, T(2, 1) = (2, 0) = -2 \epsilon_1^\prime + 2 \epsilon_2^\prime$.
\end{frame}

\begin{frame}{.}
    \begin{definition}
        Let $A$ and $B$ be $n\times n$ (square) matrices over the field $F$. We say that \ti{$B$ is similar to $A$ over $F$} if there is an invertible $n \times n$ matrix $P$ over $F$ such that $B = P^{-1}AP$.
    \end{definition}
    \begin{block}{Exercise 3.4.4}
        Let $V$ be a two-dimensional vector space over the field $F$, and let $\mc{B}$ be an ordered basis for $V$.
        If $T$ is a linear operator on $V$ and $[T]_{\mc{B}} = \left[\begin{matrix}
            a & b \\ c & d
        \end{matrix}\right]$.
        Prove that $T^2 - (a+d)T + (ad - bc)I$ = 0.
    \end{block}

    For any $\alpha \in V$, $[T(\alpha)]_{\mc{B}} = [T]_{\mc{B}} [\alpha]_{\mc{B}}$ and $[T(T(\alpha))]_{\mc{B}} = [T]_{\mc{B}}^2 [\alpha]_{\mc{B}}$ and $[I(\alpha)]_{\mc{B}} = [\alpha]_{\mc{B}}$.
    Then $[T^2(\alpha)]_{\mc{B}} = \left[\begin{matrix}
    a & b\\ c & d
    \end{matrix}\right]^2 [\alpha]_{\mc{B}} = \left[\begin{matrix}
        a^2 + bc & ab + bd \\ ac + dc & bc + d^2
    \end{matrix}\right] [\alpha]_{\mc{B}}$.
    $(a+d)[T(\alpha)]_{\mc{B}} = \left[\begin{matrix}
    a^2 + ad & ab + bd \\ ac + cd & ad + d^2
    \end{matrix}\right] [\alpha]_{\mc{B}}$.
    $(ad -bc)[I(\alpha)]_{\mc{B}} = \left[\begin{matrix}
        ad - bc & 0 \\ 0 & ad -bc
    \end{matrix}\right][\alpha]_{\mc{B}}$.

    This implies $[(T^2 - (a+d)T + (ad-bc)I)(\alpha)]_{\mc{B}} = \left[\begin{matrix}
    0 & 0 \\ 0 & 0
    \end{matrix}\right][\alpha]_{\mc{B}}$.
    Therefore, for any $\alpha \in V$, $(T^2 - (a+d)T + (ad - bc)I) (\alpha) = 0$.
\end{frame}

\begin{frame}{.}
    \begin{block}{Exercise 3.4.5}
        Let $T$ be the linear operator on $\mbb{R}^3$, the matrix of which in the standard ordered basis is $A = \left[\begin{matrix}
            1 & 2&1 \\0&1&1\\-1&3&4
        \end{matrix}\right]$.
        Find a basis for the range of $T$ and a basis for the null space of $T$.
    \end{block}

    For any $\alpha \in \mbb{R}^3$, $[T(\alpha)]_{\mc{B}} = A [\alpha]_{\mc{B}}$.
    Let $\mc{B} = \{\epsilon_1, \epsilon_2, \epsilon_3\}$ and $[\alpha]_{\mc{B}} = \left[\begin{matrix}
    x_1 \\ x_2 \\ x_3
    \end{matrix}\right]$.
    Then $T(\alpha) = [T(\alpha)]_{\mc{B}} = A \left[\begin{matrix}
    x_1 \\ x_2 \\ x_3
    \end{matrix}\right] = 
    \left[\begin{matrix}
        1 \\ 0 \\ -1
    \end{matrix}\right]x_1 +
    \left[\begin{matrix}
    2 \\ 1 \\ 3
    \end{matrix}\right]x_2 +
    \left[\begin{matrix}
    1 \\ 1\\ 4
    \end{matrix}\right]x_3 =
    \left[\begin{matrix}
    1 \\ 0 \\-1
    \end{matrix}\right](x_1 + x_2) +
    \left[\begin{matrix}
    1\\1\\4
    \end{matrix}\right](x_2 + x_3)$.

    Then the basis of range of $T$ is $\left[\begin{matrix}
    1 \\ 0 \\ -1
    \end{matrix}\right]$ and $\left[\begin{matrix}
    1 \\ 1 \\ 4
    \end{matrix}\right]$.

    The null space is $\forall x_2 \in \mbb{R}, -x_1 = x_2 = -x_3$.
    So the basis of null space of $T$ is $\left[\begin{matrix}
    -1 \\ 1 \\ -1
    \end{matrix}\right]$.

\end{frame}

\begin{frame}{.}
    \begin{block}{Exercise 3.4.8}
        Let $\theta$ be a real number.
        Prove that the following two matrices are similar over the field of complex numbers
        \[
            \left[\begin{matrix}
                \cos \theta & -\sin \theta \\ \sin \theta & \cos \theta
            \end{matrix}\right],
            \left[\begin{matrix}
                e^{i \theta} & 0 \\ 0 & e^{-i \theta}
            \end{matrix}\right]
        \]
    \end{block}
    By taylor series, $e^{i \theta} = 1 + \frac{i\theta}{1!} - \frac{\theta^2}{2!} - \frac{i\theta^3}{3!} + \frac{\theta^4}{4!} + \cdots = \left(1 - \frac{\theta^2}{2!} + \frac{\theta^4}{4!} - \cdots \right) + i\left(\frac{\theta}{1!} - \frac{\theta^3}{3!} + \frac{\theta^5}{5!} - \cdots \right) = \cos \theta + i \sin \theta$.
    Note that $\cos \theta = 1  - \frac{\theta}{2!} + \frac{\theta^2}{4!} -  \cdots $ and $\sin \theta =  \frac{\theta}{1!} - \frac{\theta^3}{3!} + \frac{\theta^5}{5!} - \cdots$.
    Similarly $e^{-i \theta} = \cos \theta - i \sin \theta$.
    Let $\mc{B} = \{ \left[\begin{matrix}
     1 \\ 0
    \end{matrix}\right], \left[\begin{matrix}
    0 \\ 1
    \end{matrix}\right] \}$ and $\mc{C} = \{\begin{bmatrix}
    1 \\ -i
    \end{bmatrix}, \begin{bmatrix}
    1 \\ i
    \end{bmatrix}\}$ be the ordered basis.
    Let $T$ be the linear operator in $\mbb{C}^2$ defined by $T(\alpha) = \begin{bmatrix}
        \cos \theta & - \sin \theta \\ \sin \theta & \cos \theta
    \end{bmatrix} \alpha$, for any $\alpha \in \mbb{C}^2$.
    Then $[T]_{\mc{B}} = \begin{bmatrix}
        \cos \theta & - \sin \theta \\ \sin \theta & \cos \theta
    \end{bmatrix}$ and
    $[T]_{\mc{C}} = \begin{bmatrix}
       e^{i \theta} & 0 \\ 0 & e^{-i \theta}
    \end{bmatrix}$.
    Let $U$ be the change of basis from $\mc{B}$ to $\mc{C}$. Then $[U]_{\mc{B}} = \begin{bmatrix}
     1 & 1 \\ -i & i
    \end{bmatrix}$ and $[U]_{\mc{B}}^{-1} = 1/2 \begin{bmatrix}
        1 & i \\ 1 &-i
    \end{bmatrix}$.

    Then $[T]_{\mc{C}} = [U]_{\mc{B}}^{-1}[T]_{\mc{B}}[U]_{\mc{B}}$ holds.
    Therefore, $\begin{bmatrix}
    \cos \theta & - \sin \theta \\ \sin \theta & \cos \theta
    \end{bmatrix}$ and $\begin{bmatrix}
        e^{i \theta} & 0 \\ 0 & e^{-i \theta}
    \end{bmatrix}$ are similar.
\end{frame}

\begin{frame}{.}
    \begin{block}{$\star$ Exercise 3.4.9}
        Let $V$ be a finite-dimensional vector space over the field $F$ and let $S$ and $T$ be linear operator on $V$. We ask: When do there exist ordered bases $\mc{B}$ and $\mc{B}^\prime$ for $V$ such that $[S]_{\mc{B}} = [T]_{\mc{B}^\prime}$?
        Prove that such bases exist if and only if there is an invertible linear operator $U$ on $V$ such that $T = U S U^{-1}$.
    \end{block}
    
    ($\implies$): Let $\mc{B} = \{\alpha_1, \dots, \alpha_n\}$ and $\mc{B}^\prime = \{\alpha_1^\prime, \dots, \alpha_n^\prime\}$ be the ordered basis of $V$.
    Let assume $[S]_{\mc{B}} = [T]_{\mc{B}^\prime}$ holds.
    Let $U$ be the change of basis from $\mc{B}$ onto $\mc{B}^\prime$.
    % For any $1 \leq j \leq n$, $[S(\alpha_j)]_{\mc{B}} = [T(\beta_j)]_{\mc{C}}$. Also
    Note that $[U]_{\mc{B}^\prime}^{\mc{B}} = I$ and $[U^{-1}]_{\mc{B}}^{\mc{B}^\prime}=I$.
    For any $\alpha \in V$, $[T(\alpha)]_{\mc{B}^\prime} = [T]_{\mc{B}^\prime}[\alpha]_{\mc{B}^\prime} = [S]_{\mc{B}}[\alpha]_{\mc{B}^\prime} = [U]^{\mc{B}}_{\mc{B}^\prime} [S]_{\mc{B}} [U^{-1}]_{\mc{B}}^{\mc{B}^\prime} [\alpha]_{\mc{B}} = [U S U^{-1}]_{\mc{B}^\prime} [\alpha]_{\mc{B}}$.
    $[T]_{\mc{B}^\prime} = [U S U^{-1}]_{\mc{B}^\prime} \implies T = U S U^{-1}$.

    % ($\impliedby$): Let $[S]_{\mc{B}} = A, [T]_{\mc{B}^\prime}, [U]^{\mc{B}}_{\mc{B}^\prime} = C,  [U^{-1}]_{\mc{B}}^{\mc{B}^\prime} = C^{-1}$.
    % By the assumption, $TU = US$ holds.
    % For any $\alpha \in V$, $\alpha = x_1\alpha_1 + \cdots + x_n \alpha_n$.
    % Then $(TU)(\alpha) = (US)(\alpha)$.

    % In left side, $(TU)(\alpha) = (TU)(x_1\alpha_1 + \cdots + x_n \alpha_n) = T ( \sum_{i=1}^n x_i \sum_{j=1}^n C_{j,i}\alpha_{j}^\prime) = T(\sum_{j=1}^n \sum_{i=1}^n x_i C_{j,i}\alpha_j^\prime) = \sum_{j=1}^n \sum_{i=1}^n x_i C_{j,i} T(\alpha_j^\prime) = \sum_{j=1}^n \sum_{i=1}^n x_i C_{j,i} \sum_{k=1}^n B_{k,j} \alpha_k^\prime = \sum_{j=1}^n \sum_{i=1}^n \sum_{k=1}^n B_{k,j} C_{j,i} x_i \alpha_k^\prime$.

    % Also in right side, $(US)(\alpha) = (US)(x_1 \alpha_1 + \cdots + x_n \alpha_n) = U ( \sum_{i=1}^n x_i \sum_{j=1}^n A_{j,i} \alpha_j) = \sum_{j=1}^n \sum_{i=1}^n A_{j,i} x_i U(\alpha_j) = \sum_{i=1}^n \sum_{j=1}^n \sum_{k=1}^n C_{k,j}A_{j,i} x_i \alpha^\prime_k$.

    % Thus, $\sum_{j=1}^n C_{k,j} A_{j,i} = \sum_{j=1}^n B_{k,j} C_{j,i} \implies CA = BC$ holds.

    ($\impliedby$): Let $\mc{B} = \{\alpha_1, \dots , \alpha_n\}$ be the ordered basis of $V$ and $\mc{B}^\prime = \{\alpha_1^\prime, \dots, \alpha_n^\prime\}$ be its image under $U$. Then $[T]_{\mc{B^\prime}} = [USU^{-1}]_{\mc{B}^\prime} = [U]_{\mc{B}^\prime}^{\mc{B}} [S]_{\mc{B}} [U^{-1}]_{\mc{B}}^{\mc{B}^\prime}$ holds and $[U]^{\mc{B}}_{\mc{B}^\prime} =[U^{-1}]^{\mc{B}^\prime}_{\mc{B}}= I$. Thus, $[T]_{\mc{B}^\prime} = [S]_{\mc{B}}$ holds.
\end{frame}

\begin{frame}{.}
    \begin{block}{$\star$ Exercise 3.4.11}
        Let $W$ be the space of $n \times 1$ column matrices over a field $F$.
        If $A$ is an $n \times n$ matrix over $F$, then $A$ defines a linear operator $L_A$ on $W$ through left multiplication: $L_A (X) = AX$.
        Prove that every linear operator on $W$ is left-multiplication by some $n\times n$ matrix, i.e., is $L_A$ for some $A$.

        Now suppose $V$ is an $n$-dimensional vector space over the field $F$, and let $\mc{B}$ be an ordered basis for $V$. For each $\alpha$ in $V$, define $U(\alpha) =[\alpha]_{\mc{B}}$.
        Prove that $U$ is an isomorphism of $V$ onto $W$.
        If $T$ is a linear operator on $V$, then $UTU^{-1}$ is a linear operator on $W$. Accordingly, $UTU^{-1}$ is left multiplication by some $n\times n$ matrix $A$.
        What is $A$?
    \end{block}

    Let $\mc{B} = \{\epsilon_1, \dots, \epsilon_n\}$ be the ordered standard basis on $W$. Then for any linear operator $T$ onto $V$, image of $\mc{B}$ by $T$ is unique. i.e., $\{T(\epsilon_1), \dots, T(\epsilon_n)\}$ is unique.
    Let $A_{:, j} = T(\epsilon_j)$.
    Then $L_A(\epsilon_j) = A_{:, j} = T(\epsilon_j)$.
    Then for any $\alpha \in W$, let $\alpha = x_1 \epsilon_1 + \cdots + x_n \epsilon_n$.
    $L_{A}(\alpha) = x_1 L_A(\epsilon_1) + \cdots + x_n L_A(\epsilon_n) = AX$.
    Also, $T(\alpha) = x_1 T(\epsilon_1) + x_n T(\epsilon_n) = AX$.
    Therefore $L_A = T$.
    $\qed$

    Let $\mc{B} = \{\alpha_1, \dots, \alpha_n\}$. For any $\beta_1, \beta_2 \in V$ and $c \in F$, there exists scalars $x_i, y_i$ s.t. $\beta_1 = x_1 \alpha_1 + \cdots + x_n \alpha_n, \beta_2 = y_1 \alpha_1 + \cdots + x_n \alpha_n$.
    Then $U(\beta_1) = [x_1, \dots, x_n]^{-1}, U(\beta_2) = [y_1, \dots, y_n]^{-1}$.
    $U(c\beta_1 + \beta_2) = c [y_1, \dots , y_n]^{-1} + [x_1, \dots , x_n]^{-1} = c U(\beta_1) + U(\beta_2)$.
    Thus, $U$ is linear.

    Also, for any $\alpha \in V$, $U(\alpha) = 0 \implies \alpha = 0$.
    Thus $\ker U = \{0\}$ and $U$ is invertible.
    Therefore $U$ is isomorphism of $V$ onto $W$.
    $\qed$

    Note that $U(\alpha_j) = \epsilon_j$ where $\epsilon_j$ is $j$-th standard basis of $W$.
    Let $[T]_{\mc{B}} = M$.
    Then $(UTU^{-1}) (\epsilon_j) = UT (\alpha_j) = U(M_{1j} \alpha_1 + \cdots + M_{nj} \alpha_n) = M_{;, j}$.
    This implies $UTU^{-1} = L_{[T]_{\mc{B}}}$.
\end{frame}

\begin{frame}{.}
    \begin{block}{$\star$ Exercise 3.4.12}
        Let $V$ be an $n$-dimensional vector space over the field $F$, and let $\mc{B} = \{\alpha_1, \dots, \alpha_n\}$ be an ordered basis for $V$.
        \begin{enumerate}
            \item According to Theorem \ref{th:1}, there is a unique linear operator $T$ on $V$ s.t. $T(\alpha_j) = \alpha_{j+1}, j=1, \dots, n-1,T(\alpha_n) = 0$.
            What is the matrix $A$ of $T$ in the ordered basis $\mc{B}$?
            \item Prove that $T^n = 0$ but $T^{n-1} \neq 0$.
            \item Let $S$ be any linear operator on $V$ s.t. $S^n = 0$ but $S^{n-1} \neq 0$.
            Prove that there is an ordered basis $\mc{B}^\prime$ for $V$ s.t. the matrix of $S$ in the ordered basis $\mc{B}^\prime$ is the matrix $A$ of (1.).
            \item Prove that if $M$ and $N$ are $n \times n$ matrices over $F$ such that $M^n = N^n = 0$ but $M^{n-1} \neq 0 \neq N^{n-1}$, then $M$ and $N$ are similar.
        \end{enumerate}
    \end{block}
    (1.): $[T]_{\mc{B}} = \begin{bmatrix}
     | & \cdots &| & | \\ \epsilon_2 & \cdots &\epsilon_n & 0 \\ | & \cdots & | &|
    \end{bmatrix}$.

    (2.): $[T]^n_{\mc{B}} = 0$ but $[T]^{n-1}_{\mc{B}} = \begin{bmatrix}
        0 & \cdots & 0 & 1 \\ 0 & \cdots & 0 & 0 \\ \vdots & \empty & \vdots & \vdots \\ 0 & \cdots & 0 & 0
    \end{bmatrix}$.
    For any $\alpha \in V$, $\alpha = x_1 \alpha_1 + \cdots + x_n \alpha_n$ and $[T^n (\alpha)]_{\mc{B}} = [T]^n_{\mc{B}} [\alpha]_{\mc{B}} = 0 \implies T^n = 0$.
    But $[T^{n-1}(\alpha_1)]_{\mc{B}} = [T]^{n-1}_{\mc{B}} [\alpha_1]_{\mc{B}} = [0, \cdots, 0, 1]^{-1} \implies T^{n-1} \neq 0$.
\end{frame}

\begin{frame}{.}
    (3.): \textcolor{red}{to be continue}
    (4.): 
\end{frame}

\begin{frame}{.}
    \begin{block}{Exercise 3.4.13}
        Let $V$ and $W$ be finite-dimensional vector spaces over the field $F$ and let $T$ be a linear transformation from $V$ into $W$.
        If $\mc{B} = \{\alpha_1, \dots, \alpha_n\}, \mc{B}^\prime = \{\beta_1, \dots, \beta_m\}$ are ordered bases for $V$ and $W$, respectively, define the linear transformations $E^{p,q}$ as $E^{p,q}(\alpha_i) = \delta_{iq} \beta_p$.
        Then the $E^{p,q}, 1 \leq p \leq m, 1 \leq q \leq n$, form a basis for $L(V,W)$ and so $T = \sum_{p=1}^n \sum_{q=1}^m A_{p,q} E^{p,q}$ for certain scalars $A_{p,q}$ (the coordinates of $T$ in this basis for $L(V,W)$).
        Show that the matrix $A$ with entries $A_{p,q}$ is precisely the matrix of $T$ relative to the pair $\mc{B}, \mc{B}^\prime$.
    \end{block}

    $T(\alpha_j) = \sum_{p=1}^m A_{p,j} E^{p,j}(\alpha_j) = \sum_{p=1}^m A_{p,j} \beta_p$.
    This implies $([T]^{\mc{B}}_{\mc{B}^\prime})_{:, j} = A_{:, j}$.
    Since each column of $([T])_{\mc{B}^\prime}^{\mc{B}}$ and $A$ are same, $[T]_{\mc{B}^\prime}^{\mc{B}} = A$.
    $\qed$.
\end{frame}

\subsection{Linear Functionals}
\begingroup
    \setbeamertemplate{frametitle}{%
    \vskip1ex
    \usebeamerfont{frametitle}%
    \insertframetitle\par        %  ← 원하는 대로 변경 가능
    \vskip1ex
    \hrule                             % 밑줄(선택)
    }
    \begin{frame}
        \frametitle{Table of Contents}
        \tableofcontents[currentsubsection]
    \end{frame}
\endgroup

\begin{frame}{.}
    If $V$ is a vector space over the field $F$, a linear transformation $f$ from $V$ into the scalar field $F$ is also called a \tb{linear functional} on $V$.
    If we start from scratch, this means that $f$ is a function from $V$ into $F$ such that
    \[
        f(c \alpha + \beta) = c f(\alpha) + f(\beta)
    \]
    for all vectors $\alpha$ and $\beta$ in $v$ and all scalars $c$ in $F$.
    The concept of linear functional is important in the study of finite-dimensional spaces because it helps to organize and clarify the discussion of subspaces, linear equations, and coordinates.


\end{frame}

\begin{frame}{.}
    \begin{example}
        Let $F$ be a field and let $a_1, \dots, a_n$ be scalars in $F$.
        Define a function $f$ on $F^n$ by
        \[
            f(x_1, \dots, x_n) = a_1 x_1 + \cdots + a_n x_n
        \]
        Then $f$ is a linear functional on $F^n$ (dot product, standard inner product).
        It is the linear functional which is represented by the matrix $[a_1 \cdots a_n]$ relative to the standard ordered basis for $F^n$ and the basis $\{1\}$ for $F$:
        \[
            a_j = f(\epsilon_j), j=1, \dots, n
        \]
        Every linear functional on $F^n$ is of this form, for some scalars $a_1, \dots, a_n$.
        That is immediate from the definition of linear functional because we define $a_j = f(\epsilon_j)$ and use the linearity
        \[
            f(x_1, \dots, x_n) = f\left(\sum_j x_j \epsilon_j \right) =\sum_j x_j f(\epsilon_j) = \sum_j a_j x_j
        \]
    \end{example}
\end{frame}

\begin{frame}{.}
    \begin{example}
        Here is an important example of a linear functional.
        Let $n$ be a positive integer and $F$ a field.
        If $A$ is an $n \times n$ matrix with entries in $F$, the \tb{trace} of $A$ is the scalar
        \[
        \tr A \coloneq A_{1,1} + \cdots + A_{n, n}
        \]
        The trace function is a linear functional on the matrix space $F^{n \times n}$ because
        \[
            \tr (cA +B)= \sum_{i=1}^n (cA_{i,i} + B_{i,i}) = c \tr A + \tr B
        \]
    \end{example}

    \begin{itemize}
        \item Note that for matrix $A,B \in F^{n \times n}$, $\tr(AB) = \tr(BA)$
        \item For similar matrix $A,B \in F^{n \times n}$, $\tr(A) = \tr(PBP^{-1}) = \tr(BPP^{-1}) = \tr(B)$
    \end{itemize}

    \begin{example}
        Let $[a,b]$ be a closed interval on the real line and let $C([a,b])$ be the space of continuous real-valued functions on $[a,b]$.
        Then
        \[L(g) = \int_a^b g(t) dt\]
        defines a linear functional $L$ on $C([a,b])$.
    \end{example}
\end{frame}

\begin{frame}{.}
    If $V$ is a vector space, the collection of all linear functionals on $V$ forms a vector space in a natural way.
    It is the space $L(V,F)$.
    We denote this space by $V^\ast$ and call it the \tb{dual space} of $V$.
    \[
        V^\ast \coloneq L(V,F)
    \]
    Note that the basis of $F$ is $\{1\}$.
    If $V$ is finite-dimensional, then because of Theorem \ref{th:5}, \[\dim V = \dim V^\ast\] 

    \bigskip
    Let $\mc{B} = \{\alpha_1, \dots, \alpha_n\}$ be a basis of $V$.
    By theorem \ref{th:1}, there exists unique $n$ linear functionals on $V$ s.t.
    \[
        f_i (\alpha_j) = \delta_{ij}
    \]
    This linear functionals $\mc{B}^\ast = \{f_1, \dots , f_n\}$ consists basis of $V^\ast$.
    Below is the proof.
    First, $\Span \mc{B^\ast} = V^\ast$.
    For any $f \in V^\ast$, $f(\alpha_j) = c_j$ for some scalars $c_j$ by theorem \ref{th:1}.
    Then $f = (c_1 f_1 + \cdots + c_n f_n)$ because $(c_1 f_1 + \cdots + c_n f_n)(\alpha_j) = c_1 \delta_{1j} + \cdots + c_n \delta_{nj} = c_j$.
    This implies $\Span \mc{B}^\ast = V^\ast$.

    \smallskip
    Second, $\mc{B}^\ast$ is lin. ind.
    %Let $f = c_1 f_1 + \cdots c_n f_n \in V^\ast$.
    Let $(c_1 f_1 + \cdots + c_n f_n) = 0$. Then $\forall 1 \leq j \leq n, (c_1 f_1 + \cdots + c_n f_n)(\alpha_j) = 0 \implies c_j = 0 \implies \mc{B}^\ast$ is lin. ind.

    \smallskip
    Therefore, $\mc{B}^\ast$ is basis of $V^\ast$.
    This $\mc{B}^\ast$ is called \tb{dual basis} of $\mc{B}$.
\end{frame}

\begin{frame}{.}
    \begin{theorem}
        Let $V$ be a finite-dimensional vector space over the field $F$, and let $\mc{B} =\{\alpha_1, \dots, \alpha_n\}$ be a basis for $V$.
        Then there is a unique dual basis $\mc{B}^\ast = \{f_1, \dots, f_n \}$ for $V^\ast$ s.t. $f_i ( \alpha_j ) = \delta_{ij}$.
        For each linear functional $f \in V$ we have $f = \sum_{i=1}^n f(\alpha_i) f_i$ and for each vector $\alpha$ in $V$ we have $\alpha = \sum_{i=1}^n f_i (\alpha) \alpha_i$
    \end{theorem}

    \ti{Proof.}
    We already know that $\mc{B}^\ast$ is a unique dual basis which is \tb{dual to $\mc{B}$}.
    Because $\mc{B}^\ast$ is basis, $f \in V^\ast$ can be expressed by $f = c_i f_1 + \cdots + c_n f_n$ for some scalars $c_i$.
    Then $f(\alpha_j) = c_1 \delta_{1j} + \cdots + c_n \delta_{nj} = c_j$.
    Thus, $f = f(\alpha_1)f_1 + \cdots + f(\alpha_n) f_n$ holds.

    \smallskip
    For any $\alpha \in V$, there are some scalars $x_i$ s.t. $\alpha = x_1 \alpha_1 + \cdots + x_n \alpha_n$.
    Then $f_i(\alpha) = x_1 f_i (\alpha_1) + \cdots + x_n f_i (\alpha_n) = x_i$.
    Thus $\alpha = f_1(\alpha) \alpha_1 + \cdots + f_n (\alpha) \alpha_n$.
    $\qed$

    \bigskip
    \begin{itemize}
        \item Equation $\alpha = \sum_{i=1}^n f_i (\alpha) \alpha_i$ provides us with a nice way of describing what the dual basis is.
        It says, if $\mc{B} = \{\alpha_1, \dots, \alpha_n \}$ is ordered basis for $V$ and $\mc{B}^\ast = \{f_1, \dots, f_n \}$ is the dual basis of $\mc{B}$, then $f_i$ is precisely the function which assigns to each vector $\alpha$ in $V$ the $i$th coordinate of $\alpha$ relative to the ordered basis $\mc{B}$. Thus we may also call the $f_i$, the coordinate functions for $\mc{B}$.
        \item For $f \in V^\ast$, ther exists some scalars $a_i$ s.t. $f(\alpha_i) = a_i$.
        For $\alpha \in V$, $\alpha = x_1 \alpha_1 + \cdots + x_n \alpha_n$.
        Then $f(\alpha) = x_1 a_1 + \cdots + x_n a_n$.
    \end{itemize}
\end{frame}

\begin{frame}{.}
    \begin{example}
        Let $V$ be the vector space of all polynomial functions from $\mbb{R}$ to $\mbb{R}$ which have degree less than $3$.
        For some $t_1, t_2, t_3 \in \mbb{R}$ and $p \in V$, define $L_{i}(p) \coloneq p(t_i), i=1,2,3$.
        Then $L_i$ becomes linear functional on $V$ because $L_i \in L(V, \mbb{R})$.
        Note that $\dim V = 3$.

        \smallskip
        For any scalar $c_1, c_2, c_3 \in \mbb{R}$, let $c_1 L_1 + c_2 L_2 + c_3 L_3 = 0$.
        Then for $1, x, x^2$, which are the elements of $V$, $c_1 + c_2 + c_3 = 0, t_1 c_1 + t_2 c_2 + t_3 c_3 =0 , t_1^2 c_1 + t_2^2 c_2 + t_3 ^2 c_3 = 0$ holds.
        Rewriting, $\begin{bmatrix}
        1 & 1 & 1 \\ t_1 & t_2 & t_3 \\ t_1^2 & t_2^2 & t_3^2
        \end{bmatrix} \begin{bmatrix}
         c_1 \\ c_2 \\c_3
        \end{bmatrix} = 0$.
        Since $t_1 \neq t_2 \neq t_3$, so the matrix $\begin{bmatrix}
        1 & 1 & 1 \\ t_1 & t_2 & t_3 \\ t_1^2 & t_2^2 & t_3^2
        \end{bmatrix}$ is invertible, $c_1 = c_2 = c_3 = 0$ and $\{L_1, L_2, L_3\}$ lin. ind. and $| \{L_1, L_2, L_3 \}| = \dim V$, $\mc{B}^\ast = \{L_1, L_2, L_3\}$ is basis of $V^\ast$.

        Then what is the basis for $V$, of which $\mc{B}^\ast$ is the dual? basis of $V$, $\mc{B} = \{p_1, p_2, p_3\}$, which is the dual of $\mc{B}^\ast$ should satisfy $L_i(p_j) = p_j(t_i) = \delta_{ij}$.
        $p_1(x) = \frac{(x - t_2 )(x - t_3)}{(t_1 - t_2)(t_1 - t_3)}, p_2(x) = \frac{(x - t_1)(x - t_3)}{(t_2 - t_1)(t_2 - t_3)}, p_3(x) = \frac{(x - t_1)(x - t_2)}{(t_3 - t_1)(t_3 - t_2)}$ is the basis to which $\mc{B}^\ast$ is dual.

        \smallskip
        Thus $\forall p \in V, p = p(t_1)p_1 + p(t_2)p_2 + p(t_3)p_3$.
    \end{example}
\end{frame}

\begin{frame}{.}
    \begin{definition}
        If $V$ is a vector space over the field $F$ and $S$ is a subset of $V$, the \tb{annihilator} of $S$ is the set $S^0$ of linear functionals $f$ on $V$ s.t. $f(\alpha) = 0$ for every $\alpha$ in $S$.
    \end{definition}
    \begin{itemize}
        \item It should be clear that $S^0$ is a subspace of $V^\ast$, whether $S$ is a subspace of $V$ or not.
        \item If $S$ is the set consisting of the zero vector alone, then $S^0 = V^\ast$.
        \item If $S =V$, then $S^0$ is the zero subspace of $V^\ast$.
    \end{itemize}

    \begin{theorem} \label{th:6}
        Let $V$ be a finite-dimensional vector space over the field $F$, and let $W$ be a subspace of $V$.
        Then
        \[
            \dim W + \dim W^0 = \dim V
        \]
    \end{theorem}

    \ti{Proof.}
    Let $k$ be the dimension of $W$ and $\{\alpha_1, \dots, \alpha_k\}$.
    We can extend toward basis of $V$, $\{\alpha_1, \dots, \alpha_n\}$.
    Then there exists unique dual basis $\mc{B}^\ast = \{f_1, \dots, f_n\}$ for $V^\ast$.

    \underline{Claim: $\{f_{k+1}, \dots, f_k\}$ is the basis of $W^0$}.
    Note that $i>k, j \leq k$, $f_{i}(\alpha_j) = 0$.
    For any $f \in W^0$, there exists some scalars $c_i$ such that $f(\alpha_j) = 0$ and $f(\alpha_i) = c_i$, where $i >k, j \leq k$.
    Then $f = c_{k+1} f_{k+1} + \cdots + c_n f_{n}$ because $(c_{k+1} f_{k+1} + \cdots +c_n f_n)(\alpha_j) = 0$ and $(c_{k+1} f_{k+1} + \cdots + c_n f_n)(\alpha_i) = c_i$.
    Thus, $\Span \{f_{k+1}, \dots, f_n\} = W^0$.

    $\therefore \dim W + \dim W^0 = k + (n-k) = n = \dim V$.
    $\qed$
\end{frame}

\begin{frame}{.}
    In a vector space of dimension $n$, a subspace of dimension $n-1$ is called a \tb{hyperspace}.
    Such spaces are sometimes called hyperplanes of subspaces of codimmension $1$.

    \begin{itemize}
        \item Note that this hyperspace is the null space (kernel) of some linear functionals $f \in V^\ast$.
        Let hyperspace $W \subset V$ has basis $\{\alpha_1, \dots, \alpha_{n-1}\}$ where basis of $V$ is $\{\alpha_1, \dots, \alpha_n\}$.
        Then for any $f \in V^\ast$ s.t. $f(\alpha_1) =0, \dots, f(\alpha_{n-1}) = 0, f(\alpha_n) \neq 0$, $\ker f = \Span \{\alpha_1, \dots, \alpha_{n-1}\}= W$.
    \end{itemize}

    \begin{corollary} \label{cor:1}
        If $W$ is a $k$-dimensional subspace of an $n$-dimensional vector space $V$, then $W$ is the intersection of $(n-k)$ hyperspaces in $V$.
    \end{corollary}

    \ti{Proof.}
    Let basis of $W$ be $\{\alpha_1, \dots, \alpha_k\}$ and basis of $V$ is $\{\alpha_1, \dots, \alpha_n\}$, which is extended basis of $W$.
    Let $\{f_1, \dots, f_n\}$ be the dual basis of $V$.
    By theorem \ref{th:6}, $\{f_{k+1}, \dots, f_{n}\}$ is the basis of $W^0$.

    Note that for each $f_i, \forall i>k$, $\ker f_i$ consists individual hyperspace $S_i$ of $V$.
    Basis of $S_i$ is $\{\alpha_1, \dots, \alpha_{i-1}, \alpha_{i+1}, \dots \alpha_n\}$.
    Then basis of $\bigcap_{i={k+1}}^n S_i$ is $\{\alpha_1, \dots, \alpha_k\}$
    So $\bigcap_{i={k+1}}^n S_i$ has same basis with $W$ and therefore $\bigcap_{i={k+1}^n} S_i = W$.
    $\qed$
\end{frame}

\begin{frame}{.}
    \begin{corollary}
        If $W_1$ and $W_2$ are subspaces of a finite-dimensional vector space, then $W_1 = W_2$ if and only if $W_1^0 = W_2^0$
    \end{corollary}
    \ti{Proof.} If $W_1 = W_2$ then $W_1^0 = W_2^0$ is obvious.
    If $W_1 \neq W_2$, suppose there exists $\alpha$ s.t. $\alpha \in W_1, \alpha \notin W_2$.
    By Corollary \ref{cor:1}, there exists hyperspace $S$ and functional $f$ s.t. 
    $f(\alpha) \neq 0$  and $\forall \beta \in S, f(\beta) = 0$.
    This $f \in W_2^0$ but $f \notin W_1^0$.
    Thus $W^0_1 \neq W^0_2$.
    $\qed$
\end{frame}

\begin{frame}{.}
    \begin{example}
        Here are three linear funtionals on $R^4$:

        \[
        \begin{aligned}
            &f_1 (x_1, x_2, x_3, x_4) = x_1 + 2x_2 + 2x_3 + x_4\\
            &f_2 (x_1, x_2, x_3, x_4) = 2x_2 + x_4\\
            &f_3 (x_1, x_2, x_3, x_4) = -2x_1 -4x_3 + 3x_4
        \end{aligned}
        \]

        The subspace $W$ which $f_1, f_2, f_3$ annihilate is same with solution space of $AX = \begin{bmatrix}
            1 & 2 & 2 & 1 \\ 0 & 2 & 0 & 1 \\ -2 &0 & -4 & 3
        \end{bmatrix} \begin{bmatrix}
        x_1 \\ x_2 \\ x_3
        \end{bmatrix} = 0$.

        We can get solution space easily with by calculate $R = \begin{bmatrix}
        1 & 0 & 2 &0 \\ 0 &1 & 0 & 0 \\ 0 & 0 & 0& 1
        \end{bmatrix}$, row-reduced echelon form of $A$.
        This implies three linear functionals
        \[
        \begin{aligned}
            &g_1(x_1, x_2, x_3, x_4) = x_1 + 2x_3 \\
            &g_2(x_1, x_2, x_3, x_4) = x_2 \\
            &g_3(x_1, x_2, x_3, x_4) = x_4
        \end{aligned}
        \]
        span the same subspace of $(R^4)^\ast$ and annihilate the same subspace of $R^4$.

        The subspace annihilated by both $\{f_1, f_2, f_3\}$ and $\{g_1, g_2, g_3\}$ consists of the vectors with $x_1 = -2 x_3, x_2=x_4 =0$.
    \end{example}
\end{frame}

\begin{frame}{.}
    \begin{block}{Exercise 3.5.4}
        Let $V$ be the vector space of all polynomial functions $p$ from $\mbb{R}$ into $\mbb{R}$ which have degree $2$ or less: $p(x)= c_0 + c_1 x + c_2 x^2$.
        Define three linear functionals on $V$ by $f_1(p) = \int_0^1 p(x) dx, f_2(p) = \int_0^2 p(x) dx, f_3(p) = \int_0^{-1} p(x) dx$.
        Show that $\{f_1, f_2, f_3\}$ is a basis for $V^\ast$ by exhibiting the basis for $V$ of which it is the dual.

        \smallskip
        Let $\{p_1, p_2, p_3\}$ be the ordered basis of $V$ and $\{f_1, f_2, f_3\}$ be the dual basis of $\{p_1, p_2, p_3\}$.
        Let $p_1(x) = c_0 + c_1 x + c_2 x^2$. Then $f_1(p_1) = c_0 + 1/2c_1 + 1/3 c_2 = 1,f_2(p_1) = 2c_0 + 2c_1 + 8/3 c_2 = 0 ,f_3(p_1) = -c_0 +1/2 c_1 - 1/3 c_2 = 0,  \implies c_0 = 1, c_1=1, c_2 = -3/2$.
        Thus $p_1(x) = 1 + x -3/2 x^2$.
        Similarly, $p_2(x) = -1/6 + 1/2x^2, p_3(x) = 1/3 -x + 1/2x^2$.
    \end{block}

    \begin{block}{Exercise 3.5.5}
        If $A,B$ are $n\times n $ complex matrices, show that $AB -BA = I$ is impossible.

        \smallskip
        Note that for matrix $A,B, \tr (A+B) = \tr(A) + \tr(B)$.
        $\tr(AB - BA) = \tr(AB) - \tr(AB) = 0 \neq n = \tr(I)$.
        $\qed$
    \end{block}
\end{frame}

\begin{frame}{.}
    \begin{block}{Exercise 3.5.6}
        Let $m$ and $n$ be positive integers and $F$ a field.
        Let $f_1, \dots, f_m$ be linear functionals on $F^n$.
        For $\alpha \in F^n$ define $T(\alpha) \coloneq (f_1(\alpha), \dots, f_m(\alpha))$.
        Show that $T$ is a linear transformations from $F^n$ into $F^m$.
        Then show that every linear transformation from $F^n$ into $F^m$ is of the above form, for some $f_1, \dots, f_m$.

        \smallskip
        For $\alpha_1, \alpha_2 \in F^n$ and $c \in F$, $T(c\alpha_1 + \alpha_2) = (f_1(c\alpha_1 + \alpha_2), \dots, f_m(c\alpha_1 + \alpha_2)) = c( f_1(\alpha_1), \dots, f_m(\alpha_1)) + (f_1(\alpha_2), \dots, f_m(\alpha_2)) = cT(\alpha_1) + T(\alpha_2)$.
        Thus $T \in L(F^n, F^m)$.

        Let $\{\epsilon_1, \dots, \epsilon_n\}$ be the ordered standard basis of $F^n$.
        For any $K \in L(F^n, F^m)$ and $1 \leq j \leq n$, there exists scalars $\beta_{ij} \in F, 1 \leq i \leq m$ s.t. $K(\epsilon_j) = (\beta_{1j}, \dots, \beta_{mj})$.
        Let $f_i(\epsilon_j) \coloneq \beta_{ij}$.
        For any $\alpha = c_1 \epsilon_1 + \cdots + c_n \epsilon_n \in F^n$, $K(\alpha) = c_1 K(\epsilon_1) + \cdots + c_n = (\sum_{j=1}^n c_j \beta_{1j}, \cdots, \sum_{j=1}^n c_j \beta_{mj}) = (f_1(\alpha), \cdots, f_m(\alpha))$.
        $\qed$
    \end{block}
\end{frame}

\begin{frame}{.}
    \begin{block}{Exercise 3.5.11}    
        Let $W_1$ and $W_2$ be subspaces of a finite-dimensional vector space $V$.
        \begin{itemize}
            \item Prove that $(W_1 + W_2)^0 = W_1^0 \cap W^0_2$
            \item Prove that $(W_1 \cap W_2)^0 = W_1^0 + W^0_2$
        \end{itemize}
        \textcolor{red}{to be continue...}
    \end{block}
    \begin{block}{Exercise 3.5.12}
        Let $V$ be a finite-dimensional vector space over the field $F$ and let $W$ be a subspace of $V$. If $f$ is a linear functional on $W$, prove that there is a linear functional $g$ on $V$ s.t. $g(\alpha) = f(\alpha)$ for each $\alpha$ in the subspace $W$.
    
        \textcolor{red}{to be continue...}
    \end{block}

    \begin{block}{Exercise 3.5.13}
        Let $F$ be a subfield of the field of complex numbers and let $V$ be any vector space over $F$.
        Suppose that $f$ and $g$ are linear functionals on $V$ such that the function $h$ defined by $h(\alpha) = f(\alpha) g(\alpha)$ is also linear functional on $V$.
        Prove that either $f=0$ or $g=0$.

        \textcolor{red}{to be continue...}
    \end{block}
\end{frame}

\begin{frame}{Exercise 3.5.14}
    \begin{block}{Exercise 3.5.14}
        Let $F$ be a field of characteristic zero and let $V$ be a finite-dimensional vector space over $F$.
        If $\alpha_1, \dots, \alpha_m$ are finitely many vectors in $V$, each different from the zero vector, prove that there is a linear functional $f$ on $V$ such that $f(\alpha_i) \neq 0, i=1, \dots, m$.

        \textcolor{red}{to be continue...}
    \end{block}

    \begin{block}{Exercise 3.5.15}
        It is true that similar matrices have the same trace.
        Thus we can define the trace of a linear operator on a finite-dimensional space to be the trace of any matrix which represents the operator in an ordered basis.
        This is well-defined since all such representing matrices for one operator are similar.

        Now let $V$ the space of all $2\times 2$ matrices over the field $F$ and let $P$ be a fixed $2 \times 2$ matrix.
        Let $T$ be the linear operator on $V$ defined by $T(A) =PA$.
        Prove that $\tr(T) = 2 \tr(P)$.

        \smallskip
        Let call $\epsilon_{11} = \begin{bmatrix}
        1 & 0 \\ 0 & 0
        \end{bmatrix}, \epsilon_{12} = \begin{bmatrix}
        0 & 1 \\ 0 & 0
        \end{bmatrix}, \epsilon_{21} = \begin{bmatrix}
        0 & 0 \\ 1 & 0
        \end{bmatrix}, \epsilon_{22} = \begin{bmatrix}
        0 & 0 \\ 0 & 1
        \end{bmatrix}$ as standard basis of $F^{2 \times 2}$.
        $\mc{B} = \{\epsilon_{11}, \epsilon_{21}, \epsilon_{12}, \epsilon_{22}\}$ is ordered basis of $V$.
        Then $[T]_{\mc{B}} = \begin{bmatrix}
        P_{11} & P_{12} & 0 & 0 \\P_{21} & P_{22} & 0 & 0 \\ 0 & 0 & P_{11} & P_{12} \\ 0 & 0 & P_{21} & P_{22}
        \end{bmatrix}$.
        Thus $\tr (T) = \tr ([T]_{\mc{B}}) = 2(P_{11} + P_{22}) = 2\tr(P)$.
        $\qed$
    \end{block}
\end{frame}

\begin{frame}{.}

    \begin{block}{Exercise 3.5.16}    
        Show that the trace functional on $n \times n$ matrices is unique in the following sense.
        If $W$ is the space of $n \times n$ matrices over the field $F$ and if $f$ is a linear functional on $W$ such that $f(AB) = f(BA)$ for each $A$ and $B$ in $W$, then $f$ is a scalar multiple of the trace function.
        If, in addition, $f(I) = n$, then $f$ is the trace function.

        \smallskip
        $\mc{B} = \{\epsilon_{11}, \dots, \epsilon_{n1}, \epsilon_{21}, \dots, \epsilon_{n2}, \dots, \epsilon_{nn}\}$ is ordered basis of $W$ where $\epsilon_{ij}$ is the $ij$-th standard basis of $F^{n \times n}$.
        Let $\{f_{11}, \dots, f_{nn}\}$ be the dual basis of $\mc{B}$.
        Note that $\forall A \in W, f_{ij}(A) = A_{ij}$.
        For any linear functional $f \in W^\ast$, $f$ can be expressed by some scalars $c_{ij}$ s.t. $f = c_{11}f_{11} + \cdots +c_{nn} f_{nn}$.
        % Then $f(AB) = c_{11} (\sum_{k=1}^n A_{1k} B_{k1}) + \cdots + c_{nn} (\sum_{k=1}^n A_{nk} B_{kn}) = f(BA) = c_{11} (\sum_{k=1}^n B_{1k}A_{k1}) + \cdots + c_{nn}( \sum_{k=1}^n B_{nk}A_{kn})$.
        Then $f(AB) = c_{11} f_{11}(AB) + \cdots + c_{nn} f_{nn}(AB) = f(BA) = c_{11} f_{11}(BA) + \cdots + c_{nn} f_{nn}(BA)$.
        This implies $c_{11} f_{11} (AB - BA) + \cdots + c_{nn} f_{nn} (AB-BA) = 0$.
        For $i, j$ s.t. $i \neq j$, $(AB)_{ij} \neq (BA)_{ij}$ generally, thus $c_{ij} = 0$.

        Therefore $f = c_{11}f_{11} + c_{22}f_{22} + \cdots + c_{nn} f_{nn}$.


        \textcolor{red}{...}
    \end{block}

    \begin{block}{Exercise 3.5.17}
        Let $W$ be the space of $n \times n$ matrices over the field $F$, and let $W_0$ be the subspace spanned by the matrices $C$ of the form $C = AB - BA$.
        Prove that $W_0$ is exactly the subspace of matrices which have trace zero. 
    \end{block}
\end{frame}

\subsection{The Double Dual}

\begingroup
    \setbeamertemplate{frametitle}{%
    \vskip1ex
    \usebeamerfont{frametitle}%
    \insertframetitle\par        %  ← 원하는 대로 변경 가능
    \vskip1ex
    \hrule                             % 밑줄(선택)
    }
    \begin{frame}
        \frametitle{Table of Contents}
        \tableofcontents[currentsubsection]
    \end{frame}
\endgroup

\begin{frame}{.}
    Let $V$ be the vector space.
    The double dual is the vector space that is dual space of $V^\ast$.
    \[
    V^{\ast \ast} \coloneq L(L(V, F), F)
    \]

    Let $\alpha \in V$.
    Define $L_{\alpha}$ as $L_{\alpha}(f) \coloneq f(\alpha)$ where $\forall f \in V^\ast$.
    Is this $L_{\alpha}$ linear transformation?

    \[
        \begin{aligned}
            L_{\alpha} (cf + g) = (cf+g)(\alpha) = cf(\alpha) + g(\alpha) = c L_{\alpha}(f) + L_{\alpha}(g)
        \end{aligned}
    \]
    This implies $L_{\alpha}$ is linear transformation and $L_{\alpha} \in V^{\ast\ast}$.
    Note that if $V$ is finite-dimensional and $\alpha \neq 0$, then $L_{\alpha} \neq 0$.

    \begin{theorem} \label{th:7}
        Let $V$ be a finite-dimensional vector space over the field $F$.
        For each vector $\alpha \in V$ define $L_{\alpha} (f) =f(\alpha), f \in V^\ast$.
        The mapping $\Phi : V \to V^{\ast\ast}$ is then an isomorphism of $V$ onto $V^{\ast\ast}$.
    \end{theorem}
    \ti{proof.}

    \begin{itemize}
        \item (Linearity):
        Let $\alpha, \beta \in V$, $c \in F$, and $\gamma = c\alpha + \beta$.
        $\forall f \in V^\ast, \Phi(c \alpha + \beta)(f) = L_{c \alpha + \beta}(f) = f(c \alpha + \beta) = cf(\alpha) + f(\beta) = c L_{\alpha}(f) + L_{\beta}(f) = c\Phi(\alpha)(f) + \Phi(\beta)(f)$.
        Thus, $\Phi$ is linear map.
        \item (Invertible): For $\alpha \in V$, Let assume $\Phi(\alpha) = 0$.
        Then $L_{\alpha} = 0$ and this implies $\alpha = 0$.
        Thus, $\ker \Phi =\{0\}$ and $\Phi$ is invertible.
    \end{itemize}
\end{frame}

\begin{frame}{.}
    \begin{corollary}
        Let $V$ be a finite-dimensional vector space over the field $F$.
        If $L$ is a linear functional on the dual space $V^\ast$ of $V$, then there is a unique vector $\alpha$ in $V$ s.t.
        \[
            L(f) = f(\alpha)
        \]
        for every $f \in V^\ast$.
    \end{corollary}
    \ti{Proof.}
    Skip.
    \begin{corollary}
        Let $V$ be a finite-dimensional vector space over the field $F$.
        Each basis for $V^\ast$ is the dual of some basis for $V$.
    \end{corollary}
    \ti{Proof.}
    Let $\mc{B}^\ast = \{f_1, \dots, f_n\}$ be a basis for $V^\ast$.
    Then there is a basis $\{L_1, \dots, L_n\}$ for $V^{\ast\ast}$ s.t. $L_{i} (f_j) = \delta_{ij}$.
    By Theorem \ref{th:7}, there exists $\alpha_i$ s.t. $L_i(f) = f(\alpha_i)$ for every $f$ in $V^\ast$.
    Then $L_{i}(f_j) = f_j(\alpha_i) = \delta_{ij}$.
    Since $\Phi: V \to V^{\ast\ast}$ is isomorphism, by Theorem \ref{th:3}, $\{\alpha_1, \dots, \alpha_n\}$ is basis for $V$.
    $f_j(\alpha_i) = \delta_{ij}$ implies that $\{f_1, \dots, f_n\}$ is dual basis for $\{\alpha_1, \dots, \alpha_n\}$.
\end{frame}

\begin{frame}{.}
    \begin{definition}
        Let $V$ be the vector space. If subspace $N$ of $V$ satisfies under conditions, then $N$ is called by \tb{maximal proper subspace}.
        \begin{itemize}
            \item $N \subsetneq V$
            \item For another subspace $W$, $N \subseteq W \subseteq V \implies W=N$ or $W =V$
        \end{itemize}

        A \tb{hyperspace} in $V$ is a maximal proper subspace of $V$.
    \end{definition}
\end{frame}

\begin{frame}{.}
\begin{theorem}
        If $f$ is a non-zero linear functional on the vector space $V$, then the null space of $f$ is a hyperspace in $V$.
        Conversely, every hyperspace in $V$ is the null space of a (not unique) non-zero linear functional on $V$.
    \end{theorem}
    \ti{Proof.}
    Let $N_f$ be the kernel space of $f$ and $\alpha$ be a vector in $V$ s.t. $f(\alpha) \neq 0$, i.e., $\alpha \notin N_f$.
    Let $\beta \in V$.
    \underline{Claim: $\beta \in \Span N_f \cup \{\alpha\}$.}
    If $f(\beta) = 0$, then $\beta \in N_f$.
    So $\beta \in \Span N_f \cup \{\alpha\}$.
    If $f(\beta) \neq 0$, then $f(\beta) = cf(\alpha)$ where $c \in F$.
    Then $f(\beta - c \alpha) = 0$ and $\beta - c \alpha \in N_f$.
    This implies there exists $\gamma \in N_f$ s.t. $
    \beta - c \alpha = \gamma$ and $\beta \in \Span N_f \cup \{\alpha\}$.
    So, $V \subseteq \Span N_f \cup \{\alpha\} \subseteq V \implies V = \Span N_f \cup \{\alpha\}$ and $N_f$ is hyperspace of $V$.

    Let $N$ be any hyperspace in $V$.
    Since $N$ is proper subset of $V$, $\exists \alpha \notin N$. And $\Span N \cup \{\alpha\} = V$ because $N$ is maximal proper subspace.
    Then for any $\beta \in V$, $\beta = \gamma + c \alpha$. This $\beta$ is uniquely determined by $\gamma$ and $c$.
    To show that $\gamma$ and $c$ is unique, assume there exists another $\gamma^\prime$ and $c^\prime$ s.t. $\beta = \gamma^\prime + c^\prime \alpha$.
    Then $\beta = \gamma + c \alpha = \gamma^\prime c^\prime \alpha \implies (c - c^\prime) \alpha = (\gamma - \gamma^\prime)$. If $(c-c^\prime) \neq 0$, then this implies $\alpha \in N$ (Contradict!).
    Thus, $c = c^\prime, \gamma = \gamma^\prime$.
    So, for every $\beta \in V$, there exists unique scalar $c$ corresponding to $\beta$ and let define linear functional $g \in L(V, F)$ s.t. $g(\beta) = c$.
    Then $N = \ker g$.
    $\qed$
\end{frame}

\begin{frame}{.}
    \begin{lemma}
        Let $f$ and $g$ are linear functionals on a vector space $V$.
        $g$ is a scalar multiple of $f$ $\iff$ $f(\alpha) = 0 \implies g(\alpha) = 0$ (i.e., $\ker f \subseteq \ker g$).
    \end{lemma}
    \ti{Proof.}
    ($\implies$:) If $f = 0$ then $g = 0$ and for any $c \in F$, $g = c f$.
    Else if $f \neq 0$, $\exists c$ s.t. $g(\alpha) = c f(\alpha)$ for all $\alpha \in V$.
    For $\alpha \in \ker f$, $g(\alpha) = c f(\alpha) = 0$.
    Thus, $\ker f  \subseteq \ker g$.
    For $\alpha \notin \ker f$, $g(\alpha) = cf(\alpha)$.

    ($\impliedby$:)
    If $f = 0$ then for any $\alpha \in V$, $f(\alpha) = 0$ and $g(\alpha) = 0$.
    Else if $f \neq 0$. Then $\ker f$ is hyperplane and there exist some $\alpha$ s.t. $\alpha \notin \ker f, \alpha \in V$ and $\Span \ker f \cup \{\alpha\} = V$.
    Let define $c$ by $c = g(\alpha)/ f(\alpha)$.
    Then $cf(\alpha) = g(\alpha)$ holds.
    So, $\forall \alpha \in V$, $g$ is multiple of $f$.
    Also, $\forall \gamma \in \ker f$, $c f(\gamma) = 0 = g(\gamma)$ holds.
    So, $g$ is multiple of $f$ for all element of $V$.
    $\qed$
\end{frame}

\begin{frame}{.}
    \begin{theorem}
        Let $g, f_1, \dots, f_r$ be linear functionals on a vector space $V$ with respective null spaces $N, N_1, \dots, N_r$.
        Then $g$ is a linear combination of $f_1, \dots, f_r$ if and only if $N$ contains the intersection $N_1 \cap \cdots \cap N_r$.
    \end{theorem}
    ($\implies$:) There exists scalars $c_1, \dots, c_r$ s.t. $g = c_1 f_1 + \cdots + c_r f_r$.
    For any vecor $\alpha \in N_1 \cap \cdots \cap N_r$, $g(\alpha) = c_1f_1(\alpha)+ \cdots + c_r f_r(\alpha) = 0$.
    Thus, $N_1 \cap \cdots \cap N_r \subseteq N$.

    ($\impliedby$:) For any vector $\alpha \in N_1 \cap \cdots \cap N_r$, $\alpha \in N$.
    So, $f_1(\alpha) = 0, \dots, f_r(\alpha)=0$ and $g(\alpha) = 0$.
    The $r=1$ case can be proved by previous Lemma.
    Suppose $r=k-1$ case holds.
    Let $\alpha \in N_1 \cap \cdots \cap N_{k}$.
    Then $g = c_1 f_1 + \cdots + c_{k-1} f_{k-1}$ and $g(\alpha) = c_1 f_1(\alpha) + \cdots + c_{k-1} f_{k-1}(\alpha) = 0$.
    For fixed $\beta$ s.t.  $\beta \notin N_1 \cap \cdots \cap N_k$ and $\beta \in N_1 \cap \cdots \cap N_{k-1}$, Let consider another linear functional $g^\prime$ s.t. $g^\prime(\beta) = c_k f_k(\beta)$ for a scalar $c_k$ and $g^\prime(\alpha) = 0$.
    Claim : $g^\prime - c_k f_k = g$.

    \textcolor{red}{To be continue..}
\end{frame}

\subsection{The Transpose of a Linear Transformation}

\begingroup
    \setbeamertemplate{frametitle}{%
    \vskip1ex
    \usebeamerfont{frametitle}%
    \insertframetitle\par        %  ← 원하는 대로 변경 가능
    \vskip1ex
    \hrule                             % 밑줄(선택)
    }
    \begin{frame}
        \frametitle{Table of Contents}
        \tableofcontents[currentsubsection]
    \end{frame}
\endgroup

\begin{frame}{.}
    \begin{theorem}
        Let $V$ and $W$ be vector spaces over the field $F$.
        For each linear transformation $T$ from $V$ into $W$, there is a unique linear transformation $T^\top$ from $W^\ast$ into $V^\ast$ such that $(T^\top g) (\alpha) = g(T (\alpha))$ for every $g \in W^\ast$ and $\alpha \in V$.
    \end{theorem}
    \ti{Proof.}
    First, $T^\top$ is linear transformation.
    For $g_1, g_2 \in L(W, F), c \in F, \alpha \in V$, $T^\top(cg_1 + g_2)(\alpha)=(cg_1 + g_2)(T(\alpha)) = cg_1(T(\alpha)) + g_2 (T(\alpha)) = c(T^\top g_1)(\alpha) + (T^\top g_2) (\alpha)$.
    So $T^\top (cg_1 + g_2) = c T^\top (g_1) + T^\top (g_2)$ and $T^\top$ is linear transformation.

    Second, $T^\top$ is unique for $T$. Let assume there exists another $U \in L(W^\ast, V^\ast)$ such that $(Ug)(\alpha) = g(T(\alpha))$ for any $g \in L(W^\ast, V^\ast), \alpha \in V$.
    Then $(Ug)(\alpha) = (T^\top g)(\alpha) =g(T(\alpha)) \implies (Ug -T^\top g)(\alpha) = 0 \implies (U-T^\top)(g) = 0 \implies U = T^\top$.
    $\qed$

    \bigskip
    We shall call $T^\top$ the \tb{transpose} of $T$. This transformation $T^\top$ is often called the adjoint of $T$.
    However, we shell not use this terminology.
\end{frame}

\begin{frame}{.}
    \begin{theorem}
        Let $V$ and $W$ be vector spaces over the field $F$, and let $T$ be a linear transformation from $V$ into $W$.
        The kernel (null space) of $T^\top$ is the annihilator of the range of $T$.
        If $V$ and $W$ are finite-dimensional, then
        \begin{enumerate}
            \item $\dim \im (T^\top) = \dim \im T$
            \item $\im T^\top$ is the annihilator of the kernel of $T$.
        \end{enumerate}
    \end{theorem}
    \ti{Proof.}
    For any $g \in W^\ast, \alpha \in V$, $(T^\top g)(\alpha) = g(T(\alpha))$.
    $\ker T^\top = \{g | (T^\top g)(\alpha) = 0, \forall \alpha \in V\}$.
    $(\im T)^0 = \{g | g(T(\alpha)) = 0 ,\forall \alpha \in V\}$
    Thus, \underline{$\ker T^\top = (\im T)^0$}.

    (1.) Let $\dim V = n$ and $\dim W = m$.
    Let $\dim \im T = r$.
    Then the $\dim (\im T)^0 = \dim \ker T^\top =  m-r$.
    $T^\top$ is linear transformation from $W^\ast$ to $V^\ast$.
    So $\dim W^\ast = \dim \ker T^\top + \dim \im T^\top \implies m = (m-r) + \dim \im T^\top \implies \dim \im T^\top = r$.
    
    (2.) Let $f = T^\top g$. For any $\alpha \in \ker T$, $f(\alpha) = (T^\top g)(\alpha) = g(T(\alpha)) = g(0) = 0$.
    So $f$ is annihilator for $\ker T$.
    Thus, for any $f = T^\top g$, $f$ is annihilator of $\ker T$ and this implies $\im T^\top \subseteq (\ker T)^0$.
    And $\dim \ker T = (n-r) \implies \dim (\ker T)^0 = r = \dim \im T^\top$.

    $\therefore \im T^\top = (\ker T)^0$.
    $\qed$
\end{frame}

\begin{frame}{.}
    \begin{theorem}
        Let $V$ and $W$ be finite-dimensional vector spaces over the field $F$.
        Let $\mc{B}$ be an ordered basis for $V$ with dual basis $\mc{B}^\ast$, and let $\mc{C}$ be an ordered basis for $W$ with dual basis $\mc{C}^\ast$.
        Let $T$ be a linear transformation from $V$ into $W$.
        Let $A$ be the matrix of $T$ relative to $\mc{B}$, $\mc{C}$ and let $B$ be the matrix of $T^\top$ relative to $\mc{C}^\top$, $\mc{B}^\ast$.
        Then $B_{i,j} = A_{j,i}$.
    \end{theorem}

    \ti{Proof.}
    Let $\mc{B} = \{\alpha_1, \dots, \alpha_n\}$, $\mc{C} = \{\beta_1. \dots, \beta_m\}$, $\mc{B}^\ast=\{f_1, \dots, f_n\}, \mc{C}^\ast = \{g_1, \dots, g_m\}$.
    By definition, $T(\alpha_j) = \sum_{i=1}^m A_{i, j} \beta_i$, $T^\top (g_j) = \sum_{i=1}^n B_{i,j} f_i$.

    Then $T^\top (g_j)(\alpha_i) = g_j (T(\alpha_i)) = g_j (\sum_{k=1}^m A_{ki}\beta_k) = \sum_{k=1}^m A_{ki} g_j(\beta_k) = A_{ji}$.

    For any linear functional $f$, $f = \sum_{i=1}^n f(\alpha_i) f_i$.
    Let $f = T^\top g_j$ then it becomes $T^\top g_j = \sum_{i=1}^m A_{j,i} f_i$.
    But $T^\top g_j = \sum_{i=1}^n B_{i,j} f_i$ by definition.

    $\therefore A_{j,i} = B_{i,j}$.
    $\qed$

    \begin{definition}
        If $A$ is an $m \times n$ matrix over the field $F$, the \tb{transpose} of $A$ is the $n \times m$ matrix $A^\top$ defined by $A^\top_{i,j} = A_{j,i}$.
    \end{definition}
\end{frame}

\begin{frame}{.}
    \begin{theorem}
        Let $A$ be any $m\times n$ matrix over the field $F$.
        Then the row rank of $A$ is equal to the column rank of $A$.
    \end{theorem}
    \ti{Proof.}
    Let $\mc{B}$ be the standard ordered basis for $F^n$ and $\mc{B}^\prime$ the standard ordered basis for $F^m$.
    Let $T$ be the linear transformation from $F^n$ into $F^m$ such that the matrix of $T$ relative to the pair $\mc{B},\mc{B}^\prime$ is $A$.

    For any $x=(x_1, \dots, x_n) \in F^n, y = (y_1, \dots, y_m) \in F^m$, $y_i = \sum_{j=1}^m A_{ij} x_j$.
    So, $col A$ = $\dim \im T$.
    Similarly, $T^\top$ has matrix to the dual basis $\mc{B}^{\prime \ast}$ and $\mc{B}^\ast$, $A^\top$, and this matrix $A^\top$ has column rank same with $\dim \im T^\top = \dim \im T = col A$.
    $\therefore col A^\top = row A^\top = col A$.

    \begin{definition}
        If $A$ is $m \times n$ matrix over $F$ and $T$ is the linear transformation from $F^n$ into $F^m$ such that $T(x) = y, x\in F^n, y \in F^m$, then
        $\dim \im T = row A = col A$ and we call this number simply the \tb{rank} of $A$.
    \end{definition}
\end{frame}

\begin{frame}{.}
    \begin{example}    
        Let $V$ be an $n$-dimensional vector space over the field $F$, and let $T$ be a linear operator on $V$.
        Suppose $\mc{B} = \{\alpha_1, \dots, \alpha_n\}$ is an ordered basis for $V$.
        The matrix of $T$ in the ordered basis $\mc{B}$ is defined to be the $n \times n$ matrix $A$ such that $T(\alpha_j) = \sum_{j=1}^n A_{ij} \alpha_i$.
        If $\{f_1, \dots, f_n\}$ is the dual basis of $\mc{B}$, this can be stated simply $A_{ij}=f_i (T(\alpha_j))$.

        \smallskip
        Let us see what happens when we change basis.
        Suppose $\mc{B}^\prime = \{\alpha^\prime_1, \dots, \alpha^\prime_n\}$ is another ordered basis for $V$, with dual basis $\{f_1^\prime, \dots, f^\prime_n\}$.
        If $B$ is the matrix of $T$ in the ordered basis $\mc{B}^\prime$, then $T(\alpha_j^\prime) = \sum_{j=1}^n B_{ij} \alpha_i$ and $B_{ij} = f^\prime_i (T(\alpha_j^\prime))$.

        \smallskip
        Let $U$ be the invertible linear operator such that $U(\alpha_j)= \alpha_j^\prime$.
        Then the transpose of $U$ is given by $U^\top f^\prime_i = f_i$.
        It is easy to verify that since $U$ is invertible, so is $U^\top$ and $(U^\top)^{-1}$.
        Thus $f^\prime_i = (U^{-1})^\top f_i, i=1, \dots, n$.
        $\therefore B_{ij} = [(U)^{-1}f_i](T(\alpha_j^\prime)) = f_i(U^\{-1\}(T(\alpha^\prime_j))) = f_i(U^{-1} T U(\alpha_j))$.

        \smallskip
        Now what does this say? $f_i((U^{-1}T U)(\alpha_j))$ is the $i,j$ entry of the matrix of $U^{-1}TU$ in the ordered basis $\mc{B}$.
        Our computation above shows that this scalar is also the $i,j$ entry of the matrix of $T$ in the ordered basis $\mc{B}^\prime$.
        In other words, $[T]_{\mc{B}^\prime} = [U^{-1} T U]_{\mc{B}} = [U^{-1}]_{\mc{B}} [T]_{\mc{B}} [U]_{\mc{B}} = [U]_{\mc{B}}^{-1} [T]_{\mc{B}} [U]_{\mc{B}}$ and this is precisely the change-of-basis formula which we derived earlier.
    \end{example}
\end{frame}

\begin{frame}{.}
    \begin{block}{Exercise 3.7.3}
        Let $V$ be the space of all $n \times n$ matrices over the field $F$ and let $B$ be a fixed $n \times n$ matrix.
        If $T$ is the linear operator on $V$ defined by $T(A) = AB - BA$, and if $f$ is the trace function, what is $T^\top f$?

        \smallskip
        By definition, for any $A \in F^{n \times n}$, $T^\top f (A) = f(T(A)) = f(AB-BA) = \tr(AB) -\tr(BA) = 0$.
        So, $T^\top f = 0$
    \end{block}

    \begin{block}{Exercise 3.7.4}
        Let $V$ be a finite-dimensional vector space over the field $F$ and let $T$ be a linear operator on $V$.
        Let $c$ be a scalar and suppose there is a non-zero vector $\alpha$ in $V$ such that $T(\alpha) = c\alpha$.
        Prove that there is non-zero linear functional $f$ on $V$ such that $T^\top f = cf$.

        \smallskip
        Let $f \in V^\ast$ and $f \neq 0$. Then for any $\alpha \in V$, $T^\top f(\alpha) = f(T(\alpha)) = f(c\alpha) = cf(\alpha)$.
        $\qed$
    \end{block}
\end{frame}

\begin{frame}{.}
    \begin{block}{Exercise 3.7.5}
        Let $A$ be an $m \times n$ matrix with \ti{real} entries.
        Prove that $A =0 \iff \tr(A^\top A) = 0$.
        ($\implies$:) Trivial.

        ($\impliedby$:) $\sum_{k=1}^m A_{ki} A_{ki} = A_{ki}^2 = 0$ where $i=1, \dots, m$ and this implies $A_{ki} = 0$.
        So $A =0$.
    \end{block}

    \begin{block}{Exercise 3.7.7}
        Let $V$ be finite-dimensional vector space over the field $F$.
        Show that $\Phi(T) = T^\top$ is an isomorphism of $L(V,V)$ onto $L(V^\ast, V^\ast)$.

        \smallskip
        (Linearity:) Let $T,S \in L(V,V)$ and $c \in F$.
        $\Phi(cT +S) = (cT + S)^\top$.
        For any $\alpha \in V$ and $f \in V^\ast$, $(cT + S)^\top (f) (\alpha) = f((cT + S)(\alpha)) = f(cT(\alpha) + S(\alpha)) = cf(T(\alpha)) + f(S(\alpha)) = c(T^\top f) (\alpha) + (S^\top f)(\alpha)$.
        This implies $(cT + S)^{\top} = cT^\top + S^\top$ and $\Phi(cT + S) = (cT + S)^\top = c T^\top + S^\top = c\Phi(T) + \Phi(S)$.

        \smallskip
        (Invertibility:) Let $\Phi(T) = T^\top = 0$. Then for any $f \in V^\ast$ and $\alpha \in V$, $(T^\top f) (\alpha) = f(T(\alpha)) =0$.
        This implies $T = 0$ and $\ker \Phi = \{0\}$.
        So, $\Phi$ is bijective and invertible.

        \smallskip
        $\therefore \Phi$ is an isomorphism.
    \end{block}
\end{frame}

\begin{frame}{.}

    \begin{block}{Exercise 3.7.8 $\star$} 
        Let $V$ be the vector space of $n \times n$ matrices over the field $F$.
        \begin{enumerate}
            \item If $B$ is a fixed $n\times n$ matrix, define a function $f_B$ on $V$ by $f_B(A) = \tr(B^\top A)$.
            Show that $f_B$ is a linear functional on $V$.
            \item Show that every linear functional on $V$ is of the above form, i.e., is $f_B$ for some $B$.
            \item Show that $\Phi(B) = f_B$ is an isomorphism of $V$ onto $V^\ast$.
        \end{enumerate}

        (1.) It is trivial that $f_B: V \to F$.
        So, if $f_B$ is linear then $f_B$ is linear functional.
        For any $A^1, A^2 \in V$ and $c \in F$, $f_B(cA +B) = \tr(B^\top(cA^1+A^2))$.

        The $i$-th element of $f_B(cA^1+A^2)$ is $\sum_{i=1}^n \sum_{k=1}^n B_{ki} (cA^1_{ki} + A^2_{ki}) = c\sum_{i=1}^n\sum_{k=1}^n B_{ki}A^1_{ki} + \sum_{i=1}^n\sum_{k=1}^n B_{ki} A^2_{ki} = c\tr(B^\top A^1)+ \tr(B^\top A^2) = cf_B(A^1) +f_B(A^2)$.
        Therefore, $f_B$ is linear functional on $V$.

        \smallskip
        (2.) Let $\epsilon_{ij}$ be the standard basis of $F^{n\times n}$.
        Note that for $B \in F^{n\times n}$, $\tr(B^\top \epsilon_{ij}) = B_{i,j}$.
        For a $f \in V$, $\exists B_{i,j} \in F$ such that $f(\epsilon_{ij}) = B_{i,j}$.
        Then $f(\epsilon_{ij}) = B_{i,j} =Tr(B^\top \epsilon_{ij})$ holds.
        This implies for any $f \in V^\ast$, $\exists B \in F^{n \times n}$ s.t. $f(A) = \tr(B^\top A)$ for any $A \in F^{n \times n}$.
    \end{block}
\end{frame}

\begin{frame}{.}
    \smallskip
    (3. Linearity) For any $A^1, A^2, B \in V$, $\tr(B^\top(cA^1 + A^2)) = \sum_{i=1}^n \sum_{k=1}^n B_{ki} (cA^1_{ki} A^2_{ki}) = c\sum_{i=1}^n \sum_{k=1}^n B_{ki} A^1_{ki} + \sum_{i=1}^n \sum_{k=1}^n B_{ki} A^2_{ki} = c\tr(B^\top A^1) + \tr(B^\top A^2)$.
    This implies $\Phi(cA^1 + A^2) = \tr(B^\top(cA^1 + A^2)) = c\tr(B^\top A^1) = \tr(A^2) = c \Phi(A^1) + \Phi(A^2)$ and $\Phi$ is linear.

    (3. Invertibility) Let $\Phi(B) = f_B =0$.
    Then for any $A \in V$, $f_B(A) = \tr(B^\top A) = 0 = \sum_{i=1}^n \sum_{k=1}^n B_{ki} A_{ki} = 0$.
    Let $A = e_{ki}$ then $B = 0$.
    So, $\ker \Phi = \{0\}$ and $\Phi$ is isomorphism.

\end{frame}

\end{document}