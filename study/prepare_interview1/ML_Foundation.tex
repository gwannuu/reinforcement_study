\documentclass[8pt]{beamer}
\usefonttheme[onlymath]{serif}


\setbeamertemplate{frametitle}{%
  \vskip1ex
  \usebeamerfont{frametitle}%
  \insertsubsectionhead\par        %  ← 원하는 대로 변경 가능
  \vskip1ex
  \hrule                             % 밑줄(선택)
}

% 테마 선택 (선택 사항)
% \usetheme{Madrid} % 기본 테마, 다른 테마 사용 가능
% \font{serif}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage[T1]{fontenc} % To use combination of textbf, textit
\usepackage[dvipsnames]{xcolor}   % can use more variant colors

% \setcounter{MaxMatrixCols}{20}

% (필요한 패키지들)
% \usepackage{amsthm}
\setbeamertemplate{theorems}[numbered]  % 정리, 정의 등에 번호를 달아줌

% \theoremstyle{plain} % insert bellow all blocks you want in italic
% \newtheorem{theorem}{Theorem}[section] % to number according to section
% 
% \theoremstyle{definition} % insert bellow all blocks you want in normal text
% \newtheorem{definition}{Definition}[section] % to number according to section
% \newtheorem*{idea}{Proof idea} % no numbered block

\newtheorem{proposition}[theorem]{Proposition}

\usepackage{tcolorbox}

% 필요할 경우 패키지 추가
\usepackage{graphicx} % 이미지 삽입을 위한 패키지
\usepackage{amsmath}   % 수식 사용
\usepackage{hyperref}  % 하이퍼링크 추가
\usepackage{cleveref}
\usepackage{multicol}  % 여러 열 나누기
\usepackage{ulem} % 취소선 및줄 나누기



\newcommand{\mrm}[1]{\mathrm{#1}}
\newcommand{\mbb}[1]{\mathbb{#1}}
\newcommand{\mb}[1]{\mathbf{#1}}
\newcommand{\mc}[1]{\mathcal{#1}}
\newcommand{\tb}[1]{\textbf{#1}}
\newcommand{\ti}[1]{\textit{#1}}
\newcommand{\mypois}[1]{\operatorname{Pois}(#1)}

\newcommand{\myber}[1]{\operatorname{Bern}\!\left(#1\right)}
\newcommand{\mybin}[2]{\operatorname{Bin}\!\left(#1,#2\right)}
\newcommand{\mytoinf}[1]{#1 \rightarrow \infty}
\newcommand{\myexp}[1]{\exp{\left(#1\right)}}
\newcommand{\myunif}[2]{\operatorname{Unif}\!\left(#1, #2\right)}
\newcommand{\mygeom}[1]{\operatorname{Geom}\!\left(#1\right)}
\newcommand{\myexpo}[1]{\operatorname{Expo}\!\left(#1\right)}
\newcommand{\abs}[1]{\left\lvert #1 \right\rvert}
\newcommand{\expec}[1]{\operatorname{E}\left[ #1 \right]}
\newcommand{\myvar}[1]{\operatorname{Var}\left[#1\right]}
\newcommand{\myskew}[1]{\operatorname{Skew}\!\left[#1\right]}
\newcommand{\mykurt}[1]{\operatorname{Kurt}\!\left[#1\right]}
\newcommand{\mywei}[2]{\operatorname{Wei}\!\left(#1, #2\right)}
\newcommand{\Span}[1]{\operatorname{Span}\!\left(#1\right)}
\newcommand{\argmax}[1]{\operatorname{arg max}_{#1}}
\newcommand{\argmin}[1]{\operatorname{arg min}_{#1}}
\newcommand{\nll}[1]{\operatorname{NLL}\!\left(#1\right)}
\newcommand{\rss}[1]{\operatorname{RSS}\!\left(#1\right)}

% 발표 제목, 저자, 날짜 설정
\title{ML Foundation}
\author{Gwanwoo Choi}
% \date{}

\begin{document}
% 표지 슬라이드
\begin{frame}
    \titlepage
\end{frame}

\subsection{Linear Regression}


\begin{frame}{.}
  Suppose we had collected $n$ data points $\{(x_i, y_i) \mid 1\leq i \leq n, x_i \in \mbb{R}^d, y\in \mbb{R}\}$ and elements of $(x_i, y_i)$ seems like having linear relationship with each other. How can we estimate underlying linear relationship?

  \bigskip

  Let assume $\mbb{E}[y|x] = w^\top x + \beta$ where $w \in \mbb{R}^{d}, \beta \in \mbb{R}$ and $y_i = w^\top x_i + \beta + \epsilon_i$ where $\epsilon_i \sim \mathcal{N}(0, \sigma^2)$, in which assume there exists some observation noise for observing $y_i$. Our \tb{goal} is to \tb{find best paramters} $w, \beta$ that best explains the relationship between $x_i$ and $y_i$.

  Then $y_i - w^\top x_i - \beta \sim \mathcal{N}(0, \sigma^2)$. $P(y_i|x_i) = \frac{1}{\sqrt{2\pi}\sigma} \exp{\left(-\frac{(y_i - w^\top x_i - \beta)^2}{2 \sigma^2}\right)}$.

  The better our estimate of parameter $w, \beta$ is, The greater $\prod_{i=1}^{n} P(y_i|x_i)$ becomes. So, our goal tunrs to find $\argmax{w, \beta} \prod_{i=1}^n P(y_i|x_i)$.
  And to find $\argmax{w, \beta} \prod_{i=1}^n P(y_i|x_i)$ is same with $\argmin{w, \beta} \sum_{i=1}^n -\log{P(y_i|x_i)}$.

  \[
    -\log{P(y_i|x_i)} = \log{\sqrt{2\pi}\sigma} + \frac{(y_i - w^\top x_i - \beta)^2}{2\sigma^2}
  \]
  This finding $\argmin{w, \beta} \sum_{i=1}^n - \log{P(y_i|x_i)}$ strategy is called \tb{Maximum Likelihood Principle (MLP)} and $-\log{P(y_i|x_i)}$ is called \tb{Negative Log-Likelihood (NLL)} for parameter $w, \beta$.
\end{frame}

\begin{frame}{.}
  $\nll{w, \beta} = n \log{\sqrt{2\pi}\sigma} + \sum_{i=1}^n \frac{(y_i - w^\top x_i - \beta)^2}{2\sigma^2}$ and since $\nll{w, \beta}$ is \ti{convex function} for $w, \beta$, $w^\ast, \beta^\ast := \argmin{w, \beta} \nll{w, \beta}$ satisfies $\nabla \nll{w^\ast, \beta^\ast} = 0$.

  In this case $\nabla_{w, \beta} \rss{w^\ast, \beta^\ast} = 0$ where $\rss{w, \beta} = \frac{1}{2}\sum_{i=1}^n (y_i - w^\top x_i - \beta)^2$.

  \bigskip
  Let $\forall j, 1 \leq j \leq n, Y_j = y_j \leq n$ and $X_{ij} = x_i, \forall 1 \leq i\leq n, \forall 1 \leq j \leq d$ and $X_{*(d+1)} = \mb{1}$ (so, $Y \in \mbb{R}^{n}$ and $X \in \mbb{R}^{n \times (d+1)}$). Let $\bar{w} = \left[\begin{matrix} w_1 \\ \vdots \\ w_d \\ \beta  \end{matrix}\right]$ Then we can write $\rss{\bar{w}} = \frac{1}{2} (X\bar{w} - Y)^\top (X\bar{w} - Y) =\frac{1}{2}\abs{X\bar{w} - Y}^2_2, \nabla_{\bar{w}}\rss{\bar{w}} = \sum_{i=1}^n (X_{i*} \bar{w} - Y_i) X_{i*}$
\end{frame}

\end{document}