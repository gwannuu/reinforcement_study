\documentclass[11pt]{beamer}
\usefonttheme[onlymath]{serif}

\setbeamersize{text margin left=1.5em}
\setbeamersize{text margin right=1.5em}




\setbeamertemplate{frametitle}{%
  \vskip1ex
  \usebeamerfont{frametitle}%
  \insertframetitle\par %frame 에서 지정한 title 사용
  %\insertsubsectionhead\par        % subsection의 header를 사용
  \vskip1ex
  \hrule                             % 밑줄(선택)
}

\setbeamertemplate{blocks}[rounded][shadow=true] % 블록 테두리 둥글게

\setbeamertemplate{itemize item}{\usebeamerfont{itemize item}\textbullet}
\setbeamertemplate{itemize subitem}{\usebeamerfont{itemize subitem}\textbullet}
\setbeamertemplate{itemize subsubitem}{\usebeamerfont{itemize subsubitem}\textbullet}



%\setbeamerfont{itemize/enumerate subbody}{parent=itemize/enumerate body} % 
%\setbeamerfont{itemize/enumerate subbody}{size=\usebeamerfont{itemize/enumerate body}\size}

\makeatletter
% Taken from beamer.cls' default geometry settings
% http://mirrors.ctan.org/macros/latex/contrib/beamer/base/beamer.cls
\geometry{%
  papersize={\fpeval{\beamer@paperwidth*1.5}pt,\fpeval{\beamer@paperheight*1.5}pt},
  hmargin=\fpeval{0.5 * 1.5}cm,% 1cm
  vmargin=0cm,%
  head=\fpeval{0.5*1.5}cm,% 0.5cm
  headsep=0pt,%
  foot=\fpeval{0.5*1.5}cm% 0.5cm
}
\makeatother %from search keyword beamer size, get this search result -> https://tex.stackexchange.com/questions/586756/beamer-use-glyphs-from-smaller-font-size-but-enlarge


% Reference bibtex
% style = numeric, apa, authoryear-comp
\usepackage[backend=biber, style=authoryear-comp, natbib=true]{biblatex}
\addbibresource{../../references.bib}



% 테마 선택 (선택 사항)
% \usetheme{Madrid} % 기본 테마, 다른 테마 사용 가능
% \font{serif}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage[T1]{fontenc} % To use combination of textbf, textit
\usepackage[dvipsnames]{xcolor}   % can use more variant colors
%\usepackage{lmodern} %다른 폰트 사용: 문서의 서문에 추가하면 Computer Modern 폰트의 확장 버전인 Latin Modern 폰트를 사용할 수 있습니다. 이 폰트는 더 다양한 크기와 스타일을 지원하여 문제를 해결해 줄 수 있습니다.

% \setcounter{MaxMatrixCols}{20}

% (필요한 패키지들)
% \usepackage{amsthm}
\setbeamertemplate{theorems}[numbered]  % 정리, 정의 등에 번호를 달아줌

% \theoremstyle{plain} % insert bellow all blocks you want in italic
% \newtheorem{theorem}{Theorem}[section] % to number according to section
% 
% \theoremstyle{definition} % insert bellow all blocks you want in normal text
% \newtheorem{definition}{Definition}[section] % to number according to section
% \newtheorem*{idea}{Proof idea} % no numbered block

\newtheorem{proposition}[theorem]{Proposition}

\usepackage{tcolorbox}

% 필요할 경우 패키지 추가
\usepackage{graphicx} % 이미지 삽입을 위한 패키지
\usepackage{amsmath}   % 수식 사용
\usepackage{hyperref}  % 하이퍼링크 추가
\hypersetup{colorlinks} % search keyword: beamer hyperref color -> https://tex.stackexchange.com/questions/13423/how-to-change-the-color-of-href-links-for-real
\usepackage{cleveref}
\usepackage{multicol}  % 여러 열 나누기
\usepackage{ulem} % 취소선 및줄 나누기
\usepackage{mathtools} % dcases
%\usepackage{xparse} % NewDocumentCommand
\usepackage[boxed, lined]{algorithm2e} % to use algorithm block



% \NewDocumentCommand{\DefThreeOp}{m}{%
%   % \csname #1\endcsname 라는 이름으로, 3개 인자를 받는 새 매크로를 정의
%   \expandafter\NewDocumentCommand\csname #1\endcsname{mmm}{%
%     \operatorname{#1}\!\bigl(##1,\,##2,\,##3\bigr)%
%   }%
% }

\newcommand{\mrm}[1]{\mathrm{#1}}
\newcommand{\mbb}[1]{\mathbb{#1}}
\newcommand{\mb}[1]{\mathbf{#1}}
\newcommand{\mc}[1]{\mathcal{#1}}
\newcommand{\tb}[1]{\textbf{#1}}
\newcommand{\ti}[1]{\textit{#1}}
\newcommand{\Pois}[1]{\operatorname{Pois}(#1)}

\newcommand{\myber}[1]{\operatorname{Bern}\!\left(#1\right)}
\newcommand{\Bin}[2]{\operatorname{Bin}\!\left(#1,#2\right)}
\newcommand{\NBin}[2]{\operatorname{NBin}\!\left(#1,#2\right)}
\newcommand{\mytoinf}[1]{#1 \rightarrow \infty}
\newcommand{\myexp}[1]{\exp{\left(#1\right)}}
\newcommand{\Unif}[2]{\operatorname{Unif}\!\left(#1, #2\right)}
\newcommand{\UnifOne}[1]{\operatorname{Unif}\!\left(#1\right)}
\newcommand{\mygeom}[1]{\operatorname{Geom}\!\left(#1\right)}
\newcommand{\Expo}[1]{\operatorname{Expo}\!\left(#1\right)}
\newcommand{\abs}[1]{\left\lvert #1 \right\rvert}
\newcommand{\floor}[1]{\left\lfloor #1 \right \rfloor}
\newcommand{\expec}[1]{\operatorname{E}\left[ #1 \right]}
\newcommand{\Var}[1]{\operatorname{Var}\left[#1\right]}
\newcommand{\myskew}[1]{\operatorname{Skew}\!\left[#1\right]}
\newcommand{\mykurt}[1]{\operatorname{Kurt}\!\left[#1\right]}
\newcommand{\mywei}[2]{\operatorname{Wei}\!\left(#1, #2\right)}
\newcommand{\Span}{\operatorname{Span}}
\newcommand{\Cov}[2]{\operatorname{Cov}\!\left(#1, #2\right)}
\newcommand{\intinfty}{\int_{-\infty}^\infty}
\newcommand{\Corr}[2]{\operatorname{Corr}\!\left(#1, #2\right)}
\newcommand{\Mult}[3]{\operatorname{Mult}_{#1}\!\left(#2, #3\right)}
\newcommand{\Beta}[2]{\operatorname{Beta}\!\left(#1, #2\right)}
\newcommand{\HGeom}[3]{\operatorname{HGeom}\!\left(#1, #2, #3\right)}
\newcommand{\NHGeom}[3]{\operatorname{NHGeom}\!\left(#1,#2, #3\right)}
\newcommand{\GammaDist}[2]{\operatorname{Gamma}\!\left(#1, #2\right)}
%\DefThreeOp{PHGeom}

\newcommand{\im}{\operatorname{im}}
\newcommand{\tr}{\operatorname{tr}}
\newcommand{\supp}{\operatorname{supp}}


% 발표 제목, 저자, 날짜 설정
\title{Conservative Q-Learning for Offline Reinforcement Learning}
\author{Gwanwoo Choi}
% \date{}

\begin{document}
% 표지 슬라이드

\begin{frame}
    \titlepage
\end{frame}

% \subsection{Domain Randomization}
% \begingroup
%     \setbeamertemplate{frametitle}{%
%     \vskip1ex
%     \usebeamerfont{frametitle}%
%     \insertframetitle\par        %  ← 원하는 대로 변경 가능
%     \vskip1ex
%     \hrule                             % 밑줄(선택)
%     }
%     \begin{frame}
%         \frametitle{Table of Contents}
%         \tableofcontents[currentsubsection]
%     \end{frame}
% \endgroup

% % 목차 

% \begin{frame}{Curriculum Learning}

% \end{frame}

\begin{frame}{Preliminaries}
  \begin{itemize}
    \item MDP: $<\mc{S}, \mc{A},T, r, \gamma>$
    \item $\mc{S}$ - State space
    \item $\mc{A}$ - Action space
    \item $T: \mc{S}\times \mc{A} \to \Delta(S)$ - Transition Probability
    \item $r: \mc{S} \times \mc{A} \to \mbb{R}$ - Reward Function ($\abs{r(s,a)} \leq R_{\text{max}}$)
    \item $\gamma \in \mbb{R}$ - Discounted Factor
    \item $\pi : \mc{S} \to \Delta(\mc{A})$ - Policy
  \end{itemize}
\end{frame}


\begin{frame}{Online RL}
  \begin{block}{Bellman Operator}
    \[
      (\mc{B}^\pi Q)(s,a) = r(s,a) + \gamma \mbb{E}_{s^\prime \sim T(\cdot |s,a)}[\mbb{E}_{a^\prime \sim \pi(\cdot | s^\prime)}[Q(s^\prime, a^\prime)]] 
    \]
  \end{block}

  \begin{block}{Bellman Optimal Operator}
    \[
      (\mc{B}^\ast Q)(s,a) = r(s,a) + \gamma \mbb{E}_{s^\prime \sim T(\cdot |s,a)}[\max_{a^\prime} Q(s^\prime,a^\prime)]
    \]
  \end{block}
  This equation is used to learn Q-Learning or DQN.
\end{frame}

\begin{frame}{Offline RL}
  \begin{itemize}
    \item Unlike Online RL, in offline RL, agent can't interact with environment.
    \item Instead, agent can learn from offline dataset $\mc{D}$, where trajectories collected by behavior policy $\beta$ had gathered.
  \end{itemize}

  Preliminaries
  \begin{itemize}
    \item MDP: $<\mc{S}, \mc{A},T, r, \gamma>$
    \item $\mc{S}$ - State space
    \item $\mc{A}$ - Action space
    \item $T: \mc{S}\times \mc{A} \to \Delta(S)$ - Transition Probability
    \item $r: \mc{S} \times \mc{A} \to \mbb{R}$ - Reward Function ($\abs{r(s,a)} \leq R_{\text{max}}$)
    \item $\gamma \in \mbb{R}$ - Discounted Factor
    \item $\pi, \beta : \mc{S} \to \Delta(\mc{A})$ - Policy, Behavior Policy
    \item $\mc{D} = \{(s,a,r,s)\}$ - Offline Dataset (collected by $\beta$)
  \end{itemize}
\end{frame}

\begin{frame}{Offline RL}
  \begin{itemize}
    \item In online RL, agent can steadily learn about reward function $r(s,a)$ and transition dynamics $T(s^\prime | s,a)$.
    \item But offline RL, agent only can infer $r(s,a)$ and $T(s^\prime |s,a)$ from trajectories $(s,a,r,s^\prime) \in \mc{D}$.
    \item Adding to this challenge, the offline dataset does not cover the full range of possible $(s,a,r,s^\prime)$ transitions.
  \end{itemize}
  So, \textbf{Empirical Bellman Operator} is introduced
\end{frame}

\begin{frame}{Offline RL}
  \begin{block}{Empirical Bellman Operator}
    \[
      (\hat{\mc{B}}^\pi Q)(s,a) = \hat{r} + \gamma \mbb{E}_{s^\prime \sim \hat{T}(\cdot | s,a)}\left[\mbb{E}_{a^\prime \sim \hat{\beta}(\cdot |s^\prime)} \left[ Q(s^\prime,a^\prime) \right]\right]
    \]

    where

    \[
    \begin{aligned}
    &\abs{\mc{D}(s)} = \sum_{(s_i,a_i,r_i,s_i^\prime) \in \mc{D}}\mb{1}[s=s_i] \\
    &\abs{\mc{D}(s,a)} = \sum_{(s_i,a_i,r_i,s^\prime_i)\in\mc{D}}\mb{1}[s=s_i, a=a_i]\\
    &\abs{\mc{D}(s,a,\cdot, s^\prime)} = \sum_{(s_i, a_i, r_i, s^\prime_i)} \mb{1}[s=s_i, a=a_i, s^\prime=s_i^\prime] \\
    &\hat{r}(s,a) = \frac{1}{\abs{\mc{D}(s,a)}} \sum_{(s_i, a_i, r_i, s_i^\prime) \in \mc{D}}\mb{1}[s=s_i,a=a_i]\cdot r_i \\
    &\hat{T}(s^\prime|s,a) = \frac{\abs{\mc{D}(s,a,\cdot, s^\prime)}}{\abs{\mc{D}(s,a)}} \\
    &\hat{\beta}(a|s) = \frac{\abs{\mc{D}(s,a)}}{\abs{\mc{D}(s)}}
    \end{aligned}
    \]
  \end{block}
\end{frame}

\begin{frame}{Offline RL}
  \begin{block}{Basic Offline RL framework (Policy Iteration)}
    Randomly initialize $\hat{Q}^{0}$ and update $\hat{Q}^k$ and $\hat{\pi}^k$ by following rule
    \[
    \begin{gathered}
      \hat{Q}^{k+1} \leftarrow \arg\min_Q \mbb{E}_{(s,a,r,s^\prime) \sim \mc{D}}\left[\left(r + \gamma \mbb{E}_{a^\prime \sim \hat{\pi}^k (\cdot | s^\prime)}[\hat{Q}^k(s^\prime, a^\prime)]  - Q(s,a)\right)^2\right] \\
      \hat{\pi}^{k+1} \leftarrow \arg\max_\pi \mbb{E}_{s \sim \mc{D}} \left[\mbb{E}_{a \sim \hat{\pi}^k(\cdot|s)} \left[\hat{Q}^{k+1}(s,a)\right]\right]
    \end{gathered}
    \]
  \end{block}
  But there exists some problems
  \begin{itemize}
    \item The policy distribution $\beta$ and $\hat{\pi}^k$ are mismatched during the learning of $\hat{Q}^k$ and $\hat{\pi}^k$.
    \begin{itemize}
      \item Recall that in original policy iteration, bellman operator updates $Q^k$ using transitions sampled from $\pi^k$.Then $\pi^k$ is updated to follow the maximizing $Q^k$ direction.
      \item In this version, note that $\hat{Q}^k$ is updated by single transition which is sampled from $\beta$, but 
    \end{itemize}
    \item $\hat{\pi}^k(\cdot|s)$ can sample $a$ where $\beta(a|s) = 0$ holds. (O.O.D. situation, Mismatch with distribution of $\mc{D}$)
  \end{itemize}
\end{frame}


\begin{frame}{Conservative Off-Policy Evaluation}
  First approach:
  \begin{itemize}
    \item Utilize Empirical Bellman Operator, which samples $(s,a,r,s^\prime)$ obtained by $\beta$
    \item To prevent Q-value overestiation, depress Q-value itself
  \end{itemize}

  \begin{block}{Conservative Policy Evaluation (lower-bound)}
    For any policy $\mu : \mc{S} \to \Delta{\mc{A}}, \supp \mu \subset \supp \beta$,
    \[
      \hat{Q}^{k+1} \leftarrow \arg \min_{Q} \textcolor{red}{\alpha \mbb{E}_{s \sim \mc{D}, a \sim \mu(\cdot|s)}[Q(s,a)]} + \frac{1}{2} \mbb{E}_{s,a, s^\prime \sim \mc{D}} \left[\left(Q(s,a) - \hat{\mc{B}}^\pi \hat{Q}^k (s,a)\right)^2\right]
    \]

    Recall that $ (\hat{\mc{B}}^\pi Q)(s,a) = \hat{r} + \gamma \mbb{E}_{s^\prime \sim \hat{T}(\cdot | s,a)}\left[\mbb{E}_{a^\prime \sim \hat{\beta}(\cdot |s^\prime)} \left[ Q(s^\prime,a^\prime) \right]\right]$
  \end{block}

  \begin{itemize}
    \item $\supp \mu \subset \supp \beta$ means that $\forall s,a, \beta(a|s) \neq 0 \implies \mu(a|s)\neq 0$.
    \item The first term $\mbb{E}_{s \sim \mc{D}, a \sim \mu(\cdot|s)}[Q(s,a)]$ depress the value of $Q(s,a)$
    \item The second term $\frac{1}{2} \mbb{E}_{s,a \sim \mc{D}} \left[\left(Q(s,a) - \hat{\mc{B}}^\pi \hat{Q}^k (s,a)\right)^2\right]$ fits $Q(s,a)$ values, but with only using offline dataset $\mc{D}$.

  \end{itemize}
\end{frame}

\begin{frame}{Conservative Off-Policy Evaluation}
  \begin{block}{Conservative Policy Evaluation (lower-bound)}
    For any policy $\mu : \mc{S} \to \Delta{\mc{A}}, \supp \mu \subset \supp \beta$,
    \begin{equation} \label{eq:1}
      \hat{Q}^{k+1} \leftarrow \arg \min_{Q} \alpha \mbb{E}_{s \sim \mc{D}, a \sim \mu(\cdot|s)}[Q(s,a)] + \frac{1}{2} \mbb{E}_{s,a, s^\prime \sim \mc{D}} \left[\left(Q(s,a) - \hat{\mc{B}}^\pi \hat{Q}^k (s,a)\right)^2\right]
    \end{equation}

  \end{block}
  
  \begin{block}{Lower Bound Inequation}
    For learned $\hat{Q}^\pi = \lim_{k \to \infty} \hat{Q}^k$ which follows above approach, with probability $\geq 1-\delta$, following inequation holds
    \[
    \begin{aligned}
      \forall s \in \mc{D}, a, \hat{Q}^\pi(s,a) \leq &Q^\pi(s,a) - \alpha \left[(I - \gamma T^\pi)^{-1} \frac{\mu}{\hat{\beta}}\right](s,a) \\
      &+\left[(I-\gamma T^\pi)^{-1} \frac{C_{r,T,\delta}R_{\text{max}}}{(1-\gamma)\sqrt{\abs{\mc{D}(s,a)}}}\right](s,a)
    \end{aligned}
    \]
    where $\delta$ is small value $\in (0,1)$, $T^\pi Q(s,a)= \mbb{E}_{s^\prime \sim T(\cdot|s,a)}[\mbb{E}_{a^\prime \sim \pi(\cdot|s^\prime)}[Q(s^\prime,a^\prime)]]$, $C_{r,T,\delta}$ is a constant dependent on $r, T, \delta$.

    Thus, if $\alpha$ is a sufficiently large, then $\hat{Q}^\pi (s,a) \leq Q^\pi(s,a), \forall s\in \mc{D},a$.
    When $\hat{\mc{B}}^\pi = \mc{B}^\pi$, any $\alpha > 0$ guarantees inequation.
  \end{block}
    The inequality show that the more $\abs{\mc{D}(s,a)}$ increases, the theoretical value of $\alpha$ decreases.
\end{frame}

\begin{frame}{Conservative Off-Policy Evaluation}
  \begin{block}{Lower Bound Inequation (Proof)}
    From \href{eq:1}{equation 1}, we can obtain following equation by setting the derivative to zero
    \begin{equation}
      \begin{aligned}
        &\frac{\partial}{\partial Q(s,a)} \left[\alpha \mbb{E}_{s \sim \mc{D}, a \sim \mu(a|s)}Q(s,a)\right] + \frac{1}{2} \mbb{E}_{s \sim \mc{D}}\left[ \left( Q(s,a) - \hat{\mc{B}}^\pi \hat{Q}^k (s,a)\right)^2\right] =0 \\
        \implies &\alpha d^{\beta} \mu(a|s) + d^\pi (s) \beta(a|s) \left[ Q(s,a) - \hat{\mc{B}}^\pi \hat{Q}^k (s,a)\right] =0 \\
        \implies &\hat{Q}^{k+1} (s,a) = \hat{\mc{B}}^\pi \hat{Q}^k - \alpha \frac{\mu(a|s)}{\beta(a|s)}
      \end{aligned}
    \end{equation}

    Meanwhile, with consideration for sampling error, under inequation holds with high probability (w.h.p.) (Please refer \href{appendix:sampling_error}{Appendix about $C_{r,T,\delta}$})
    \[
      \forall Q,s,a \in \mc{D}, \abs{\hat{\mc{B}}^\pi Q(s,a) - \mc{B}^\pi Q(s,a)} \leq \frac{C_{r,T,\delta} R_{\text{max}}}{(1-\gamma)\sqrt{\abs{\mc{D}(s,a)}}}
    \]

    Combining this result with sampling error consideration, we can get \hypertarget{text:why_assume_overestimation}{under inequation}
    \[
    \begin{aligned}
      &\abs{\hat{Q}^{k+1} + \alpha \frac{\mu(a|s)}{\hat{\beta}(a|s)}} \leq \frac{C_{r,T.\delta}R_{\max}}{(1-\gamma) \sqrt{\abs{\mc{D}(s,a)}}}
      \implies \hat{Q}^{k+1} \leq \mc{B}^\pi \hat{Q}^k - \alpha \frac{\mu(a|s)}{\hat{\beta}(a|s)}  + \frac{C_{r,T,\delta} R_{\text{max}}}{(1- \gamma) \sqrt{\abs{\mc{D}(s,a)}}}
    \end{aligned}
    \]
  \end{block}
\end{frame}

\begin{frame}{Conservative Off-Policy Evaluation}
  \begin{block}{Lower Bound Inequation (Proof, cont')}
    By fixed point theorem, there exists unique $\lim_{k \to \infty}\hat{Q}^{k} = \hat{Q}^\pi$.
    This $\hat{Q}^\pi$ also satisfies following inequation
    \[
      \hat{Q}^\pi \leq \mc{B}^\pi \hat{Q}^\pi - \alpha \frac{\mu(a|s)}{\hat{\beta}(a|s)} + \frac{C_{r,T,\delta}R_{\text{max}}}{(1-\gamma) \sqrt{\abs{\mc{D}(s,a)}}}
    \]
    We can this inequation as following vectorized form and finally can derive our result of theorem 1
    \[
      \begin{aligned}
        &(I - \gamma T^\pi) \hat{Q}^\pi \leq R + \alpha \frac{\mu}{\hat{\beta}} +\frac{C_{r,T,\delta}R_{\text{max}}}{(1-\gamma) \sqrt{\abs{\mc{D}}}} \\
        \implies&\hat{Q}^\pi \leq (I - \gamma T^\pi)^{-1} \left[ R + \alpha \frac{\mu}{\hat{\beta}} + \frac{C_{r,T,\delta}R_{\text{max}}}{(1-\gamma) \sqrt{\abs{\mc{D}}}}\right] \\
        \implies &\forall s,a, \hat{Q}^\pi(s,a) \leq Q^\pi (s,a) - \alpha \left[ (I - \gamma T^\pi)^{-1}\left[\frac{\mu}{\hat{\beta}}\right]\right](s,a) \\
        &+ \left[(I - \gamma T^\pi)^{-1} \frac{C_{r,T,\delta}R_{\text{max}}}{(1-\gamma) \sqrt{\abs{\mc{D}}}}\right] (s,a)
      \end{aligned}
    \]
  \end{block}
\end{frame}

\begin{frame}{Conservative Off-Policy Evaluation}
  \begin{block}{Lower Bound Inequation (Proof, cont')}
    In order to guarantee a lower bound, $\alpha$ can be chosen to cancel any potential \hypertarget{text:why_only_consider_overestimation}{overestimation incurred} by $\frac{C_{r,T,\delta}R_{\text{max}}}{(1-\gamma) \sqrt{\abs{\mc{D}}}}$.

  \end{block}
\end{frame}

\begin{frame}{Conservative Off-Policy Evaluation}
  % Not only decrease the L2 error between $Q^{k+1}(s,a)$ and $\hat{\mc{B}}^{\pi} \hat{Q}^k(s,a)$, But also decrease $\hat{Q}^{k+1}(s,a)$ itself.

  The problem of first approach is:
  \begin{itemize}
    \item If $\alpha$ becomes bigger, then $Q(s,a)$ could be undervalued.
  \end{itemize}

  Second approach:
  \begin{itemize}
    \item Elevate $Q(s,a)$ in proportion to the probability of $(s,a)\in \mc{D}$.
  \end{itemize}

  \begin{block}{Conservative Policy Evaluation (more tighten lower-bound)}
    For any policy $\mu : \mc{S} \to \Delta{\mc{A}}, \supp \mu \subset \supp \beta$,
    \[
    \begin{aligned}
          \hat{Q}^{k+1} \leftarrow &\arg \min_{Q} \alpha \cdot \left(\mbb{E}_{s \sim \mc{D}, a \sim \mu(\cdot|s)}[Q(s,a)] \textcolor{red}{- \mbb{E}_{s \sim \mc{D}, a \sim \hat{\beta}(\cdot|s)} \left[Q(s,a)\right]} \right) \\
          &+ \frac{1}{2} \mbb{E}_{s,a,s^\prime \sim \mc{D}} \left[ \left(Q(s,a) - \hat{\mc{B}}^\pi \hat{Q}^{k}(s,a)\right)^2\right]
  \end{aligned}
  \]
  Recall that $ (\hat{\mc{B}}^\pi Q)(s,a) = \hat{r} + \gamma \mbb{E}_{s^\prime \sim \hat{T}(\cdot | s,a)}\left[\mbb{E}_{a^\prime \sim \hat{\beta}(\cdot |s^\prime)} \left[ Q(s^\prime,a^\prime) \right]\right]$
  \end{block}

  % Add maximization term for the $Q^{k+1}(s,a)$ value if $(s,a)$ is in dataset $\mc{D}$.

  \begin{itemize}
    \item Note that in both evaluation methods, size of $\alpha$ plays a crucial role that makes hold the \underline{lower bound inequation}(See this later)
    \item The less the $\alpha$ is, the higher probability to hold lower bound
  \end{itemize}
\end{frame}

\begin{frame}{Conservative Off-Policy Evaluation}
    \begin{block}{Conservative Policy Evaluation (more tighten lower-bound)}
    For any policy $\mu : \mc{S} \to \Delta{\mc{A}}, \supp \mu \subset \supp \beta$,
    \[
    \begin{aligned}
          \hat{Q}^{k+1} \leftarrow &\arg \min_{Q} \alpha \cdot \textcolor{red}{\left(\mbb{E}_{s \sim \mc{D}, a \sim \mu(\cdot|s)}[Q(s,a)] - \mbb{E}_{s \sim \mc{D}, a \sim \hat{\beta}(\cdot|s)} \left[Q(s,a)\right] \right)} \\
          &+ \frac{1}{2} \mbb{E}_{s,a,s^\prime \sim \mc{D}} \left[ \left(Q(s,a) - \hat{\mc{B}}^\pi \hat{Q}^{k}(s,a)\right)^2\right]
  \end{aligned}
  \]
  Recall that $ (\hat{\mc{B}}^\pi Q)(s,a) = \hat{r} + \gamma \mbb{E}_{s^\prime \sim \hat{T}(\cdot | s,a)}\left[\mbb{E}_{a^\prime \sim \hat{\beta}(\cdot |s^\prime)} \left[ Q(s^\prime,a^\prime) \right]\right]$
  \end{block}

  What is the true meaning of \textcolor{red}{red} terms?
  \begin{itemize}
    \item Suppose that for $s$, there exists possible actions $a^1, a^2$.
    \item Let assume $\mu(a^1|s) = 1$ and $\pi(a^1|s) = 0.5$.
    \item Then for $(s,a^1)$, the red term becomes $\left[ Q(s,a^1) - \frac{1}{2}Q(s,a^1) \right]$.
  \end{itemize}


\end{frame}


\begin{frame}{Conservative Off-Policy Evaluation}
  \begin{block}{Conservative Policy Evaluation (more tighten lower-bound)}
    For any policy $\mu : \mc{S} \to \Delta{\mc{A}}, \supp \mu \subset \supp \beta$,
    \[
    \begin{aligned}
      \hat{Q}^{k+1} \leftarrow &\arg \min_{Q} \alpha \cdot \left(\mbb{E}_{s \sim \mc{D}, a \sim \mu(\cdot|s)}[Q(s,a)] - \mbb{E}_{s \sim \mc{D}, a \sim \hat{\beta}(\cdot|s)} \left[Q(s,a)\right] \right) \\
      &+ \frac{1}{2} \mbb{E}_{s,a,s^\prime \sim \mc{D}} \left[ \left(Q(s,a) - \hat{\mc{B}}^\pi \hat{Q}^{k}(s,a)\right)^2\right]
  \end{aligned}
  \]
  \end{block}

  \begin{block}{(More Tighten) Lower Bound Inequation}
    For $\hat{V}^{\pi}(s) = \mbb{E}_{\pi(a|s)}$, lower-bounds the true value of the policy obtained via exact policy evaluation, $V^\pi(s) = \mbb{E}_{\pi}(a|s) [Q^\pi(s,a)]$, when $\mu=\pi$, according to:
    \[
      \forall s \in \mc{D}, \hat{V}^\pi (s) \leq V^\pi (s) - \alpha \left[(I - \gamma T^\pi)^{-1} \mbb{E}_\pi  \left[\frac{\pi}{\hat{\beta}} - 1\right]\right] (s) + \left[(I - \gamma T^\pi)^{-1} \frac{C_{r, T, \delta}R_{\text{max}}}{(1-\gamma)\sqrt{\abs{\mc{D}}}}\right](s)
    \]
    Thus, if $\alpha \geq \frac{C_{r,T}R_{\text{max}}}{1-\gamma} \cdot \max_{s\in \mc{D}}\frac{1}{\sqrt{\abs{\mc{D}(s)}}}\cdot \left[\sum_a \pi(a|s) (\frac{\pi(a|s)}{\hat{\beta}(a|s)}-1)\right]^{-1}$, $\forall s \in \mc{D}$, $\hat{V}^{\pi}(s) \leq V^\pi (s)$, with probability $\geq 1 - \delta$.

    When $\hat{\mc{B}}^\pi = \mc{B}^\pi$, then any $\alpha >0$ guarantees $\hat{V}^\pi (s) \leq V^{\pi} (s), \forall s \in \mc{D}$
  \end{block}
  For suitable $\alpha$, both bounds hold under sampling error and function approximation.
\end{frame}


\begin{frame}{Conservative Q-Learning}
  General approach for offline policy learning, which is called \underline{Conservative Q-Learning (CQL)}.
  \begin{itemize}
    \item We can obtain Q-values that lower-bound the value of a policy $\pi$ by solving second approach with $\mu = \pi$.
    \item So, just update $\mu$ to maximize current Q-function iterate
  \end{itemize}
  \begin{block}{Conservative Q-Learning (CQL)}
    \[
    \begin{aligned}
      \min_{Q} \textcolor{red}{\max_{\mu}} \ &\alpha \left( \mbb{E}_{s \sim \mc{D}, \textcolor{red}{a \sim \mu(a|s)}}[Q(s,a)] - \mbb{E}_{s \sim \mc{D}, a \sim \hat{\pi}_\beta} [Q(s,a)] \right) \\
      &+ \frac{1}{2} \mbb{E}_{s,a,s^\prime \sim \mc{D}}\left[\left(Q(s,a) - \hat{\mc{B}}^{\pi_k}\hat{Q}^k (s,a) \right)^2 \right] + \textcolor{red}{\mc{R}(\mu)}
    \end{aligned}
    \]
    where $\mc{R}(\mu)$ is particular choices of regularizer
  \end{block}
\end{frame}

\begin{frame}{Conservative Q-Learning}
  \begin{block}{CQL($\mc{H}$)}
    In CQL($\mc{H}$), $\mc{R} = \mc{H}(\mu)$.
    The optimization problem becomes the form of
    \[
      \max_\mu \mbb{E}_{x \sim \mu(x)}[f(x)] + \mc{H}(\mu) \text{ s.t. }  \sum_x \mu(x)=1,\mu(x) \geq 0, \forall x
    \]

    We can solve this optimization problem by Lagrangian Multiplier.
    Let $J(\mu) = \mbb{E}_{x \sim \mu(x)} [f(x) + \mc{H}(\mu)] = \sum_x \mu(x)f(x) - \sum_x \mu(x) \log \mu(x)$ and $g(\mu) = \sum_x \mu(x) - 1 = 0$.
    Then
    \[
    \begin{gathered}
      \mc{L}(\mu, \lambda) = J(\mu) - \lambda \cdot g(\mu) \\
      \implies \mc{L}(\mu, \lambda) = \left(\sum_x \mu(x) f(x) - \sum_x \mu(x) \log \mu(x)\right) - \lambda \left(\sum_x \mu(x) -1\right) \\
      \text{Let } \forall \mu(x), \frac{\partial \mc{L}}{\partial \mu(x)} = 0 \implies f(x) - \log \mu(x) - 1 - \lambda = 0 \implies \mu(x) = \exp(f(x)) \cdot \exp(f(x)) \cdot \exp(-(1+\lambda)) \\
      \text{From the fact } \sum_x \mu(x)=1, \sum_x \exp(f(x)) \cdot \exp(-(1+\lambda)) =1 \implies \exp(1+\lambda) = \sum_x \exp(f(x)) \\
      \therefore \mu(x) = \frac{\exp(f(x))}{\sum_x \exp(f(x))}
    \end{gathered}
    \]
  \end{block}
\end{frame}

\begin{frame}{Conservative Q-Learning}
  If we plugin this $\mu(a|s) = \frac{Q(s,a)}{\sum_x \exp Q(s,a)}$, the generalized CQL framework can be specified by
  \[
    \begin{aligned}
      &\min_Q \max_\mu \alpha (\mbb{E}_{s \sim \mc{D}, a \sim \mu(\cdot|s)}[Q(s,a)] - \mbb{E}_{s \sim \mc{D}, a \sim \hat{\beta}(\cdot|s)}[Q(s,a)]) + \frac{1}{2}\mbb{E}_{s,a,s^\prime \sim \mc{D}}\left[\left(Q(s,a) - \hat{\mc{B}}^{\pi_k} \hat{Q}^k (s,a)\right)\right] + \mc{H}(\mu) \\
      &\implies \min_Q \left(\mbb{E}_{s \sim \mc{D}}\left[\sum_a \frac{\exp(Q(s,a))}{Z}Q(s,a)\right] - \mbb{E}_{s \sim \mc{D}, a \sim \hat{\beta}(\cdot|s)}[Q(s,a)] \right)\\
      &+\frac{1}{2}\left[\left(Q(s,a) - \hat{\mc{B}}^{\pi_k}\hat{Q}^k (s,a)\right)^2\right] - \mbb{E}_{s \sim \mc{D}}\left[\sum_a \frac{\exp(Q(s,a))}{Z} \log \exp (Q(s,a)) - \log Z\right] \\
      &\implies \min_Q \alpha \mbb{E}_{s \sim \mc{D}} \left[ \log \sum_a \exp(Q(s,a)) - \mbb{E}_{a \sim \hat{\beta}}[Q(s,a)]\right] + \frac{1}{2}\mbb{E}_{s,a,s^\prime \sim \mc{D}}\left[\left(Q(s,a) - \hat{\mc{B}}^{\pi_k} \hat{Q}^k(s,a)\right)^2\right] \label{eq:cql_h}
    \end{aligned}
  \]
\end{frame}

\begin{frame}{Conservative Q-Learning}
  \begin{block}{CQL($D_{KL}$)}
    If we choose $\mc{R}(\mu)$ to be the KL-divergence against a prior distribution, $\rho(a|s)$, i.e., $\mc{R}(\mu) = - D_{\text{KL}}(\mu, \rho)$, then we get $\mu(a|s) \propto \rho(a|s) \cdot \exp(Q(s,a))$.
    Following is the derivation.
    \[
    \begin{gathered}
      \max_\mu \mbb{E}[x \sim \mu(\cdot)][f(x)] - D_{\text{KL}}(\mu \parallel \rho) \text{ s.t. } \sum_x \mu(x) = 1, \mu(x) \geq 0, \forall x \\
      J(\mu) = \sum_x \mu(x)f(x) - \sum_x \mu(x) \log \frac{\mu(x)}{\rho(x)} \cdot g(\mu) = \sum_x \mu(x) - 1 \\
      \mc{L}(\mu, \lambda) = J(\mu) - \lambda g(\mu) \\
      \forall \mu(x), \frac{\partial \mc{L}}{\partial \mu(x)} = 0 \implies f(x) - \log \mu(x) -1 + \log \rho(x) - \lambda \implies \mu(x) = \exp(\log(\rho(x)) + f(x) - (1+\lambda))\\
      \text{Let } \sum_x \mu(x) = 1 \implies \exp(-(\lambda +1)) \sum_x \exp(\log(\rho(x))+f(x)) = 1 \implies \exp(\lambda+1) = \frac{1}{\sum_x \rho(x) \exp (f(x))} \\
      \therefore \mu(x) = \frac{\rho(x) \exp(f(x))}{\sum_x \rho(x) \exp(f(x))}
    \end{gathered}
    \]
  \end{block}
\end{frame}

\begin{frame}{Conservative Q-Learning}
  So, plugging this result to CQL framework,
  \[
    \begin{aligned}
      \min_Q \max_\mu \ & \alpha (\mbb{E}_{s \sim \mc{D}, a \sim \mu(\cdot|s)}[Q(s,a)] - \mbb{E}_{s \sim \mc{D}, a \sim \hat{\beta}(\cdot|s)}[Q(s,a)]) + \frac{1}{2}\mbb{E}_{s,a,s^\prime \sim \mc{D}}\left[\left(Q(s,a) - \hat{\mc{B}}^{\pi_k} \hat{Q}^k (s,a)\right)\right] \\
      &-D_{KL}(\mu \parallel \rho) \\
      \min_Q \ &\alpha \left(\mbb{E}_{s \sim \mc{D}}\left[\frac{\sum_a \rho(a|s)\exp(Q(s,a))}{Z}\right] - \mbb{E}_{s \sim \mc{D}, a \sim \hat{\beta}(\cdot|s)}[Q(s,a)]\right) \\
      &+ \frac{1}{2}\mbb{E}_{s,a,s^\prime \sim \mc{D}}\left[\left( Q(s,a) - \hat{\mc{B}}^{\pi_k} \hat{Q}^k (s,a)\right)\right] - \mbb{E}_{s\sim \mc{D}}\left[\sum_a \frac{\rho(a|s)\exp(Q(s,a))}{Z} \log \frac{\rho(a|s)\exp(s,a)}{\rho(a|s) Z}\right] \\
      \text{t.b.c...}
    \end{aligned}
  \]
\end{frame}

\begin{frame}{Appendix}
  \begin{block}{Sampling Error of Empirical Bellman Operator} \label{appendix:sampling_error}
    In Offline-RL, there exists \tb{sampling error} due to the finite size of the datset $\mc{D}$.

    \begin{block}{Assumption D.1. (Concentration properties)}
      For all $s,a \in \mc{D}$, the following relationships hold with high probability, $\geq 1-\delta$, where $C_{r,\delta}$ and $C_{T,\delta}$ depends on $\delta$ via a $\sqrt{\log(1/\delta)}$ dependency
      \[
        \abs{r - r(s,a)} \leq \frac{C_{r,\delta}}{\sqrt{\abs{\mc{D}(s,a)}}}, \abs{\hat{T}(s^\prime|s,a) - T(s^\prime |s,a)}_1 \leq \frac{C_{T,\delta}}{\sqrt{\mc{D}(s,a)}}
      \]
    \end{block}
    Under this assumption, the difference between the empirical Bellman operator and the actual Bellman operator can be bounded
    \[
      \begin{aligned}
        \abs{\hat{\mc{B}}^\pi - \mc{B}^\pi \hat{Q}^k} &= \abs{ (r - r(s,a)) + \gamma \sum_{x^\prime} \left(\hat{T}(s^\prime|s,a) - T(s^\prime |s,a)\right)\mbb{E}_{\pi(\cdot | s^\prime)}\left[ \hat{Q}^k (s^\prime, a^\prime)\right]} \\
        & \leq \abs{r - r(s,a)} + \gamma \abs{\sum_{s^\prime} \left(\hat{T}(s^\prime |s,a) - T(s^\prime |s,a)\right)\mbb{E}_{\pi(\cdot | s^\prime)}\left[\hat{Q}^k (s^\prime, a^\prime)\right]} \\
        &\abs{r - r(s,a)} + \gamma \abs{\sum_{s^\prime} \left(\hat{T}(s^\prime|s,a) - T(s^\prime|s,a)\right)}_1 \abs{\mbb{E}_{\pi(\cdot | s^\prime)}\left[\hat{Q}^k(s^\prime, a^\prime)\right]}_\infty \\
        &\leq \frac{C_{r,\delta}}{\sqrt{\abs{\mc{D}(s,a)}}} + \frac{C_{T,\delta}}{\sqrt{\abs{\mc{D}(s,a)}}} \cdot \frac{2\gamma R_{\text{max}}}{(1-\gamma)} = \frac{C_{r,T,\delta}}{\sqrt{\abs{\mc{D}(s,a)}}} \cdot \frac{R_{\text{max}}}{(1-\gamma)}
      \end{aligned}
    \]
  \end{block}
\end{frame}

\begin{frame}{Question}
  \begin{itemize}
    \item \hyperlink{text:why_only_consider_overestimation}{Why only consider overestimation problem?}
    \begin{itemize}
      \item \hyperlink{text:why_assume_overestimation}{In this equation, the absolute is removed with assuming $\hat{\mc{B}}^\pi Q(s,a) \geq \mc{B}^\pi Q(s,a)$}
    \end{itemize}
  \end{itemize}
\end{frame}

\end{document}